# daily log

apply jobs in uchicago
study pandas
tickets:
 Improve the accuracy for grizzly and velo

Improve the accuracy for grizzly and velo
The accuracy numbers are currently:
GRIZZLY MO X 7MG
66.06643658
VELO PLUS
54.18661019

Improve the accuracy for grizzly and velo
The accuracy numbers are currently:
GRIZZLY MO X 7MG
66.06643658
VELO PLUS
54.18661019
suggest what options can we do:
Optimizations and things to keep in mind:
the filter_training_lgb is set so we discard no more than 5% sales in the 02 - ML Model
the wpe on the 06 - Evaluate notebook can't be changed
I need ideas to improve the accuracy
do not output code
<repository_contents>
<file path="grizzly_velo.json">
{
  "_comment_readme": "This configuration file is being used for the PE Remodel project as of June 2025",

  "_comment_macroeconomic": "1 to use macroeconomics data in lgb model, otherwise put 0",
  "include_macroeconomic": 1,

  "catalog": "rai_prod_uc",

  "_comment_extra_num_feats_lgb": "list of optional numerical features",
  "extra_num_feats_lgb": ["rsd_quantity_ind_perc"],
  "extra_num_feats_lgb_industry": 1,
  
  "_comment_variables": "put the following variables to 1 if you want a pe for each of those groups, otherwise put 0",
  "cig_length": 0,
  "price_tier": 0,
  "menthol": 0,
  
  "price_threshold": 1,
  "cpu_or_serving": "cpu",
  "industry_id": [1],
  "industry_unique_id": 1,
  "industry": "fmc",

  "_comment_states": "states for which we want PE numbers",
  "states": ["AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL",
     "GA", "ID", "IL", "IN", "IA", "HI", "KS", "KY", "LA", "ME", "MD", "MA", "MI",
     "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND",
     "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA",
     "WA", "WV", "WI", "WY"],

  "_comment_products": "list of products and competitors by product_lineup_sk",
    "products": 
{"GRIZZLY MO X 7MG": {"id": 1053221, "other": [], "filter_training_lgb": "(rsd_quantity_y / rsd_quantity_roll4) <= 6 and (rsd_quantity_roll4 / rsd_quantity_y) <= 6 and (rsd_price_perc < 2) and (rsd_price_perc > 0.5) and (data_points >= 10)", "hyperparameters": {"min_delta": 0.0001}}, "VELO PLUS": {"id": 1052079, "other": [], "filter_training_lgb": "(rsd_quantity_y / rsd_quantity_roll4) <= 5 and (rsd_quantity_roll4 / rsd_quantity_y) <= 5 and (rsd_price_perc < 2) and (rsd_price_perc > 0.5) and (data_points >= 10)", "hyperparameters": {"min_delta": 0.0001}}},
  "_comment_simulation_price_variation": "we are going to simulate bump of -5%, -2.5%, 0%, 2.5% and 5% of the price and see what is the impact on the volume",
  "simulation_price_variation": [0.95, 0.975, 1, 1.025, 1.05],

  "_comment_min_weekly_rsd_quantity_for_pe_computation": "we won't compute a PE for (store,product) that have less than this amount of weekly qauntity",
  "min_weekly_rsd_quantity_for_pe_computation": 1
}
</file>

<file path="run_crosspe_fmc_mo_to">
import json
export_path = "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/"
export_path_for_reynolds = "s3://rai-eap-rgm-qa-us-east-1/insiteai/outputs"

max_predict_date = "2025-06-01"
main_folder = f"{max_predict_date[0:4]}/{max_predict_date[5:7]}"
def run_main(exp_name_suffix, config_name, run_download=0, run_density=0, run_eda=0, test=0, compute_cross_pe=1, export_suffix="", max_predict_date=max_predict_date, number_of_predictions = 1):
    dbutils.notebook.run("../notebooks/00 - PE Full Pass", 0, {
        "run_id": f"{main_folder}/{exp_name_suffix}",
        "export_path": export_path,
        "config_path": f"""/Workspace{"/".join(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[:-2])}/config/crosspe_fmc_mo_to/{config_name}.json""",
        "max_predict_date": max_predict_date,
        "number_of_predictions": number_of_predictions, # put several to run a stability analysis
        "compute_cross_pe": compute_cross_pe,
        "future_steps": 0,
        "run_pe_str": 0,
        "run_download": run_download,
        "run_density": run_density,
        "run_eda": run_eda,
        "crosspe_category_project": 1,
        "export_suffix": export_suffix,
    })

# COMMAND ----------


# FMC Manuel
exp_name_suffix = "fmc"
run_main(
    exp_name_suffix=exp_name_suffix,
    config_name="grizzly_velo",
    run_download=0,
    run_density=0,
    run_eda=0,
    export_suffix="_pe_manuel",
    compute_cross_pe=0,
    max_predict_date="2025-06-01",
    number_of_predictions=6,
)

</file>

<files>
<file path="notebooks/00 - PE Full Pass.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # PE Full Pass Notebook
# MAGIC
# MAGIC This notebook is the main notebook used to generate PE for FMC, Vapor, Modern Oral and Traditional Oral PE (MO and TO to be added).
# MAGIC
# MAGIC The main steps are:
# MAGIC  - "Get data": download RSD, STR, econometric, KPI data from Redshift
# MAGIC  - "Train ML model": Train the machine learning model and save it to mlflow on databricks
# MAGIC  - "Curve Simulation": Create simulations of price variations
# MAGIC  - "PE RSD Pass": Compute a PE for each RSD account
# MAGIC  - "Delete Curve Simulation files": Remove temporary files to keep the storage on s3 low
# MAGIC  - "PE STR Pass": Compute PE STR
# MAGIC  - "Future PE": Compute forecasted RSD PE

# COMMAND ----------

dbutils.widgets.removeAll()

# COMMAND ----------

dbutils.widgets.text("run_id", "2023/test") # id for the run, this should be unique or it will overwrite an existing run
dbutils.widgets.text("export_path", "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/")
dbutils.widgets.text("config_path", "../config/fmc_test.json")
dbutils.widgets.text("max_predict_date", "2023-03-01") # date for which we generate a pe (has to be a historical data, not a future one)
dbutils.widgets.text("number_of_predictions", "1") # '1' by default, this will compute pe for the max_predict_date, if we want to compute more historical PE increase this number (2 will compute pe for max_predict_date + the month before)
dbutils.widgets.text("compute_cross_pe", "0") # '1' to enable cross pe computation, '0' to disable
dbutils.widgets.text("future_steps", "0") # number of months for which to forecast pe
dbutils.widgets.text("run_pe_str", "1")
dbutils.widgets.text("run_download", "1")
dbutils.widgets.text("run_modeling", "1")
dbutils.widgets.text("run_density", "1")
dbutils.widgets.text("run_eda", "1")
dbutils.widgets.text("run_accuracy", "0")
dbutils.widgets.text("run_drift", "0")
dbutils.widgets.text("run_histogram", "0")
dbutils.widgets.text("crosspe_category_project", "0")
dbutils.widgets.text("dynamic_dates_alignment","0") # for period equal to max_predict date use dynamic dates to select data ranges
dbutils.widgets.text("delta", "1") # Add delta parameter, 1 = skip cleaning step 04.2
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

# MAGIC %run ./utils.py 

# COMMAND ----------

import json
import numpy as np
import pandas as pd

run_id = dbutils.widgets.get("run_id")
export_path = dbutils.widgets.get("export_path")
config_path = dbutils.widgets.get("config_path")
max_predict_date = dbutils.widgets.get("max_predict_date")
number_of_predictions = int(dbutils.widgets.get("number_of_predictions"))
compute_cross_pe = dbutils.widgets.get("compute_cross_pe")
future_steps = dbutils.widgets.get("future_steps")
run_pe_str = int(dbutils.widgets.get("run_pe_str"))
run_download = int(dbutils.widgets.get("run_download"))
run_density = int(dbutils.widgets.get("run_density"))
run_eda = int(dbutils.widgets.get("run_eda"))
run_histogram = int(dbutils.widgets.get("run_histogram"))
run_modeling = int(dbutils.widgets.get("run_modeling"))
run_accuracy = int(dbutils.widgets.get("run_accuracy"))
run_drift = int(dbutils.widgets.get("run_drift"))
crosspe_category_project = dbutils.widgets.get("crosspe_category_project")
dynamic_dates_alignment = int(dbutils.widgets.get("dynamic_dates_alignment"))
delta = int(dbutils.widgets.get("delta"))
export_suffix = dbutils.widgets.get("export_suffix")

predict_dates = pd.date_range(end=max_predict_date, periods=number_of_predictions, freq="MS", closed="right").date.astype(str)
with open(config_path, "r") as f:
    config = json.load(f)
history_start_date = config.get("history_start_date", "none") # first date from training dataset, 'none' for two years of history
# When dynamic alignment is enabled, derive history range from available data
if dynamic_dates_alignment == 1:
    history_start_date, dynamic_end_date = get_dynamic_start_end_dates(no_years=2)

elif history_start_date=='none':
    history_start_date = str((pd.to_datetime(max_predict_date)-pd.DateOffset(years=2)).date())
simulation_price_variation = config.get("simulation_price_variation")
min_weekly_rsd_quantity_for_pe_computation = config.get("min_weekly_rsd_quantity_for_pe_computation")
states = config.get("states")
products = config.get("products")
all_product_id = config.get(
    "rsd_product_id_to_download", # new method to download the list of products
    list(np.concatenate([[d.get("id")] + d.get("other") for d in config.get("products").values()]).flat) # old method to download the list of products
)
main_product_id = list(np.unique(np.array([[d.get("id")] for d in config.get("products").values()]).flat))
product_names = list(products.keys())
generate_data = config.get("generate_data", 0)
include_macroeconomic = config.get("include_macroeconomic") # Should be set to 1 in config file if macroeconomic features need to be included
get_by_length_level = config.get("cig_length", 0)
get_by_price_tier = config.get("price_tier", 0)
get_by_menthol = config.get("menthol", 0)
get_extra_num_feats_lgb = config.get("extra_num_feats_lgb", [])
get_extra_num_feats_lgb_industry = config.get("extra_num_feats_lgb_industry", 1)
if get_extra_num_feats_lgb_industry==1:
    get_extra_num_feats_lgb = (
        get_extra_num_feats_lgb +
        [f"rsd_quantity_industry_roll{r}_perc" for r in [2,4,8,13,26]] + [f"rsd_quantity_industry_roll2"]
    )
group_target_products = config.get("group_target_products", 0)
grouping = config.get("grouping", "")
grouped_targets = config.get('grouped_targets', "")
group_rsd_data = config.get("group_rsd_data", {})
cpu_or_serving = config.get("cpu_or_serving")
price_threshold = config.get("price_threshold")
hyperparameters_file = config.get("hyperparameters_file", "")
industry_id = config.get("industry_id")
industry_unique_id = config.get("industry_unique_id")
industry = config.get("industry")
additional_filters_redshift = config.get("additional_filters_redshift", "")
density_use_all_products = config.get("density_use_all_products")
density_cpu_threshold = config.get("density_cpu_threshold")
density_weeks_threshold = config.get("density_weeks_threshold")
catalog = config.get("catalog", 0)


print(f"States: {states}")
print(f"Predict date: {predict_dates}")
print(f"History start date: {history_start_date}")
print(f"Compute cross pe: {compute_cross_pe}")
print(f"Future steps: {future_steps}")
print(f"Include Macroeconomics: {include_macroeconomic}")
print(f"group_target_products: {group_target_products}")
print(f"grouped targets: {grouped_targets}")
print(f"cpu_or_serving: {cpu_or_serving}")
print(f"get_by_length_level: {get_by_length_level}")
print(f"get_by_price_tier: {get_by_price_tier}")
print(f"get_by_menthol: {get_by_menthol}")
print(f"get_extra_num_feats_lgb: {get_extra_num_feats_lgb}")
print(f"price_threshold: {price_threshold}")
print(f"industry_id: {industry_id}")
print(f"additional_filters_redshift: {additional_filters_redshift}")
print(f"industry: {industry}")
print(f"Dynamic dates alignment : {dynamic_dates_alignment}")
print(f"Delta: {delta}")

# COMMAND ----------

# DBTITLE 1,Get data
if run_download==1:
    if dynamic_dates_alignment == 1:
        start_date_1y, end_date = get_dynamic_start_end_dates(no_years = 1)
    else:
        start_date_1y = 'none'
        end_date = max_predict_date
        
    dbutils.notebook.run("01 - Get Data", 0, {
        "export_path": export_path,
        "run_id": run_id,
        "states": json.dumps(states),
        "rsd_product_ids": str(all_product_id),
        "str_product_ids": str(all_product_id),
        "start_date": history_start_date,
        "str_start_date": start_date_1y,
        "end_date": end_date, # Used in STR query - PE STR runs only for the last (max) date in the predict_dates list
        "generate_data": generate_data,
        "include_macroeconomic": include_macroeconomic,
        "group_target_products": group_target_products,
        "grouping": json.dumps(grouping),
        "group_rsd_data": json.dumps(group_rsd_data),
        "cpu_or_serving": cpu_or_serving,
        "industry_id": json.dumps(industry_id),
        "additional_filters_redshift": json.dumps(additional_filters_redshift),
        "industry": industry,
        "crosspe_category_project": crosspe_category_project,
        "catalog": catalog,
        "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,eda
if run_eda==1:
  dbutils.notebook.run("01.2 - EDA Data", 0, {
      "export_path": export_path,
      "run_id": run_id,
      "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,Regenerate product-competitor Dict in case of grouped data
if group_target_products:
    product_names = grouped_targets

    new_products = {}
    # Loop through low level producsts
    for relation in products.values():
        # Loop through groups
        for group, rel in grouping.items():
            # Look if the low level products is in this parent
            if relation['id'] in list(rel.values())[0]:
                # If parent already in the new dictionary just append the competitors
                if group in new_products.keys():
                    new_products[group]['other'] += relation['other']
                # If parent not in dict add it with it's competitors
                else:
                    new_products[group] = {'id' : [int(x) for x in list(rel.keys())][0], 'other' : relation['other']}
                    if "filter_training_lgb" in relation.keys():
                        new_products[group]["filter_training_lgb"] = relation.get("filter_training_lgb")
    # Remove duplicates in competitors
    for group in new_products.values():
        group['other'] = list(set(group['other']))
    
    products = new_products.copy()
    print(products)

    
    # reverse_grouping
    reverse_grouping = {}

    # Loop through new id and old ids that need to be agg
    for mapping in grouping.values():
        for group, old_prods in mapping.items():
            for old_prod in old_prods:
                if old_prod not in reverse_grouping.keys():
                    reverse_grouping[old_prod] = group
                else:
                    print("WARNING: A PROD IS IN MULTIPLE GROUPS")

    
    # Change competitors in the new products with the aggregated ones
    new_products = products.copy()

    # Loop through new id and old ids that need to be agg
    for mapping in new_products.values():
        new_list = []
        for competitor in mapping['other']:
            new_list.append(int(reverse_grouping.get(competitor, competitor)))
        mapping['other'] = list(set(new_list))

    products = new_products.copy()
    print(products)

# COMMAND ----------

# DBTITLE 1,Data density
# Use default values of parameters 
# if they are not provided in config
if density_cpu_threshold is None:
   density_cpu_threshold = 30
if density_weeks_threshold is None:
   density_weeks_threshold = 30 
if density_use_all_products is None:
   density_use_all_products = 0
if industry is None:
   industry = "None"

# # if run_density==1:
# #     # dbutils.notebook.run("01.3 - Data Density", 0, {
# #     #     "end_date": max_predict_date,
# #     #     "cpu_threshold": density_cpu_threshold,
# #     #     "weeks_threshold":density_weeks_threshold, 
# #     #     "main_products": str(main_product_id),
# #     #     "all_products": str(all_product_id),
# #     #     "group_target_products": group_target_products,
# #     #     "products_grouped":str(grouping).replace("\'", "\""),
# #     #     "industry": industry,
# #     #     "use_all_products": density_use_all_products   
# #     # })

if run_density==2:
    dbutils.notebook.run("01.3 - Data Density 2", 0, {
        "data_path": f"{export_path}{run_id}/data",
        "end_date": max_predict_date,
        "cpu_threshold": density_cpu_threshold,
        "weeks_threshold": density_weeks_threshold,
        "industry": industry,
        "all_products": str(all_product_id),
        "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,Price histograms
if run_histogram==1:
    dbutils.notebook.run("01.5 - EDA Histogram", 0, {
        "export_path": export_path,
        "run_id": f"{run_id}/{max_predict_date}",
        "delta": delta
    })

# COMMAND ----------

if run_eda==1:
    try:
        print(f"  - Passing export_path: {export_path}")
        print(f"  - Passing run_id: {run_id}")
        dbutils.notebook.run("01.4 - EDA Macrovariables", 0, {
            "export_path": export_path,
            "run_id": run_id,
            "delta": delta
        })
    except Exception as e:
        print(f"ERROR running Macrovariable EDA notebook: {e}")
        # raise e # Optionally uncomment to stop execution on error

# COMMAND ----------

# DBTITLE 1,Train ML model
result_ml_model_train = {}

for predict_date in predict_dates:
 model_name_date = predict_date
 if (dynamic_dates_alignment == 1) & (predict_date == max_predict_date):
   # When aligning dynamically for the most recent month, preserve the
   # historical start date but move the end date to the detected window
   predict_date = dynamic_end_date
 train_res = dbutils.notebook.run("02 - ML Model", 0, {
                                           "import_data_path": f"{export_path}{run_id}",
                                           "run_id": f"{run_id}{export_suffix}/{predict_date}",
                                           "product_names": json.dumps(product_names),
                                           "products": json.dumps(products),
                                           "industry_unique_id": industry_unique_id,
                                           "predict_date": predict_date,
                                           "history_start_date": history_start_date,
                                           "include_macroeconomic": include_macroeconomic,
                                           "price_threshold": price_threshold,
                                           "get_by_length_level": get_by_length_level,
                                           "get_by_price_tier": get_by_price_tier,
                                           "cpu_or_serving": cpu_or_serving,
                                           "hyperparameters_file": hyperparameters_file,
                                           "get_by_menthol": get_by_menthol,
                                           "get_extra_num_feats_lgb": json.dumps(get_extra_num_feats_lgb),
                                         })
 result_ml_model_train[model_name_date] = train_res
print(result_ml_model_train)

# COMMAND ----------

# DBTITLE 1,Combined step for pe computation

for predict_date in predict_dates:
    # The model URI is retrieved from the results of the ML model training step
    model_uri = json.loads(result_ml_model_train[predict_date]).get("model_uri")

    dbutils.notebook.run("../notebooks/03.1 Simulation RSD", 0, {
        # Common Parameters
        "run_id": f"{run_id}/{predict_date}",
        "export_path": export_path,
        "product_names": json.dumps(product_names),
        "states": json.dumps(states),
        "get_by_length_level": get_by_length_level,
        "get_by_price_tier": get_by_price_tier,
        "get_by_menthol": get_by_menthol,
        "delta": delta,
        "industry_unique_id": industry_unique_id,

        # Parameters from original "03 - Simulation"
        "import_data_path": f"{export_path}{run_id}",
        "model_uri": model_uri,
        "compute_cross_pe": compute_cross_pe,
        "simulation_price_variation": json.dumps(simulation_price_variation),
        "cpu_or_serving": cpu_or_serving,
        "price_threshold": price_threshold,
        "dynamic_dates_alignment": dynamic_dates_alignment if predict_date == max_predict_date else 0,

        # Parameters from original "04 - PE RSD"
        "products": json.dumps(products),
        "min_weekly_rsd_quantity": min_weekly_rsd_quantity_for_pe_computation,
        "predict_date": predict_date,
        "export_suffix": export_suffix
    })

# COMMAND ----------

# DBTITLE 1,Curve Simulation
# for predict_date in predict_dates:
#  model_uri = json.loads(result_ml_model_train[predict_date]).get("model_uri")

#  dbutils.notebook.run("03 - Simulation", 0, {
#      "import_data_path": f"{export_path}{run_id}",
#      "run_id": f"{run_id}/{predict_date}",
#      "product_names": json.dumps(product_names),
#      "states": json.dumps(states), # Not used at the moment
#      "export_path": export_path,
#      "model_uri": model_uri,
#      "compute_cross_pe": compute_cross_pe,
#      "simulation_price_variation": json.dumps(simulation_price_variation),
#      "get_by_length_level": get_by_length_level,
#      "get_by_price_tier": get_by_price_tier,
#      "cpu_or_serving": cpu_or_serving,
#      "price_threshold": price_threshold,
#      "get_by_menthol": get_by_menthol,
#      "dynamic_dates_alignment": dynamic_dates_alignment if predict_date == max_predict_date else 0,
#      "delta": delta 
#    })

# COMMAND ----------

# DBTITLE 1,PE RSD Pass
# for predict_date in predict_dates:
#  dbutils.notebook.run("04 - PE RSD", 0, {
#      "states": json.dumps(states),
#      "product_names": json.dumps(product_names),
#      "predict_date": predict_date,
#      "export_path": export_path,
#      "run_id": f"{run_id}/{predict_date}",
#      "products": json.dumps(products),
#      "min_weekly_rsd_quantity": min_weekly_rsd_quantity_for_pe_computation,
#      "get_by_length_level": get_by_length_level,
#      "get_by_price_tier": get_by_price_tier,
#      "get_by_menthol": get_by_menthol,
#      "delta": delta 
#    })

# COMMAND ----------

# DBTITLE 1,Delete Curve Simulation files (taking a lot of space on s3)
# # Conditionally run the cleaning step based on delta
# if delta != 1: 
#     dbutils.notebook.run("04.2 - Cleaning Curve Simulation Files", 0, {
#          "path_to_search": f"{export_path}{run_id}/"
#        })
# else: 
#     print("Skipping 04.2 - Cleaning Curve Simulation Files because delta=1")

# COMMAND ----------

# DBTITLE 1,PE STR Pass
if run_pe_str==1:
   for predict_date in predict_dates:
       dbutils.notebook.run("05 - PE STR", 0, {
           "import_data_path": f"{export_path}{run_id}",
           "run_id": f"{run_id}/{predict_date}",
           "export_path": export_path,
           "states": json.dumps(states), # Not used at the moment
           "product_names": json.dumps(product_names),
           "products": json.dumps(products),
           "predict_date": predict_date,
           "get_by_length_level": get_by_length_level,
           "get_by_price_tier": get_by_price_tier,
           "get_by_menthol": get_by_menthol,
           "industry": industry
           })

# COMMAND ----------

# DBTITLE 1,Evaluate
for predict_date in predict_dates:
 dbutils.notebook.run("06 - Evaluate", 0, {
     "pe_str": f"{export_path}{run_id}/{predict_date}/str_pe",
     "pe_rsd": f"{export_path}{run_id}/{predict_date}/computed_pe{export_suffix}",
     "data_path": f"{export_path}{run_id}/data/df_product.parquet",
     "cross_pe": compute_cross_pe,
     "delta": delta ,
     "industry_unique_id": industry_unique_id
   })

# COMMAND ----------

# DBTITLE 1,Visualize PE Trend
if len(predict_dates)>1 and run_pe_str==1:
 dbutils.notebook.run("07 - Visualize Trend", 0, {
     "pe_str": f"{export_path}/{run_id}{export_suffix}/*/str_pe/*.parquet"
   })

# COMMAND ----------

# DBTITLE 1,Future PE
if len(predict_dates)>=12:
 dbutils.notebook.run("08 - Future PE", 0, {
     "product_names": json.dumps(product_names),
     "export_path": export_path,
     "run_id": run_id,
     "products": json.dumps(products),
     "pe_file": f"{export_path}/{run_id}/*/computed_pe/*.parquet",
     "test_length": "0", # setup 1 to run it with 1 test date
     "future_steps": future_steps
   })

# COMMAND ----------

# DBTITLE 1,Generate Report
if len(predict_dates)>=12:
    dbutils.notebook.run("09 - Generate Report", 0, {
          "import_data_path": f"{export_path}{run_id}",
          "future_pe_file": f"{export_path}/{run_id}/forecasted_pe/*.parquet",
          "cross_pe_file": f"{export_path}/{run_id}/*/computed_pe/*.parquet",
          "min_weekly_quantity": min_weekly_rsd_quantity_for_pe_computation,
          "products": json.dumps(products),
          "predict_date": max_predict_date
        })

# COMMAND ----------

# DBTITLE 1,Accuracy
if run_accuracy == 1:
    try:
        dbutils.notebook.run(
            "../notebooks/other/accuracy_rsd_lgbm", 0,
            {
                "folder":          export_path,
                "run_id":          run_id,                 
                "max_predict_date":max_predict_date,
                "model_to_use":    "all",       
                "config_path":     config_path,
                "delta" : delta,
                "export_suffix" : export_suffix,
                "industry_unique_id" : industry_unique_id             
            }
        )
        print("accuracy_rsd_lgbm completed")
    except Exception as e:
        print("accuracy_rsd_lgbm failed:", e)

# COMMAND ----------

if run_drift == 1: 
    try:
        dbutils.notebook.run(
            "../notebooks/other/drift", 0,
            {
                "folder":      export_path,
                "run_id":      run_id,
                "product_list": str([d.get("id") for d in config.get("products").values()]),
                "industry_unique_id": industry_unique_id,
                "delta_input": 1,
                "export_suffix": export_suffix                        
            }
        )
        print("drift notebook completed")
    except Exception as e:
        print("drift notebook failed:", e)

</file>


Outputs for the 02 - ML Model.py
here are the outputs so you see for yourself
Processing GRIZZLY MO X 7MG min_data_in_leaf False ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'account_state'] -1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Train start date: 2023-06-01 00:00:00 Train end date: 2025-06-01 00:00:00 (22757, 33) (39641, 33) sales_week_forecast 2024-12-23 sales_week_start_date 2024-11-25 00:00:00 dtype: object Percentage of rows discarded: 4% Percentage of sales discarded: 4% size of testing dataset (15254, 33) (15727, 33) {'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt', 'monotone_constraints': '-1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0', 'monotone_constraints_method': 'advanced', 'seed': 42, 'n_estimators': 600, 'learning_rate': 0.1, 'bagging_fraction': 1, 'feature_fraction': 0.8, 'num_threads': -1} ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'account_state'] Training until validation scores don't improve for 25 rounds /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf51caa3-6ac7-4018-b4f9-6285998880c5/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found n_estimators in params. Will use it instead of argument 2025/07/25 06:37:51 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.6.0/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model. [25] training's l1: 0.450468 valid_1's l1: 0.611342 valid_2's l1: 0.5733 Early stopping, best iteration is: [10] training's l1: 0.463903 valid_1's l1: 0.608807 valid_2's l1: 0.568491
Processing VELO PLUS min_data_in_leaf False ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'account_state'] -1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Train start date: 2023-06-01 00:00:00 Train end date: 2025-06-01 00:00:00 /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. (841059, 33) (1156545, 33) sales_week_forecast 2024-12-16 sales_week_start_date 2024-11-18 00:00:00 dtype: object /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. Percentage of rows discarded: 5% Percentage of sales discarded: 2% size of testing dataset (260305, 33) (264801, 33) {'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt', 'monotone_constraints': '-1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0', 'monotone_constraints_method': 'advanced', 'seed': 42, 'n_estimators': 600, 'learning_rate': 0.1, 'bagging_fraction': 1, 'feature_fraction': 0.8, 'num_threads': -1} ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'account_state'] /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf51caa3-6ac7-4018-b4f9-6285998880c5/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found n_estimators in params. Will use it instead of argument Training until validation scores don't improve for 25 rounds [25] training's l1: 0.322075 valid_1's l1: 0.332356 valid_2's l1: 0.312432 [50] training's l1: 0.315757 valid_1's l1: 0.325447 valid_2's l1: 0.306211 [75] training's l1: 0.312937 valid_1's l1: 0.322764 valid_2's l1: 0.303906 [100] training's l1: 0.311239 valid_1's l1: 0.32114 valid_2's l1: 0.302598 [125] training's l1: 0.309984 valid_1's l1: 0.320246 valid_2's l1: 0.30188 [150] training's l1: 0.309049 valid_1's l1: 0.319713 valid_2's l1: 0.301527 Early stopping, best iteration is: [144] training's l1: 0.309221 valid_1's l1: 0.319638 valid_2's l1: 0.301393 2025/07/25 06:38:20 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.6.0/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model.
<file path="notebooks/02 - ML Model.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # ML Model Notebook
# MAGIC
# MAGIC Train a machine learning model for each product of interest and save it to mlflow on databricks.
# MAGIC
# MAGIC The goal of those models are to learn the dynamics of price vs quantity for each product at account level.

# COMMAND ----------

# dbutils.widgets.removeAll()
# dbutils.widgets.text("import_data_path", "/FileStore/pe/pe_model_dev")
# dbutils.widgets.text("run_id", "vapor_group_data_test")
# dbutils.widgets.text("product_names", '["VUSE ALTO POD MEN 4CART", "VUSE ALTO POD GOLDEN TOBACCO 2CART"]')
# dbutils.widgets.text("industry_unique_id", "1") 
# dbutils.widgets.text("predict_date", "2022-11-01") 
# dbutils.widgets.text("history_start_date", "2022-03-01")    # first date from training dataset, 'none' for one year history
# dbutils.widgets.text("products", """{"VUSE ALTO POD MEN 4CART": {"id": 99991, "other": [93932,83587, 61964]}, "VUSE ALTO POD GOLDEN TOBACCO 2CART": {"id": 99995, "other": [61957, 70336, 71957]}}""") 
# dbutils.widgets.text("include_macroeconomic", "0")
# dbutils.widgets.text("get_by_length_level", "0")
# dbutils.widgets.text("get_by_price_tier", "0")
# dbutils.widgets.text("get_by_menthol", "0")
# dbutils.widgets.text("cpu_or_serving", "cpu")
# dbutils.widgets.text("hyperparameters_file", "none")
# dbutils.widgets.text("get_extra_num_feats_lgb", "")
# dbutils.widgets.text("price_threshold", "1")

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

spark.conf.set("spark.sql.execution.arrow.enabled", "true")
spark.conf.set("spark.sql.execution.arrow.fallback.enabled", "false")

# COMMAND ----------

import json

import_data_path = dbutils.widgets.get("import_data_path")
run_id = dbutils.widgets.get("run_id")
product_names = json.loads(dbutils.widgets.get("product_names"))
predict_date = dbutils.widgets.get("predict_date")
history_start_date = dbutils.widgets.get("history_start_date")
products = json.loads(dbutils.widgets.get("products")) 
industry_unique_id = int(dbutils.widgets.get("industry_unique_id")) 
macroeconomics = int(dbutils.widgets.get("include_macroeconomic"))
get_by_length_level = int(dbutils.widgets.get("get_by_length_level")) 
get_by_price_tier = int(dbutils.widgets.get("get_by_price_tier")) 
cpu_or_serving = dbutils.widgets.get("cpu_or_serving")
price_threshold = dbutils.widgets.get("price_threshold")
hyperparameters_file = dbutils.widgets.get("hyperparameters_file")
get_by_menthol = int(dbutils.widgets.get("get_by_menthol"))
get_extra_num_feats_lgb = json.loads(dbutils.widgets.get("get_extra_num_feats_lgb"))

print(f"Import path data: {import_data_path}")
print(f"Run id: {run_id}")
print(f"Products: {product_names}")
print(f"Predict date: {predict_date}")
print(f"History start date: {history_start_date}")
print(f"Products: {products}")
print(f"Hyperparameters_file: {hyperparameters_file}")
print(f"price_threshold: {price_threshold}")

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

# MAGIC %run ./utils_plot.py

# COMMAND ----------

import pandas as pd

train_end_date = pd.to_datetime(predict_date)

# Compute the start date for training dataset.
if history_start_date != 'none':
  train_start_date = pd.to_datetime(history_start_date)
else:
  train_start_date = pd.to_datetime(str((train_end_date - pd.Timedelta("2y")).date()))

# hyperparameters
if len(hyperparameters_file) > 0 and hyperparameters_file != "none":
    with open(f"../config/{hyperparameters_file}.json") as hyperparameter_file:
        hyperparameters = json.load(hyperparameter_file)
else:
    hyperparameters = None

default_hyperparameters = {'metric': 'mae',
          'verbosity': -1,
          'boosting_type': 'gbdt',
          'n_estimators': 600,
          'learning_rate': 0.1,
          'bagging_fraction': 1,
          'feature_fraction': 0.8,
          # 'num_threads': -1,
          'num_threads': 10,
          'monotone_constraints_method': 'advanced',
          'seed': 42}


# COMMAND ----------

# DBTITLE 1,Train Model
import lightgbm as lgb
import mlflow.lightgbm
from mlflow.tracking import MlflowClient
from pyspark.sql.types import FloatType
from pyspark.sql.functions import col, when, log1p
from functools import reduce
from pyspark.sql import DataFrame
import plotly.express as px

class LGBMWrapper(mlflow.pyfunc.PythonModel):
  """
  Wrapper to store multiple models in one pyfunc model
  """
  def __init__(self, models):
    self.models = models

  def predict(self, context, model_input):
    results = []
    
    for model in self.models:
      product_lineup_sk = model['product_lineup_sk']
      estimator = model['estimator']
      enable_features = model['enable_features']
      cat_features = model['categorical_features']
      
      product_model_input = (model_input[model_input['product_lineup_sk'] == product_lineup_sk])
      
      if product_model_input.empty:
        continue

      for c, categories in cat_features.items():
          product_model_input[c] = (product_model_input[c].astype("category")).cat.set_categories(categories)
        
      product_model_input['forecast'] = estimator.predict(product_model_input[enable_features])
      results.append(product_model_input[['forecast']])

    results_df = pd.concat(results)
    model_input = model_input.join(results_df, how='left')

    return model_input['forecast'].to_numpy()

# set experiment workspace
mlflow.set_experiment(f'/Shared/experiments/pe')

#open mlflow.autolog() to log everythings default during the training
mlflow.autolog(disable=True, log_models=False)

with mlflow.start_run(run_name=run_id) as run:
  """ Modeling step.
  Goes product by product.  
  Creates the training dataset based on the raw data.  
  Trains the LGBoost model and stores it with its metadata in mlflow. 
  """
  mlflow.log_param("run_id", run_id)

  estimators = []
  est_parameters = {}
  est_importance = {}

  # Initial Set Up
  for product_name in product_names:
    print(f"Processing {product_name}")
    name = product_name.lower().replace(" ","").replace("/", "")
    product_lineup_sk = products.get(product_name).get("id")
    l_product_lineup_sk = products.get(product_name).get("other")
    filter_training_lgb = products.get(product_name).get("filter_training_lgb")
    if filter_training_lgb==None:
        filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
            and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
            and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
            and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
            and (data_points >= 10)
            and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""
    if "hyperparameters" in products.get(product_name).keys() and "min_data_in_leaf" in products.get(product_name).get("hyperparameters"):
        min_data_in_leaf = products.get(product_name).get("hyperparameters").get("min_data_in_leaf")
    else:
        min_data_in_leaf = False
    print("min_data_in_leaf", min_data_in_leaf)
    estimator_name = f"model_{name}"

    #Features
    
    cat_feats, num_feats, additional_cols, target_col = get_features_set(product_lineup_sk, l_product_lineup_sk, macroeconomics, get_by_length_level, get_by_price_tier, get_by_menthol, get_extra_num_feats_lgb)
    if "remove_feature" in products.get(product_name).keys():  # add feature in the config file if you want to not use that feature
      for f in products.get(product_name).get("remove_feature"):
        num_feats = [c for c in num_feats if c!=f]
        cat_feats = [c for c in cat_feats if c!=f]
        if f not in additional_cols:
          additional_cols.append(f)
    enable_features = num_feats + cat_feats    

    number_of_cross_products = len(l_product_lineup_sk)

    #Creating Data
    create_training_data_view(product_lineup_sk, l_product_lineup_sk, import_data_path, 
        macroeconomics, price_threshold, gb_price_tier=get_by_price_tier, gb_cig_length=get_by_length_level, cpu_or_serving=cpu_or_serving, gb_menthol=get_by_menthol, industry_unique_id=industry_unique_id)
    columns = list(dict.fromkeys(cat_feats + num_feats + additional_cols + [target_col]))


    train_input_df = (
        spark.sql(f"select *, case when {filter_training_lgb} then 1 else 0 end no_outlier from train_vw where sales_week_forecast>='{train_start_date}'")
        .select(*columns)
    )

    decimals_cols = [x for x in train_input_df.columns if 'Decimal' in str(train_input_df.schema[x].dataType)]
    for columns in decimals_cols:
        train_input_df = train_input_df.withColumn(columns, train_input_df[columns].cast(FloatType()))

    agg_features = None 
    if "agg_features" in products.get(product_name).keys():
      agg_features = products[product_name]["agg_features"]
      for new_col, expr_template in agg_features.items():
        train_input_df = train_input_df.withColumn(new_col, eval(expr_template))
        enable_features.append(new_col)
    
    # Ensure the conversion to Pandas happens BEFORE this block
    train_input_df = train_input_df.toPandas()

    # if product_name == "MODERN ORAL INDUSTRY":
    #     # Create smoothed price roll4
    #     train_input_df['rsd_price_smooth_roll4'] = (
    #         train_input_df
    #           .groupby(['account_sk'])['rsd_price']
    #           .transform(lambda x: x.ewm(span=8).mean().rolling(window=4, min_periods=1).mean())
    #     )

    #     # Calculate smoothed percentage change
    #     rsd_price_smooth_perc = (
    #         train_input_df['rsd_price_smooth_roll4'] /
    #         train_input_df.groupby(['account_sk'])['rsd_price_smooth_roll4'].shift(4)
    #     )
        
    #     # Revert to your original logic by REMOVING .fillna(1.0)
    #     # This will keep the NaNs in the data
    #     train_input_df['rsd_price_perc'] = rsd_price_smooth_perc

    #     # Drop the temporary column
    #     train_input_df.drop('rsd_price_smooth_roll4', axis=1, inplace=True)

    #     # Clip values to within the 10th–90th state quantiles
    #     # This now operates on data with NaNs
    #     q_low  = train_input_df.groupby('account_state')['rsd_price_perc']\
    #                 .transform(lambda s: s.quantile(0.10))
    #     q_high = train_input_df.groupby('account_state')['rsd_price_perc']\
    #                 .transform(lambda s: s.quantile(0.90))
    #     train_input_df['rsd_price_perc'] = train_input_df['rsd_price_perc'].clip(lower=q_low, upper=q_high)
    # else:
    #     pass

    categorical_features = {}
    for c in cat_feats:
        train_input_df[c] = train_input_df[c].astype("category")
        categorical_features[c] = list(train_input_df[c].cat.categories.values)

    # enable_features = enable_features + ["price_tier_int","price_tier_int_log"]

    print(enable_features)

    # The hyperparameter tuning of the model was done with Optuna 
    # (An open source hyperparameter optimization framework to automate hyperparameter search)
    
    if get_by_price_tier ==1:
      monotone_dict = {
      "price_tier_int": 1
      } 
    else:
      monotone_dict = {}

    params = {'metric': 'mae',
          'verbosity': -1,
          'boosting_type': 'gbdt',
          'monotone_constraints': f"-1,{','.join(['1']*number_of_cross_products)},{','.join([str(monotone_dict.get(feat, 0)) for feat in enable_features[number_of_cross_products + 1:]])}",
          'monotone_constraints_method': 'advanced',
          'seed': 42}
    
    print(f"-1,{','.join(['1']*number_of_cross_products)},{','.join([str(monotone_dict.get(feat, 0)) for feat in enable_features])}")
    
    if hyperparameters:
        params.update(hyperparameters.get(estimator_name,default_hyperparameters))
    else:
        params.update(default_hyperparameters)

    # Weight up competitor products prices and weight down other features
    if "hyperparameters" in products.get(product_name).keys() and "features_contri" in products.get(product_name).get("hyperparameters"):
      features_contri = [1.0] * len(enable_features)
      for feat in enable_features:
        if not(feat.endswith("rsd_price_perc")):
          features_contri[enable_features.index(feat)] = products.get(product_name).get("hyperparameters")["features_contri"]
      params["feature_contri"] = features_contri

    #for c in enable_features+[target_col]: # this rounding makes the new PE closer to the old one
    #    train_input_df[c] = train_input_df[c].astype("float32").round(2)
    train_input_df["sales_week_start_date"] = pd.to_datetime(train_input_df["sales_week_start_date"])

    # Plot data
    df_plot_full = aggregate_data_for_plot(train_input_df, no_outliers=False)
    create_quantity_price_perc_plot(df_plot_full, title=f"{product_name} - Quantity change vs price change - All Data")
    df_plot_filtered = aggregate_data_for_plot(train_input_df, no_outliers=True)
    create_quantity_price_perc_plot(df_plot_filtered, title=f"{product_name} - Quantity change vs price change - No Outliers")
    df_plot_filtered = train_input_df.loc[train_input_df.no_outlier >= 1].groupby("sales_week_forecast")[["rsd_price_perc", "rsd_quantity_perc"]].mean().reset_index()
    create_quantity_price_perc_plot(df_plot_filtered, title=f"{product_name} - Quantity change vs price change (mean aggregation) - No Outliers")


    train_mask, test_mask, test_clean_mask = split_train_test(train_input_df, train_start_date, train_end_date)
    print(train_input_df[train_mask].shape, train_input_df.shape)
    print(train_input_df[train_mask][["sales_week_forecast","sales_week_start_date"]].min())
    print(f"Percentage of rows discarded: {int(100*(1-train_input_df[train_mask].shape[0]/train_input_df[(train_input_df.sales_week_forecast < train_end_date) & (train_input_df.sales_week_forecast >= train_start_date)].shape[0]))}%")
    print(f"Percentage of sales discarded: {int(100*(1-train_input_df[train_mask]['rsd_quantity_roll4'].sum()/train_input_df[(train_input_df.sales_week_forecast < train_end_date) & (train_input_df.sales_week_forecast >= train_start_date)]['rsd_quantity_roll4'].sum()))}%")

    print("size of testing dataset", train_input_df[test_clean_mask].shape, train_input_df[test_mask].shape)
    
    if train_input_df.shape[0]==0:
      continue
    

    # Uncomment to inspect basic statistics of the training subset
    # print(train_input_df[train_mask][["rsd_price_perc", "rsd_quantity_perc"]].describe())

    # for better stability and more accurate results in the model, we add more regularization for the following products
    # 3308, 3448, 30348, 30360, 32399, 36746, 63261, 78746
    # if train_input_df[train_mask].shape[0]>1000000 and (product_lineup_sk in [] or min_data_in_leaf):
    if min_data_in_leaf:
        print("add param min_data_in_leaf")
        if "hyperparameters" in products.get(product_name).keys() and "perc_min_data_in_leaf" in products.get(product_name).get("hyperparameters"):
          params["min_data_in_leaf"] = int(products.get(product_name).get("hyperparameters")["perc_min_data_in_leaf"]*train_input_df[train_mask].shape[0])
        else:
          params["min_data_in_leaf"] = int(0.05*train_input_df[train_mask].shape[0])
        print("params[min_data_in_leaf]", params["min_data_in_leaf"])
    if "hyperparameters" in products.get(product_name).keys():
      for p in products.get(product_name).get("hyperparameters"):
        if p in ["bagging_fraction", "n_estimators", "learning_rate", "feature_fraction","early_stopping_rounds", "max_depth", "min_gain_to_split", "min_data_per_group", "num_threads", "min_split_gain", "lambda_l2", "lambda_l1", "boosting_type", "metric", "huber_delta", "num_leaves", "max_bin", "min_data_in_bin", "drop_rate", "max_drop", "skip_drop", "bagging_freq",
          "feature_fraction_bynode", "path_smooth"]:
          params[p] = products.get(product_name).get("hyperparameters")[p]

    if "hyperparameters" in products.get(product_name).keys() and "low_weight" in products.get(product_name).get("hyperparameters"):
      train_input_df["deviation"] = np.maximum(train_input_df[target_col], 1/train_input_df[target_col])
      k = train_input_df[(train_input_df.no_outlier==True)]["deviation"].quantile(products.get(product_name).get("hyperparameters").get("low_weight"))
      train_input_df.loc[(train_input_df["deviation"] <= k) & (train_mask), "weight"] = 0.1
    if "hyperparameters" in products.get(product_name).keys() and "clip_target" in products.get(product_name).get("hyperparameters"):
      c = products.get(product_name).get("hyperparameters").get("clip_target")
      train_input_df.loc[train_mask, "rsd_quantity_perc"] = train_input_df.loc[train_mask, "rsd_quantity_perc"].clip(lower=1/c, upper=c)
    
    print(params)
    print(enable_features)
    
    train_data = lgb.Dataset(train_input_df[train_mask][enable_features],
                               label=train_input_df[train_mask][target_col],
                               weight=train_input_df[train_mask]["weight"],
                               categorical_feature=cat_feats,
                               free_raw_data=False)

    test_data = lgb.Dataset(train_input_df[test_mask][enable_features],
                               label=train_input_df[test_mask][target_col],
                               #weight=train_input_df[test_mask]["weight"], # In the old code we don't set weights for test dataset
                               categorical_feature=cat_feats,
                               free_raw_data=False)

    test_clean_data = lgb.Dataset(train_input_df[test_clean_mask][enable_features],
                               label=train_input_df[test_clean_mask][target_col],
                               #weight=train_input_df[test_clean_mask]["weight"], # In the old code we don't set weights for test dataset
                               categorical_feature=cat_feats,
                               free_raw_data=False)
    
    callbacks = [lgb.log_evaluation(period=25)]
    if "hyperparameters" in products.get(product_name).keys() and "min_delta" in products.get(product_name).get("hyperparameters"):
      callbacks.append(
        lgb.early_stopping(stopping_rounds=25, first_metric_only=False, min_delta=products.get(product_name).get("hyperparameters").get("min_delta"))
      )

    estimator = lgb.train(params,
                        train_data,
                        valid_sets = [train_data, test_data, test_clean_data] if test_data.data.shape[0]>0 else [train_data],
                        # verbose_eval = 25,
                        callbacks = callbacks
                        )
   

    parameters = {
      "product_lineup_sk": str(product_lineup_sk),
      "l_product_lineup_sk": l_product_lineup_sk,
      "train_start_date": str(train_start_date.date()),
      "train_end_date": str(train_end_date.date()),
      "model_params": params,
      "cat_feats": cat_feats,
      "num_feats": num_feats,
      "remove_feats": products.get(product_name).get("remove_feature"),
      "agg_feats" : agg_features,
      "additional_cols": additional_cols,
      "target_col": target_col,
      "include_macroeconomics": str(macroeconomics),
      "cpu_or_serving": cpu_or_serving,
      "price_threshold": str(price_threshold) , 
      "gb_price_tier": str(get_by_price_tier), 
      "gb_cig_length": str(get_by_length_level), 
      "gb_menthol": str(get_by_menthol),
      "stat" : {
        "mean(rsd_quantity_roll4)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].mean()), 
        "mean(rsd_price_perc)" : str(train_input_df[train_mask]["rsd_price_perc"].mean()),
        "rsd_quantity_roll4_quantile(10)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].quantile(0.1)),
        "rsd_quantity_roll4_quantile(90)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].quantile(0.9)),
        "rsd_price_perc_quantile(10)" : str(train_input_df[train_mask]["rsd_price_perc"].quantile(0.1)),
        "rsd_price_perc_quantile(90)" : str(train_input_df[train_mask]["rsd_price_perc"].quantile(0.9)),
      }
    }

    mlflow.lightgbm.log_model(estimator, estimator_name)

    estimators.append({"estimator": estimator,
                       "model_name": estimator_name,
                       "enable_features": enable_features,
                       "categorical_features": categorical_features,
                       "product_lineup_sk": product_lineup_sk,
                       "l_product_lineup_sk": l_product_lineup_sk})
    est_parameters[estimator_name] = parameters

    df_importance = pd.DataFrame({
        "feature": estimator.feature_name(),
        "importance_gain": estimator.feature_importance(importance_type="gain"),
        "importance_split": estimator.feature_importance(importance_type="split"),
    }).sort_values("importance_gain", ascending=False)
    display(df_importance)
    est_importance[estimator_name] = df_importance.to_json(orient="records")

    # Get feature importance
    importance = estimator.feature_importance(importance_type='gain')  # or 'split'
    features = estimator.feature_name()
    # Create a DataFrame
    fi_df = pd.DataFrame({
        'Feature': features,
        'Importance': importance
    }).sort_values(by='Importance', ascending=False)
    # Plot with Plotly
    fig = px.bar(
        fi_df,
        x='Importance',
        y='Feature',
        orientation='h',
        title='LightGBM Feature Importance (by Gain)',
        labels={'Importance': 'Importance Score', 'Feature': 'Feature'},
        height=600
    )
    fig.update_layout(yaxis={'categoryorder':'total ascending'})
    fig.show()

    
  mlflow.log_dict({
    "parameters": est_parameters
  }, "parameters.json")
  mlflow.log_dict({
    "importance": est_importance
  }, "importance.json")

  wrappedModel = LGBMWrapper(estimators)
  mlflow.pyfunc.log_model(artifact_path="main_model", python_model=wrappedModel)

# COMMAND ----------

# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# from pyspark.sql.types import FloatType # For casting if needed again

# FMC_PRODUCT_ID_FOR_PLOT = 1  # CIG INDUSTRY
# REQUESTED_OUTLIER_THRESHOLD_RATIO = 2.0
# MAX_PLOTS_TO_GENERATE = 5

# print(f"--- Preparing data for Ad-hoc Plotting for Product ID: {FMC_PRODUCT_ID_FOR_PLOT} ---")
# target_product_name_for_plot = None
# target_product_details = None

# for p_name, p_details in products.items():
#     if p_details.get("id") == FMC_PRODUCT_ID_FOR_PLOT:
#         target_product_name_for_plot = p_name
#         target_product_details = p_details
#         break

# if target_product_details:
#     print(f"Target product: {target_product_name_for_plot}")
#     current_l_product_lineup_sk = target_product_details.get("other", [])
    
#     create_training_data_view(
#         product_lineup_sk=FMC_PRODUCT_ID_FOR_PLOT, 
#         l_product_linup_sk=current_l_product_lineup_sk, 
#         export_path=import_data_path, # Using import_data_path as it's used for reading
#         macroeconomics=macroeconomics, 
#         price_threshold=price_threshold, 
#         gb_price_tier=get_by_price_tier, 
#         gb_cig_length=get_by_length_level, 
#         gb_menthol=get_by_menthol, 
#         cpu_or_serving=cpu_or_serving
#     )
#     print(f"Re-created train_vw for Product ID {FMC_PRODUCT_ID_FOR_PLOT}")

#     # Re-run get_features_set for the target product ---
#     current_cat_feats, current_num_feats, current_additional_cols, current_target_col = get_features_set(
#         product_lineup_sk=FMC_PRODUCT_ID_FOR_PLOT, 
#         l_product_lineup_sk=current_l_product_lineup_sk, 
#         macroeconomics=macroeconomics, 
#         get_by_length_level=get_by_length_level, 
#         get_by_price_tier=get_by_price_tier, 
#         get_by_menthol=get_by_menthol,
#         get_extra_num_feats_lgb=get_extra_num_feats_lgb # from notebook params
#     )
    
#     # Re-run Spark SQL to get the DataFrame for the target product ---
#     # Get the filter_training_lgb specific to this product
#     current_filter_training_lgb = target_product_details.get("filter_training_lgb")
#     if current_filter_training_lgb is None: # Default filter if not specified
#         current_filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
#             and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
#             and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
#             and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
#             and (data_points >= 10)
#             and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""

#     # train_start_date should be available from the main notebook parameters
#     adhoc_train_df_spark = (
#         spark.sql(f"select *, case when {current_filter_training_lgb} then 1 else 0 end no_outlier from train_vw where sales_week_forecast>='{train_start_date}'")
#         .select(*current_cat_feats, *current_num_feats, *current_additional_cols, current_target_col)
#     )
    
#     adhoc_decimals_cols = [x for x in adhoc_train_df_spark.columns if 'Decimal' in str(adhoc_train_df_spark.schema[x].dataType)]
#     for col_name in adhoc_decimals_cols: # Renamed 'columns' to 'col_name'
#         adhoc_train_df_spark = adhoc_train_df_spark.withColumn(col_name, adhoc_train_df_spark[col_name].cast(FloatType()))

#     current_product_df_for_plot = adhoc_train_df_spark.toPandas()
    
#     if "sales_week_start_date" in current_product_df_for_plot.columns:
#         current_product_df_for_plot["sales_week_start_date"] = pd.to_datetime(current_product_df_for_plot["sales_week_start_date"])
#     else:
#         print("CRITICAL ERROR: 'sales_week_start_date' is missing after re-generating data. Plotting cannot proceed.")
#         current_product_df_for_plot = pd.DataFrame() # Empty df to skip plotting

#     print(f"Re-generated Pandas DataFrame for {target_product_name_for_plot}. Shape: {current_product_df_for_plot.shape}")

#     # Proceed with plotting logic (copied from previous iteration) ---
#     if not current_product_df_for_plot.empty and \
#        all(col in current_product_df_for_plot.columns for col in ['rsd_quantity_y', 'rsd_quantity_roll4', 'account_sk', 'sales_week_start_date']):
        
#         # Ensure numeric types (should be handled by toPandas and casting, but good for safety)
#         current_product_df_for_plot['rsd_quantity_y'] = pd.to_numeric(current_product_df_for_plot['rsd_quantity_y'], errors='coerce')
#         current_product_df_for_plot['rsd_quantity_roll4'] = pd.to_numeric(current_product_df_for_plot['rsd_quantity_roll4'], errors='coerce')
#         current_product_df_for_plot.dropna(subset=['rsd_quantity_y', 'rsd_quantity_roll4'], inplace=True)

#         plot_df_for_ratio_calc = current_product_df_for_plot[current_product_df_for_plot['rsd_quantity_roll4'] > 0.001].copy()
#         print(f"Shape of plot_df_for_ratio_calc (after rsd_quantity_roll4 > 0.001 filter & dropna): {plot_df_for_ratio_calc.shape}")

#         if not plot_df_for_ratio_calc.empty:
#             plot_df_for_ratio_calc['quantity_ratio'] = plot_df_for_ratio_calc['rsd_quantity_y'] / plot_df_for_ratio_calc['rsd_quantity_roll4']
            
#             stores_with_potential_outliers = plot_df_for_ratio_calc[
#                 plot_df_for_ratio_calc['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO
#             ]['account_sk'].unique()
#             print(f"Found {len(stores_with_potential_outliers)} store(s) with data points where quantity_ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO}.")
            
#             good_candidates_for_plotting = []
            
#             if len(stores_with_potential_outliers) > 0:
#                 print("Evaluating them against heuristic criteria...")
#                 for store_key_candidate in stores_with_potential_outliers:
#                     store_specific_data = plot_df_for_ratio_calc[plot_df_for_ratio_calc['account_sk'] == store_key_candidate].copy()
#                     outlier_data_points = store_specific_data[store_specific_data['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO]
#                     non_outlier_data_points = store_specific_data[store_specific_data['quantity_ratio'] <= REQUESTED_OUTLIER_THRESHOLD_RATIO]
                    
#                     MIN_TOTAL_WEEKS_CRITERIA = 20; MIN_OUTLIER_WEEKS_CRITERIA = 1
#                     MAX_OUTLIER_FRACTION_CRITERIA = 0.3; MAX_CV_NON_OUTLIERS_CRITERIA = 0.5 

#                     if (len(store_specific_data) >= MIN_TOTAL_WEEKS_CRITERIA and
#                         len(outlier_data_points) >= MIN_OUTLIER_WEEKS_CRITERIA and
#                         (len(outlier_data_points) / len(store_specific_data)) <= MAX_OUTLIER_FRACTION_CRITERIA and
#                         not non_outlier_data_points.empty and 'rsd_quantity_y' in non_outlier_data_points.columns):
#                         mean_qty_non_outlier = non_outlier_data_points['rsd_quantity_y'].mean()
#                         std_qty_non_outlier = non_outlier_data_points['rsd_quantity_y'].std()
#                         coeff_variation = float('inf'); is_heuristic_match = False
#                         if pd.notna(mean_qty_non_outlier) and pd.notna(std_qty_non_outlier):
#                             if mean_qty_non_outlier > 0: 
#                                 coeff_variation = std_qty_non_outlier / mean_qty_non_outlier
#                                 if coeff_variation < MAX_CV_NON_OUTLIERS_CRITERIA: is_heuristic_match = True
#                             elif mean_qty_non_outlier == 0 and std_qty_non_outlier == 0 : 
#                                 coeff_variation = 0.0; is_heuristic_match = True
#                         if is_heuristic_match:
#                             good_candidates_for_plotting.append({"account_sk": store_key_candidate, "cv": coeff_variation, 
#                                                                  "total_weeks": len(store_specific_data), "outlier_weeks": len(outlier_data_points)})
            
#             good_candidates_for_plotting.sort(key=lambda x: (x['cv'], -x['total_weeks']))
#             selected_stores_to_plot_info = good_candidates_for_plotting[:MAX_PLOTS_TO_GENERATE] 

#             if not selected_stores_to_plot_info:
#                 print(f"  Could not find any stores that meet the specified heuristic criteria for plotting for {target_product_name_for_plot} (ID: {FMC_PRODUCT_ID_FOR_PLOT}).")
#             else:
#                 print(f"\nFound {len(selected_stores_to_plot_info)} suitable store(s). Plotting top {len(selected_stores_to_plot_info)}:")
#                 for store_info in selected_stores_to_plot_info:
#                     selected_store_account_key_for_plot = store_info['account_sk']
#                     print(f"\n--- Plotting Store (account_sk): {selected_store_account_key_for_plot} ---")
#                     print(f"    CV: {store_info['cv']:.2f}, Total Weeks: {store_info['total_weeks']}, Outlier Weeks: {store_info['outlier_weeks']}")
#                     store_to_plot_df = plot_df_for_ratio_calc[plot_df_for_ratio_calc['account_sk'] == selected_store_account_key_for_plot].copy()
#                     store_to_plot_df.sort_values('sales_week_start_date', inplace=True)
#                     store_to_plot_df['is_outlier_for_plot'] = store_to_plot_df['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO
#                     plt.figure(figsize=(18, 9))
#                     sns.lineplot(data=store_to_plot_df, x='sales_week_start_date', y='rsd_quantity_y', marker='o', linestyle='-', label='Weekly Sales (rsd_quantity_y)', color='dodgerblue', zorder=1, markersize=5)
#                     outlier_points_for_plot_display = store_to_plot_df[store_to_plot_df['is_outlier_for_plot']]
#                     if not outlier_points_for_plot_display.empty:
#                         sns.scatterplot(data=outlier_points_for_plot_display, x='sales_week_start_date', y='rsd_quantity_y', color='red', s=150, edgecolor='black', label=f'Outlier (ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO})', zorder=3)
#                     sns.lineplot(data=store_to_plot_df, x='sales_week_start_date', y='rsd_quantity_roll4', marker='.', linestyle='--', label='4-Week Rolling Avg Sales (rsd_quantity_roll4)', color='grey', zorder=2, markersize=4)
#                     plt.title(f"Weekly Sales for Store {selected_store_account_key_for_plot} | Product: {target_product_name_for_plot} (ID {FMC_PRODUCT_ID_FOR_PLOT})\nHighlighting Ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO} | CV (non-outlier): {store_info['cv']:.2f}", fontsize=16)
#                     plt.xlabel("Sales Week Start Date", fontsize=14); plt.ylabel("Quantity", fontsize=14)
#                     plt.legend(title="Legend", title_fontsize='12', fontsize='10', loc='upper left'); plt.grid(True, linestyle=':', alpha=0.6)
#                     plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()
#         else: 
#             print(f"  No data available for {target_product_name_for_plot} after initial filtering for ratio calculation.")
#     else:
#         print(f"  Could not re-generate data for {target_product_name_for_plot} or critical columns are missing.")
#     print(f"--- End of Ad-hoc Plotting for Product ID: {FMC_PRODUCT_ID_FOR_PLOT} ---\n")
# else:
#     print(f"ERROR: Product ID {FMC_PRODUCT_ID_FOR_PLOT} not found in the 'products' configuration. Cannot proceed with ad-hoc plotting.")

# COMMAND ----------

dbutils.notebook.exit(json.dumps({"success": True, "model_uri": f"runs:/{run.info.run_id}/main_model"}))

# COMMAND ----------

# DBTITLE 1,test simulation
# x_values = [0.95, 0.975, 1.025, 1.05]
# # x_values = [1.01].query("account_state=='NC'")
# results = []

# for x in x_values:
#     for price_tier in train_input_df.price_tier.unique():
#         df_sim = (train_input_df[train_mask]
#         .query(f"price_tier=='{price_tier}'").copy()
#         )
#         if df_sim.shape[0]==0:
#             continue
#         df_sim["rsd_price_perc"] = x

#         y_pred = estimator.predict(df_sim[enable_features])
#         df_sim["prediction"] = y_pred
#         df_sim["rsd_price_perc"] = 1.0
#         y_pred = estimator.predict(df_sim[enable_features])
#         df_sim["prediction_no_price_change"] = y_pred

#         weighted_avg = (df_sim["prediction"] * df_sim["rsd_quantity_roll4"]).sum() / (df_sim["prediction_no_price_change"]* df_sim["rsd_quantity_roll4"]).sum()
#         results.append({"rsd_price_perc_simulated": x, "weighted_avg_prediction": weighted_avg, "price_tier": price_tier})

# df_test = pd.DataFrame(results)
# df_test["approx_elasticity"] = (df_test["weighted_avg_prediction"] - 1) / (df_test["rsd_price_perc_simulated"] - 1)

# df_test

# COMMAND ----------

# DBTITLE 1,Hyperparameters Optimization
# def run_optuna_tuning(train_input_df, enable_features, target_col, weight_col, cat_feats,
#                       train_mask, test_mask, test_clean_mask, number_of_cross_products, n_trials=20):
#     import optuna
#     from sklearn.metrics import mean_absolute_error
#     import lightgbm as lgb

#     def objective(trial):
#         params = {
#             'boosting_type': 'gbdt',
#             'metric': 'mae',
#             'verbosity': -1,
#             'n_estimators': trial.suggest_int('n_estimators', 100, 800),
#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#             'num_leaves': trial.suggest_int('num_leaves', 20, 150),
#             'min_data_in_leaf': int(0.05 * train_input_df[train_mask].shape[0]),
#             'max_depth': trial.suggest_int('max_depth', 3, 12),
#             'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
#             'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
#             'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
#             'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),
#             'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),
#             'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),
#             'monotone_constraints': f"-1,{','.join(['1']*number_of_cross_products)},{','.join(['0']*(len(enable_features)-1-number_of_cross_products))}",
#             'monotone_constraints_method': 'advanced',
#             'seed': 21,
#             'num_threads': -1
#         }

#         train_data = lgb.Dataset(
#             train_input_df[train_mask][enable_features],
#             label=train_input_df[train_mask][target_col],
#             weight=train_input_df[train_mask][weight_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         test_data = lgb.Dataset(
#             train_input_df[test_mask][enable_features],
#             label=train_input_df[test_mask][target_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         test_clean_data = lgb.Dataset(
#             train_input_df[test_clean_mask][enable_features],
#             label=train_input_df[test_clean_mask][target_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         model = lgb.train(
#             params,
#             train_data,
#             valid_sets=[train_data, test_data, test_clean_data] if test_data.data.shape[0] > 0 else [train_data],
#             verbose_eval=False
#         )

#         preds = model.predict(train_input_df[test_mask][enable_features])
#         y_true = train_input_df[test_mask][target_col].values
#         mae = mean_absolute_error(y_true, preds)

#         print(f"Trial MAE: {mae:.4f}")
#         return mae

#     print("🔧 Running Optuna tuning...")
#     study = optuna.create_study(direction="minimize")
#     study.optimize(objective, n_trials=n_trials)

#     print(f"✅ Best MAE: {study.best_value}")
#     print(f"🏆 Best Params: {study.best_params}")
#     return study.best_params

# product_hyperparams = run_optuna_tuning(
#     train_input_df=train_input_df,
#     enable_features=enable_features,
#     target_col=target_col,
#     weight_col="weight",
#     cat_feats=cat_feats,
#     train_mask=train_mask,
#     test_mask=test_mask,
#     test_clean_mask=test_clean_mask,
#     number_of_cross_products=number_of_cross_products,
#     n_trials=70
# )

# product_hyperparams

# COMMAND ----------



</file>

<file path="notebooks/03.1 Simulation RSD.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # 3-4 Simulation and RSD
# MAGIC
# MAGIC This notebook merges the functionality of `03 - Simulation` and `04 - PE RSD`.
# MAGIC
# MAGIC **Objective:**
# MAGIC Eliminate intermediate S3 writes between simulation and Price Elasticity (PE) calculation to improve performance and reduce storage costs.
# MAGIC
# MAGIC **Workflow:**
# MAGIC 1.  **Load Parameters & Model**: Fetches all necessary parameters and loads the trained ML model from MLflow.
# MAGIC 2.  **Generate Simulation Data**: For each product, it creates price variation scenarios using the ML model. This step now keeps the resulting DataFrame in Spark's memory.
# MAGIC 3.  **Calculate Price Elasticity**: The in-memory simulation DataFrame is immediately used to compute Price Elasticity via log-linear regression.
# MAGIC 4.  **Write Final Output**: The final PE results are written to the specified export path, maintaining compatibility with downstream processes.

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

dbutils.widgets.removeAll()

# Parameters from 03 - Simulation
dbutils.widgets.text("import_data_path", "s3://...")
dbutils.widgets.text("model_uri", "runs:/...")
dbutils.widgets.text("compute_cross_pe", "1")
dbutils.widgets.text("simulation_price_variation", "[0.95, 0.975, 1, 1.025, 1.05]")
dbutils.widgets.text("cpu_or_serving", "cpu")
dbutils.widgets.text("price_threshold", "1")
dbutils.widgets.text("dynamic_dates_alignment", "0")

# Parameters from 04 - PE RSD
dbutils.widgets.text("products", """{}""")
dbutils.widgets.text("min_weekly_rsd_quantity", "1")
dbutils.widgets.text("predict_date", "2023-03-01")

# Common Parameters
dbutils.widgets.text("run_id", "pe_run/2023-03-01")
dbutils.widgets.text("export_path", "s3://...")
dbutils.widgets.text("product_names", """["PRODUCT_A", "PRODUCT_B"]""")
dbutils.widgets.text("states", """["GA", "FL"]""")
dbutils.widgets.text("get_by_length_level", "0")
dbutils.widgets.text("get_by_price_tier", "0")
dbutils.widgets.text("get_by_menthol", "0")
dbutils.widgets.text("delta", "1")
dbutils.widgets.text("industry_unique_id", "1")
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

import json
import numpy as np
import pandas as pd
from functools import reduce

import mlflow
from mlflow.tracking.client import MlflowClient
from sklearn import linear_model

from pyspark.sql import DataFrame
from pyspark.sql.functions import lit, col, when
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, DateType, StringType, FloatType, IntegerType

# COMMAND ----------

# Simulation parameters
import_data_path = dbutils.widgets.get("import_data_path")
model_uri = dbutils.widgets.get("model_uri")
compute_cross_pe = int(dbutils.widgets.get("compute_cross_pe"))
simulation_price_variation = json.loads(dbutils.widgets.get("simulation_price_variation"))
cpu_or_serving = dbutils.widgets.get("cpu_or_serving")
price_threshold = dbutils.widgets.get("price_threshold")
dynamic_dates_alignment = int(dbutils.widgets.get("dynamic_dates_alignment"))
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))

# PE RSD parameters
products_config = json.loads(dbutils.widgets.get("products"))
min_weekly_rsd_quantity = int(dbutils.widgets.get("min_weekly_rsd_quantity"))
predict_date = dbutils.widgets.get("predict_date")

# Common parameters
run_id = dbutils.widgets.get("run_id")
export_path = dbutils.widgets.get("export_path")
products_names = json.loads(dbutils.widgets.get("product_names"))
states = json.loads(dbutils.widgets.get("states"))
get_by_length_level = int(dbutils.widgets.get("get_by_length_level"))
get_by_price_tier = int(dbutils.widgets.get("get_by_price_tier"))
get_by_menthol = int(dbutils.widgets.get("get_by_menthol"))
delta = int(dbutils.widgets.get("delta"))
export_suffix = dbutils.widgets.get("export_suffix")

print(f"Run ID: {run_id}")
print(f"Export Path: {export_path}")
print(f"In-memory data passing enabled. Intermediate S3 writes are disabled.")
print(f"Output format: {'Delta' if delta == 1 else 'Parquet'}")

# COMMAND ----------

client = MlflowClient()
model = mlflow.pyfunc.load_model(model_uri)
artifact_uri = client.get_run(model.metadata.run_id).to_dictionary()["info"]["artifact_uri"]
model_parameters = mlflow.artifacts.load_dict(artifact_uri + "/parameters.json").get("parameters")

L_PRODUCTS = {}
for product_name in products_names:
    prod_params = model_parameters.get(f'model_{product_name.lower().replace(" ","").replace("/", "")}')
    product_lineup_sk = int(prod_params.get("product_lineup_sk"))
    l_product_lineup_sk = prod_params.get("l_product_lineup_sk")
    L_PRODUCTS[product_lineup_sk] = l_product_lineup_sk

# COMMAND ----------

# DBTITLE 1,Helper Function: Simulation (from Notebook 03)
def simulate_points(df):
    """Simulate new data points using the prediction model and different price variations."""
    product_lineup_sk = df['product_lineup_sk'].astype(int).values[0]
    pe_type = df['pe_type'].values[0]
    cross_pid = df['simulated_price_product_lineup_sk'].values[0]
    l_product_linup_sk_tmp = L_PRODUCTS.get(product_lineup_sk, []).copy()

    # DEBUG: Print simulation entry info
    if df.shape[0] > 0 and df.shape[0] < 10:  # Only print for small groups to avoid spam
        print(f"  [DEBUG-SIM] Simulating for product_lineup_sk={product_lineup_sk}, pe_type={pe_type}, cross_pid={cross_pid}, rows={df.shape[0]}")

    X = df.copy()
    if cat_feats:
        for cat_col in cat_feats:
            if cat_col in X.columns:
                X[cat_col] = X[cat_col].astype("category")
    
    l_df = []
    
    if cross_pid == product_lineup_sk and pe_type == "pe":
        n_pid = -1
    elif cross_pid in l_product_linup_sk_tmp:
        n_pid = l_product_linup_sk_tmp.index(cross_pid)
    else:
        return pd.DataFrame()

    if X.shape[0] == 0:
        return pd.DataFrame()

    for i in simulation_price_variation:
        i = np.round(i, 4)
        
        # Update the price percentage change for the simulation
        if n_pid == -1:
            X.iloc[:, 0] = i
        else:
            X.iloc[:, n_pid + 1] = i

        # Create simulated data dictionary
        price_col_key = "rsd_price_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_price_roll4"
        quantity_col_key = "rsd_quantity_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_quantity_roll4"
        
        prediction_features = X[enable_features + ['product_lineup_sk']]
        predicted_volume_ratio = model.predict(prediction_features)

        data = {
            "pe_type": pe_type,
            "account_sk": df["account_sk"].values,
            "account_state": df["account_state"].values,
            "product_lineup_sk": df["product_lineup_sk"].values,
            "sales_week_forecast": df["sales_week_forecast"].values,
            "price": df[price_col_key].values * i,
            "volume": predicted_volume_ratio * df["rsd_quantity_roll4"],
            "rsd_price": df["rsd_price_roll4"].values,
            "rsd_quantity": df["rsd_quantity_roll4"].values,
            "rsd_quantity_industry": df["rsd_quantity_industry_roll4"].values,
            "rsd_price_industry": df["rsd_price_industry_roll4"].values,
            "rsd_price_cross": df[price_col_key].values,
            "rsd_quantity_cross": df[quantity_col_key].values,
            "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values
        }
        
        if get_by_length_level: data["cig_length"] = df["cig_length"].values
        if get_by_price_tier: data["price_tier"] = df["price_tier"].values
        if get_by_menthol: data["menthol_non_menthol"] = df["menthol_non_menthol"].values
            
        l_df.append(pd.DataFrame(data=data))

    return pd.concat(l_df) if l_df else pd.DataFrame()

# COMMAND ----------

# DBTITLE 1,Helper Function: PE Calculation (from Notebook 04)
def custom_linear_reg(df):
    """Compute price elasticity using a log-linear regression model."""
    constant = 1  # for log purposes
    
    # log(quantity) = c + pe*log(price) + dummies(weeks)
    X = df[["price", "sales_week_forecast"]]
    X.loc[:, "price"] = np.log(X["price"])
    X = pd.get_dummies(data=X, drop_first=True).values
    y = np.log(df.volume + constant)

    model = linear_model.LinearRegression(n_jobs=1)
    model.fit(X, y)
    
    results = {
        "account_sk": df["account_sk"].values[0],
        "account_state": df["account_state"].values[0],
        "product_lineup_sk": df["product_lineup_sk"].values[0],
        "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values[0],
        "pe_type": df["pe_type"].values[0],
        "pe": [model.coef_[0]],
        "intercept": [model.intercept_],
        "rsquared": [model.score(X, y)],
        "data_points_weeks": len(df.sales_week_forecast.unique()),
        "data_points": df.shape[0],
        "rsd_price": df.rsd_price.mean(),
        "rsd_quantity": df.rsd_quantity.mean(),
        "rsd_quantity_industry": df.rsd_quantity_industry.mean(),
        "rsd_price_industry": df.rsd_price_industry.mean(),
        "rsd_price_cross": df.rsd_price_cross.mean(),
        "rsd_quantity_cross": df.rsd_quantity_cross.mean(),
        "min_week": df.sales_week_forecast.min(),
        "max_week": df.sales_week_forecast.max(),
        "predict_date": df.predict_date.values[0]
    }
    
    if get_by_length_level: results["cig_length"] = df["cig_length"].values[0]
    if get_by_price_tier: results["price_tier"] = df["price_tier"].values[0]
    if get_by_menthol: results["menthol_non_menthol"] = df["menthol_non_menthol"].values[0]
  
    return pd.DataFrame(results)

# COMMAND ----------

# Schema for the output of simulate_points
simulation_schema_fields = [
    StructField("pe_type", StringType()),
    StructField("account_sk", IntegerType()),
    StructField("account_state", StringType()),
    StructField("product_lineup_sk", IntegerType()),
    StructField("sales_week_forecast", DateType()),
    StructField("price", FloatType()),
    StructField("volume", FloatType()),
    StructField("rsd_price", FloatType()),
    StructField("rsd_quantity", FloatType()),
    StructField("rsd_quantity_industry", FloatType()),
    StructField("rsd_price_industry", FloatType()),
    StructField("rsd_price_cross", FloatType()),
    StructField("rsd_quantity_cross", FloatType()),
    StructField("simulated_price_product_lineup_sk", IntegerType())
]
if get_by_length_level: simulation_schema_fields.append(StructField("cig_length", StringType()))
if get_by_price_tier: simulation_schema_fields.append(StructField("price_tier", StringType()))
if get_by_menthol: simulation_schema_fields.append(StructField("menthol_non_menthol", StringType()))
simulation_schema = StructType(simulation_schema_fields)

# Schema for the output of custom_linear_reg
pe_schema_fields = [
    StructField("account_sk", IntegerType()),
    StructField("account_state", StringType()),
    StructField("product_lineup_sk", IntegerType()),
    StructField("simulated_price_product_lineup_sk", IntegerType()),
    StructField("pe_type", StringType()),
    StructField("pe", FloatType()),
    StructField("intercept", FloatType()),
    StructField("rsquared", FloatType()),
    StructField("data_points_weeks", IntegerType()),
    StructField("data_points", IntegerType()),
    StructField("rsd_price", FloatType()),
    StructField("rsd_quantity", FloatType()),
    StructField("rsd_quantity_industry", FloatType()),
    StructField("rsd_price_industry", FloatType()),
    StructField("rsd_price_cross", FloatType()),
    StructField("rsd_quantity_cross", FloatType()),
    StructField("min_week", DateType()),
    StructField("max_week", DateType()),
    StructField("predict_date", StringType())
]
if get_by_length_level: pe_schema_fields.append(StructField("cig_length", StringType()))
if get_by_price_tier: pe_schema_fields.append(StructField("price_tier", StringType()))
if get_by_menthol: pe_schema_fields.append(StructField("menthol_non_menthol", StringType()))
pe_schema = StructType(pe_schema_fields)

# COMMAND ----------

# DBTITLE 1,Main Processing Loop: Simulation and PE Calculation
from pyspark.sql.types import DecimalType, DoubleType
from pyspark.sql import functions as F
from functools import reduce
import time

for product_name in products_names:
    print(f"--- Processing product: {product_name} ---")
    product_key = product_name.lower().replace(" ", "").replace("/", "")
    prod_params = model_parameters.get(f"model_{product_key}")

    if not prod_params:
        print(f"Warning: Could not find model parameters for {product_name}. Skipping.")
        continue

    # 1. SETUP
    cat_feats = prod_params.get("cat_feats")
    num_feats = prod_params.get("num_feats")
    agg_feats = prod_params.get("agg_feats")
    macroeconomics_flag = int(prod_params.get("include_macroeconomics"))
    product_lineup_sk = int(prod_params.get("product_lineup_sk"))
    l_product_lineup_sk = prod_params.get("l_product_lineup_sk")

    enable_features = num_feats + cat_feats

    end_date_pe = pd.to_datetime(prod_params.get("train_end_date")).date()
    start_date_pe = (end_date_pe - pd.Timedelta("1y"))
    if dynamic_dates_alignment == 1:
        start_date_pe, _ = get_dynamic_start_end_dates(no_years=1)
    
    print(f"  Simulation period: {start_date_pe} to {end_date_pe}")
    
    def simulate_points(df):
        """Simulate new data points using the prediction model and different price variations."""
        # This function will now correctly "close over" the variables from the loop
        
        # Get product context from the first row of the pandas DataFrame
        current_product_lineup_sk = df['product_lineup_sk'].astype(int).values[0]
        pe_type = df['pe_type'].values[0]
        cross_pid = df['simulated_price_product_lineup_sk'].values[0]
        l_product_linup_sk_tmp = L_PRODUCTS.get(current_product_lineup_sk, []).copy()

        X = df.copy()
        if cat_feats:
            for cat_col in cat_feats:
                if cat_col in X.columns:
                    X[cat_col] = X[cat_col].astype("category")
        
        l_df = []
        
        if cross_pid == current_product_lineup_sk and pe_type == "pe":
            n_pid = -1
        elif cross_pid in l_product_linup_sk_tmp:
            n_pid = l_product_linup_sk_tmp.index(cross_pid)
        else:
            return pd.DataFrame()

        if X.shape[0] == 0:
            return pd.DataFrame()

        for i in simulation_price_variation:
            i = np.round(i, 4)
            
            # Update the price percentage change for the simulation
            if n_pid == -1:
                X.iloc[:, 0] = i
            else:
                X.iloc[:, n_pid + 1] = i

            # Create simulated data dictionary
            price_col_key = "rsd_price_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_price_roll4"
            quantity_col_key = "rsd_quantity_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_quantity_roll4"
            
            # The model wrapper needs 'product_lineup_sk' for routing logic.
            columns_for_prediction = enable_features + ['product_lineup_sk']
            predicted_volume_ratio = model.predict(X[columns_for_prediction])

            data = {
                "pe_type": pe_type,
                "account_sk": df["account_sk"].values,
                "account_state": df["account_state"].values,
                "product_lineup_sk": df["product_lineup_sk"].values,
                "sales_week_forecast": df["sales_week_forecast"].values,
                "price": df[price_col_key].values * i,
                "volume": predicted_volume_ratio * df["rsd_quantity_roll4"],
                "rsd_price": df["rsd_price_roll4"].values,
                "rsd_quantity": df["rsd_quantity_roll4"].values,
                "rsd_quantity_industry": df["rsd_quantity_industry_roll4"].values,
                "rsd_price_industry": df["rsd_price_industry_roll4"].values,
                "rsd_price_cross": df[price_col_key].values,
                "rsd_quantity_cross": df[quantity_col_key].values,
                "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values
            }
            
            if get_by_length_level: data["cig_length"] = df["cig_length"].values
            if get_by_price_tier: data["price_tier"] = df["price_tier"].values
            if get_by_menthol: data["menthol_non_menthol"] = df["menthol_non_menthol"].values
                
            l_df.append(pd.DataFrame(data=data))

        return pd.concat(l_df) if l_df else pd.DataFrame()

    create_training_data_view(product_lineup_sk, l_product_lineup_sk, import_data_path,
        macroeconomics_flag, price_threshold, get_by_price_tier, get_by_length_level, get_by_menthol, cpu_or_serving,
        industry_unique_id=industry_unique_id)

    base_sql_query = f"""
        SELECT * FROM train_vw
        WHERE sales_week_forecast >= date('{start_date_pe}') AND sales_week_forecast <= date('{end_date_pe}')
          -- AND (rsd_quantity_y / rsd_quantity_roll4 < 2) AND (rsd_quantity_roll4 / rsd_quantity_y < 2)
          -- AND (rsd_price_y / rsd_price_roll4 < 1.5) AND (rsd_price_roll4 / rsd_price_y < 1.5)
          AND account_state IN {tuple(states) if len(states) > 1 else '(%s)' % ', '.join(map(repr, states))}
    """
    
    model_features = num_feats + cat_feats
    additional_cols = ["account_sk", "product_lineup_sk", "sales_week_forecast", 
                       "rsd_price_roll4", "rsd_quantity_roll4", "rsd_quantity_industry_roll4", "rsd_price_industry_roll4"] + \
                      [f"{pid}_rsd_quantity_roll4" for pid in l_product_lineup_sk] + \
                      [f"{pid}_rsd_price_roll4" for pid in l_product_lineup_sk]
    
    all_needed_cols = model_features + [col for col in additional_cols if col not in model_features]
    df_for_simulation = spark.sql(base_sql_query).select(*all_needed_cols)

    for field in df_for_simulation.schema.fields:
        if isinstance(field.dataType, (DecimalType, DoubleType)):
            df_for_simulation = df_for_simulation.withColumn(field.name, col(field.name).cast(FloatType()))

    df_for_simulation.cache()
    print(f"  Base data for simulation cached. Count: {df_for_simulation.count()}")
    
    # 2. SIMULATION
    all_simulation_dfs = []
    sim_groups = ["sales_week_forecast", "account_state"]
    if get_by_length_level: sim_groups.append("cig_length")
    if get_by_price_tier: sim_groups.append("price_tier")
    if get_by_menthol: sim_groups.append("menthol_non_menthol")

    print("  Generating PE simulations...")
    results_pe = (df_for_simulation
        .withColumn("simulated_price_product_lineup_sk", lit(product_lineup_sk))
        .withColumn("pe_type", lit("pe"))
        .filter("rsd_quantity_roll4 > 0")
        .groupBy(sim_groups)
        .applyInPandas(simulate_points, schema=simulation_schema))
    all_simulation_dfs.append(results_pe)

    # ---------------------------------------------------------------------------------------------------------------------------
    start = time.time()
    if compute_cross_pe:
        print("  Generating CROSSPE simulations...")
        # for cross_pid in list(set([1059168, 36744, 4157]) & set(l_product_lineup_sk)):
        for cross_pid in l_product_lineup_sk:
            results_pe = (
                df_for_simulation
                .withColumn("simulated_price_product_lineup_sk", lit(cross_pid))
                .withColumn("pe_type", lit("cross_pe"))
                .filter(f"{cross_pid}_rsd_quantity_roll4 > 0") 
                .groupBy(sim_groups)
                .applyInPandas(simulate_points, schema=simulation_schema)
            )
            all_simulation_dfs.append(results_pe)
        print("  CROSSPE simulations complete")

 
    print("  Combining all simulations without reduce using RDD...")
    first_schema = all_simulation_dfs[0].schema  
    combined_rdd = spark.sparkContext.union([df.rdd for df in all_simulation_dfs])
    df_simulation_combined = spark.createDataFrame(combined_rdd, schema=first_schema)

    # -----------------------------------------------------------------------------------------------
    df_simulation_combined.cache()
    conteo = df_simulation_combined.count()
    print(f"Total count: {conteo}")
    print("Total time to generate and cache CROSSPE:", time.time() - start, "seconds")


    # ---------------------------------------------------------------------------------------------------------------------------
    print("  [REPLICATION STEP] Rounding simulated data to mimic I/O precision changes.")
    rounding_precision = 6
    df_simulation_combined = df_simulation_combined.withColumn(
        "volume", F.round(col("volume"), rounding_precision)
    ).withColumn(
        "price", F.round(col("price"), rounding_precision)
    )

    # 3. PE CALCULATION
    print("  Calculating PE from replicated in-memory simulation data...")
    pe_groups = ["pe_type", "account_sk", "account_state", "product_lineup_sk", "simulated_price_product_lineup_sk"]
    if get_by_length_level: pe_groups.append("cig_length")
    if get_by_price_tier: pe_groups.append("price_tier")
    if get_by_menthol: pe_groups.append("menthol_non_menthol")

    final_pe_results = (df_simulation_combined
        .filter(f"rsd_quantity_cross >= {min_weekly_rsd_quantity} AND price IS NOT NULL")
        .withColumn("predict_date", lit(predict_date))
        .withColumn("volume", when(col("volume") < 0, 0).otherwise(col("volume")))
        .groupBy(pe_groups)
        .applyInPandas(custom_linear_reg, schema=pe_schema)
    )        

    # --- START: MODIFIED SECTION ---
    # 4. MATERIALIZE AND WRITE FINAL OUTPUT
    
    # Force the execution of all lazy transformations for the current product
    # by caching the results and triggering an action (.count()). This prevents
    # the non-deterministic UDF error on subsequent loop iterations.
    # final_pe_results.cache()
    # final_pe_results.count() # This action materializes the cache and triggers the computation.

    output_format = "delta" if delta == 1 else "parquet"
    output_path = f"{export_path}{run_id}/computed_pe{export_suffix}/{product_key}_{'delta' if delta==1 else 'parquet'}"
    
    print(f"  Writing final PE results to: {output_path} (Format: {output_format})")
    # This save operation will now write the readily available cached data.
    final_pe_results.write.mode('overwrite').format(output_format).save(output_path)
    
    # Clean up caches for this iteration to free up memory for the next product
    df_for_simulation.unpersist()
    final_pe_results.unpersist()
    # --- END: MODIFIED SECTION ---

print("--- All products processed. ---")

</file>

<file path="notebooks/06 - Evaluate.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Evaluate Notebook
# MAGIC
# MAGIC Plot the PE computed

# COMMAND ----------

!pip install -U kaleido==0.2.1

# COMMAND ----------

dbutils.library.restartPython() # for kaleido to work (the goal is the display png images as databricks cannot display large outputs)

# COMMAND ----------

# dbutils.widgets.removeAll()
# dbutils.widgets.text("pe_str", "/FileStore/pe/pe_model_dev/2022-06-01/str_pe/*.parquet")
# dbutils.widgets.text("pe_rsd", "/FileStore/pe/pe_model_dev/2022-06-01/computed_pe/*.parquet")
# dbutils.widgets.text("data_path", "s3://test-rey-0f513d7f0af2c2a9-datasource/pe_model_dev_test/databricks/2023/test/data/df_product.parquet")
# dbutils.widgets.text("cross_pe", "0")
# dbutils.widgets.text("delta", "1")
# dbutils.widgets.text("industry_unique_id", "1")

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

import plotly.express as px
import numpy as np
import plotly.graph_objects as go
import pandas as pd
from delta.tables import DeltaTable

from py4j.protocol import Py4JJavaError
def path_exist(path):
    try:
        base_dir = path
        if "*" in path:
             base_dir = "/".join(path.split("/")[:-1]) + "/"
        dbutils.fs.ls(base_dir) 
        return True
    except Exception as e:
        print(f"Path check failed for {path} (or its base dir): {e}")
        return False

pe_str = dbutils.widgets.get("pe_str").strip() 
pe_rsd_base_path_pattern = dbutils.widgets.get("pe_rsd").strip() 
data_path = dbutils.widgets.get("data_path").strip()
cross_pe = int(dbutils.widgets.get("cross_pe")) 
delta = int(dbutils.widgets.get("delta")) 
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))
import re

def normalize_s3_path(p: str) -> str:
    p = p.strip()
    if p.startswith("s3://"):
        scheme = "s3://"
        rest = p[len(scheme):]
        rest = re.sub(r'/+', '/', rest)  # collapse repeated slashes
        return scheme + rest
    return re.sub(r'/+', '/', p)

pe_str = normalize_s3_path(dbutils.widgets.get("pe_str"))
pe_rsd_base_path_pattern = normalize_s3_path(dbutils.widgets.get("pe_rsd"))
data_path = normalize_s3_path(dbutils.widgets.get("data_path"))

print(f"pe_str path pattern: '{pe_str}'") 
print(f"pe_rsd base path pattern: '{pe_rsd_base_path_pattern}'") 
print(f"data_path: '{data_path}'")
print(f"cross_pe flag: {cross_pe}")
print(f"delta flag: {delta}")


# Handle pe_str Reading 
# --- Build 'str' view from every <product>_delta sub-folder ----------------

try:
    (
        load_delta_tables_with_wildcards(spark, [f"{pe_str}/*"], union_all=True)
        .transform(lambda df: df if "pe_rsd" in df.columns else df.withColumnRenamed("pe", "pe_rsd"))
        .createOrReplaceTempView("str")
    )
except Exception as e:
    print(f"WARNING: Could not load STR data from {pe_str}. Using empty schema. Error: {e}")
    schema_str_ddl = (
        "`account_state` STRING, `account_sk` STRING, `product_name` STRING, "
        "`pe_rsd` DOUBLE, `estimated_pe` DOUBLE, `str_weekavg_quantity` DOUBLE, "
        "`knn_computed` INT, `scan_validated_flag` STRING, `contract` STRING, "
        "`rsd_quantity` DOUBLE, `rsd_price` DOUBLE, `str_price` DOUBLE"
    )
    spark.createDataFrame([], schema_str_ddl).createOrReplaceTempView("str")

# Handle 'rsd' view creation 
# Uses pe_rsd_base_path_pattern
if delta == 1:
    # Delta Reading Logic for 'rsd' view 
    try:
        load_delta_tables_with_wildcards(spark, [f"{pe_rsd_base_path_pattern}/*"], union_all=True).createOrReplaceTempView("rsd")
    except Exception as e: 
        print(f"ERROR: Failed during processing RSD delta data from {pe_rsd_base_path_pattern}. Error: {e}")
        raise e
else: # delta == 0
    # Parquet Reading Logic for 'rsd' view 
    pe_rsd_path = pe_rsd_base_path_pattern 
    read_format = "parquet" 
    print(f"\nDelta is not 1, attempting to read PARQUET files from: {pe_rsd_path}")
    try:
        spark.read.format(read_format).load(pe_rsd_path).createOrReplaceTempView("rsd")
        print(f"Successfully read RSD parquet data and created 'rsd' view.")
    except Exception as e: 
        print(f"ERROR: Failed to read RSD parquet data from {pe_rsd_path}. Error: {e}")
        raise e

# Handle 'data' and 'data_industry' views (conditional on delta)
def _table_exists(name: str) -> bool:
    try:
        return spark.catalog.tableExists(name)
    except:
        return False

if delta == 1:
    # --- PRODUCT (view: data) ---
    base_dir_for_product_data = "/".join(data_path.split("/")[:-1])
    product_filename_stem = data_path.split("/")[-1].replace(".parquet", "")

    actual_product_delta_path = f"{base_dir_for_product_data}/delta/{product_filename_stem}"
    alt_product_delta_path    = f"{base_dir_for_product_data}/delta/{product_filename_stem}_delta"

    created_data = False
    try:
        spark.read.format("delta").load(actual_product_delta_path).createOrReplaceTempView("data")
        print(f"Created 'data' from DELTA: {actual_product_delta_path}")
        created_data = True
    except Exception as e1:
        print(f"Warn: product DELTA not found at {actual_product_delta_path}: {e1}")
        try:
            spark.read.format("delta").load(alt_product_delta_path).createOrReplaceTempView("data")
            print(f"Created 'data' from DELTA (alt): {alt_product_delta_path}")
            created_data = True
        except Exception as e2:
            print(f"Warn: product DELTA alt failed at {alt_product_delta_path}: {e2}")

    if not created_data:
        try:
            spark.read.parquet(data_path).createOrReplaceTempView("data")
            print(f"Fell back to PARQUET for 'data': {data_path}")
        except Exception as e3:
            print(f"ERROR: could not create 'data' from parquet {data_path}: {e3}")

    # --- INDUSTRY (view: data_industry) ---
    actual_industry_delta_path = f"{base_dir_for_product_data}/delta/df_industry"
    alt_industry_delta_path    = f"{base_dir_for_product_data}/delta/df_industry_delta"
    industry_parquet_path      = f"{base_dir_for_product_data}/df_industry.parquet"

    created_industry = False
    try:
        (spark.read.format("delta").load(actual_industry_delta_path)
              .filter(f"product_lineup_sk={industry_unique_id}")
              .createOrReplaceTempView("data_industry"))
        print(f"Created 'data_industry' from DELTA: {actual_industry_delta_path}")
        created_industry = True
    except Exception as e1:
        print(f"Warn: industry DELTA not found at {actual_industry_delta_path}: {e1}")
        try:
            (spark.read.format("delta").load(alt_industry_delta_path)
                  .filter(f"product_lineup_sk={industry_unique_id}")
                  .createOrReplaceTempView("data_industry"))
            print(f"Created 'data_industry' from DELTA (alt): {alt_industry_delta_path}")
            created_industry = True
        except Exception as e2:
            print(f"Warn: industry DELTA alt failed at {alt_industry_delta_path}: {e2}")

    if not created_industry:
        try:
            (spark.read.parquet(industry_parquet_path)
                  .filter(f"product_lineup_sk={industry_unique_id}")
                  .createOrReplaceTempView("data_industry"))
            print(f"Fell back to PARQUET for 'data_industry': {industry_parquet_path}")
        except Exception as e3:
            print(f"ERROR: could not create 'data_industry' from parquet {industry_parquet_path}: {e3}")

else:
    # Existing parquet branch: keep it, but don't raise if it fails
    print(f"\nAttempting to read product data from: {data_path}")
    try:
        spark.read.parquet(data_path).createOrReplaceTempView("data")
        print(f"Created 'data' (parquet).")
    except Exception as e:
        print(f"ERROR: product parquet failed at {data_path}: {e}")

    try:
        industry_data_path = "/".join(data_path.split("/")[:-1]) + "/df_industry.parquet"
        print(f"Attempting to read industry data: {industry_data_path}")
        (spark.read.parquet(industry_data_path)
              .filter(f"product_lineup_sk={industry_unique_id}")
              .createOrReplaceTempView("data_industry"))
        print(f"Created 'data_industry' (parquet).")
    except Exception as e:
        print(f"ERROR: industry parquet failed at {industry_data_path}: {e}")


# COMMAND ----------

# MAGIC %md
# MAGIC # PE

# COMMAND ----------

# DBTITLE 1,Training volumes
sql = """
select pe_type, product_lineup_sk, simulated_price_product_lineup_sk, count(*) nb_rows
, sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
, sum(rsd_quantity_cross) rsd_quantity_cross
, count(distinct account_sk) n_acccount
from rsd 
group by pe_type, product_lineup_sk, simulated_price_product_lineup_sk
"""
df = spark.sql(sql).toPandas()
display(df)

# COMMAND ----------

# DBTITLE 1,Training and real volumes
# Training and real volumes
max_date = pe_rsd_base_path_pattern.split("/")[-2]
min_date = str((pd.to_datetime(max_date) - pd.DateOffset(years=1)).date())

if spark.catalog.tableExists("data") and spark.catalog.tableExists("data_industry"):
    display(spark.sql(f"""
        with weekly_avg_rsd as (
            select data.product_lineup_sk, sum(rsd_quantity)/52 rsd_quantity_raw_data
                 , count(distinct data.account_sk) n_account_raw_data
            from data inner join data_industry using (account_sk, sales_week_sk)
            where sales_week_end_date < '{max_date}'
              and sales_week_end_date >= '{min_date}'
              and rsd_quantity > 0
            group by data.product_lineup_sk
        ),
        pe_rsd as (
            select pe_type, product_lineup_sk, simulated_price_product_lineup_sk
                 , count(*) nb_rows
                 , sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
                 , sum(rsd_quantity_cross) rsd_quantity_cross
                 , count(distinct account_sk) n_acccount
            from rsd
            group by pe_type, product_lineup_sk, simulated_price_product_lineup_sk
        )
        select pe_rsd.*
             , weekly_avg_rsd.rsd_quantity_raw_data weekly_avg_raw_data
             , n_account_raw_data
        from pe_rsd left join weekly_avg_rsd using (product_lineup_sk)
    """))
else:
    print("Skipping 'Training and real volumes': 'data' or 'data_industry' view not available.")


# COMMAND ----------

# MAGIC %md
# MAGIC # PE Distribution

# COMMAND ----------

if path_exist(f"{pe_str}"): 
    print("STR path exists, reading from 'str' view for PE distribution...")
    df = spark.sql("""
        SELECT  account_state,
                account_sk,
                product_name,
                pe_rsd,                    -- RSD‑based PE
                estimated_pe  AS pe_str    -- STR‑based / model PE
        FROM    str
    """).toPandas()

    if df.empty:
        print("Warning: 'str' view was empty. Falling back to RSD data only for PE distribution DataFrame.")
        df = spark.sql("""
            SELECT  account_state,
                    account_sk,
                    CAST(product_lineup_sk AS string) AS product_name,
                    pe               AS pe_rsd,
                    CAST(NULL AS double)              AS pe_str
            FROM    rsd
            WHERE   product_lineup_sk = simulated_price_product_lineup_sk
        """).toPandas()  # use 'rsd' view
else:
    print("STR path does not exist, reading from 'rsd' view only for PE distribution...")
    df = spark.sql("""
        SELECT  account_state,
                account_sk,
                CAST(product_lineup_sk AS string) AS product_name,
                pe               AS pe_rsd,
                CAST(NULL AS double)              AS pe_str
        FROM    rsd
        WHERE   product_lineup_sk = simulated_price_product_lineup_sk
    """).toPandas()

print("\nDataFrame 'df' for PE Distribution head:")
print(df.head())


# COMMAND ----------

# DBTITLE 1,Histograms
if not df.empty:
    product_names_in_df = df["product_name"].unique()
    print(f"\nGenerating histograms for products: {product_names_in_df}")
    for product_name in product_names_in_df:
        # Filter data for the current product
        df_product = df.query(f"product_name==@product_name") # Use @ syntax for query variable
        
        # Prepare data for stacking (handle potential missing pe_str if only rsd data was available)
        if 'pe_str' in df_product.columns:
             df_melt = df_product[["account_sk","pe_rsd","pe_str"]].set_index("account_sk").stack().reset_index()
        else: # Should not happen with current logic creating empty view, but safe check
             df_melt = df_product[["account_sk","pe_rsd"]].set_index("account_sk").stack().reset_index()
        
        df_melt = df_melt.rename(columns={"level_1": "pe_type", 0: "pe"})
        
        # Create histogram
        fig = px.histogram(
            df_melt,
            x="pe",
            facet_col="pe_type",
            facet_col_spacing=0.1,
            color="pe_type",
            title=f"PE Distribution for {product_name}"
        )
        fig.update_yaxes(matches=None) # Allow independent y-axes
        fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1])) # Clean up facet titles
        fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))
        fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))
        try:
            display(fig.show(renderer="png")) # Use display() for Databricks environment
        except Exception as plot_err:
            print(f"Warning: Could not display histogram for {product_name}. Error: {plot_err}")
else:
    print("\nDataFrame 'df' for PE Distribution is empty. Skipping histograms.")


# COMMAND ----------

# DBTITLE 1,PE by product_lineup_sk
print("\nDescriptive Statistics for PE by Product:")
if not df.empty:
    # Ensure both columns exist before describing, handle cases where pe_str might be all null
    cols_to_describe = [col for col in ["pe_rsd", "pe_str"] if col in df.columns and df[col].notna().any()]
    if cols_to_describe:
        display(df.groupby(["product_name"])[cols_to_describe].describe().reset_index())
    else:
        print("No valid PE columns found in the DataFrame to describe.")
else:
    print("DataFrame 'df' is empty. Skipping description by product.")

# COMMAND ----------

# DBTITLE 1,PE by product_lineup_sk and state
print("\nDescriptive Statistics for PE by Product and State:")
if not df.empty:
    cols_to_describe = [col for col in ["pe_rsd", "pe_str"] if col in df.columns and df[col].notna().any()]
    if cols_to_describe:
        display(df.groupby(["product_name","account_state"])[cols_to_describe].describe().reset_index())
    else:
         print("No valid PE columns found in the DataFrame to describe.")
else:
    print("DataFrame 'df' is empty. Skipping description by product and state.")

# COMMAND ----------

# MAGIC %md
# MAGIC #PE STR

# COMMAND ----------

def display_pe_str(custom_group: str = ""):
    query = f"""
        SELECT
            product_lineup_sk,
            product_name
            {custom_group}
            , ROUND(SUM(pe_rsd * rsd_quantity) / SUM(rsd_quantity), 4)     AS wpe_rsd
            , ROUND(
                SUM(pe_rsd * str_weekavg_quantity) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity ELSE 0 END),
                4
              ) AS wpe_rsd_str_quant           -- wpe for RSD PE using STR quantity
            , ROUND(
                SUM(
                    estimated_pe * (
                        CASE WHEN scan_validated_flag = 'Y'
                             THEN str_weekavg_quantity ELSE 0 END
                    )
                ) /
                SUM(
                    CASE WHEN scan_validated_flag = 'Y'
                         THEN str_weekavg_quantity ELSE 0 END
                ),
                4
              ) AS wpe_str_scan                -- scanned stores
            , ROUND(
                SUM(
                    estimated_pe * (
                        CASE WHEN scan_validated_flag = 'N'
                             THEN str_weekavg_quantity ELSE 0 END
                    )
                ) /
                SUM(
                    CASE WHEN scan_validated_flag = 'N'
                         THEN str_weekavg_quantity ELSE 0 END
                ),
                4
              ) AS wpe_str_nonscan             -- non‑scanned stores
            , ROUND(
                SUM(estimated_pe * str_weekavg_quantity) /
                SUM(str_weekavg_quantity),
                4
              ) AS wpe_str_all                 -- all STR stores
            , ROUND(
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity ELSE 0 END),
                0
              ) AS str_weekavg_quantity_rsd
            , SUM(str_weekavg_quantity)                                    AS str_weekavg_quantity
            , ROUND(
                SUM(CASE WHEN knn_computed = 0 THEN 1 ELSE 0 END),
                0
              ) AS n_acccount_rsd
            , COUNT(DISTINCT CASE WHEN scan_validated_flag = 'N' THEN account_sk END)
              AS n_acccount_str_scan
            , COUNT(DISTINCT account_sk)                                    AS n_acccount
            , ROUND(SUM(rsd_quantity), 0)                                   AS rsd_quantity
            , ROUND(
                SUM(
                    CASE WHEN knn_computed = 0 THEN rsd_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity END),
                2
              ) AS rsd_price
            , ROUND(
                SUM(
                    CASE WHEN knn_computed = 0 THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity END),
                2
              ) AS str_price_rsd
            , ROUND(
                SUM(
                    CASE WHEN scan_validated_flag = 'Y' THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN scan_validated_flag = 'Y' THEN str_weekavg_quantity END),
                2
              ) AS str_price_scan
            , ROUND(
                SUM(
                    CASE WHEN scan_validated_flag = 'N' THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN scan_validated_flag = 'N' THEN str_weekavg_quantity END),
                2
              ) AS str_price_nonscan
        FROM
            str   -- the Spark VIEW we created
        GROUP BY
            product_lineup_sk,
            product_name
            {custom_group}
        ORDER BY
            product_name {custom_group}
    """
    try:
        group_label = custom_group if custom_group else "Product Only"
        print(f"\nDisplaying PE‑STR results (grouped by: '{group_label}')")
        display(spark.sql(query))
    except Exception as e:
        print(f"ERROR executing display_pe_str query (group: '{custom_group}'). Error: {e}")


if path_exist(f"{pe_str}"): 
    print("\n--- PE STR Section ---")
    str_view_count = spark.sql("SELECT COUNT(*) FROM str").first()[0]
    if str_view_count > 0:
        display_pe_str()
        display_pe_str(", contract")
        display_pe_str(", account_state")
        display_pe_str(", account_state, contract")
    else:
        print("Skipping PE STR displays because the 'str' view is empty.")
else:
    print("\nSkipping PE STR displays because STR path does not exist.")

# COMMAND ----------

# MAGIC %md
# MAGIC # Cross PE Distribution

# COMMAND ----------

df_crosspe = pd.DataFrame() 
if cross_pe:
    print("\n--- Cross PE Section ---")
    try:
        df_crosspe = spark.sql("""
            with name_map as (select distinct product_lineup_sk, kpl_preferred_name product_name from data)
            select rsd.*, t1.product_name, t2.product_name cross_product_name
            from rsd
            left join name_map t1 on rsd.product_lineup_sk = t1.product_lineup_sk
            left join name_map t2 on rsd.simulated_price_product_lineup_sk = t2.product_lineup_sk
            where rsd.product_lineup_sk != rsd.simulated_price_product_lineup_sk
        """).toPandas()
        print(f"Successfully generated Cross PE DataFrame with {len(df_crosspe)} rows.")
        if df_crosspe.empty:
            print("Warning: Cross PE DataFrame is empty (no cross PE data found in 'rsd' view).")
    except Exception as e:
        print(f"ERROR generating Cross PE DataFrame. Error: {e}")
        cross_pe = 0 
else:
     print("\nSkipping Cross PE section because cross_pe flag is 0.")


# COMMAND ----------

if not cross_pe or df_crosspe.empty:
    if cross_pe and df_crosspe.empty:
         # Exit only if cross_pe was requested but no data was found
         print("\nExiting notebook: Cross PE was requested (cross_pe=1), but no cross PE data was found.")
         dbutils.notebook.exit("Cross PE data not available")
    else:
         # If cross_pe was 0 initially, just print message
         print("\nCross PE section skipped.")

# COMMAND ----------

# DBTITLE 1,Boxplot
if cross_pe and not df_crosspe.empty:
    print("\nGenerating Cross PE boxplots...")
    product_names_crosspe = df_crosspe.product_name.unique()
    print(f"Products with cross PE data: {product_names_crosspe}")
    for product_name in product_names_crosspe:
        try:
            df_product_cross = df_crosspe.query(f"product_name==@product_name")
            gb = df_product_cross.groupby("cross_product_name")[["pe"]]
            stats = gb.agg(
                q1=pd.NamedAgg(column="pe", aggfunc=lambda x: x.quantile(0.25)),
                mean=pd.NamedAgg(column="pe", aggfunc="mean"),
                median=pd.NamedAgg(column="pe", aggfunc="median"),
                q3=pd.NamedAgg(column="pe", aggfunc=lambda x: x.quantile(0.75)),
                lowerfence=pd.NamedAgg(column="pe", aggfunc="min"),
                upperfence=pd.NamedAgg(column="pe", aggfunc="max"),
                count=pd.NamedAgg(column="pe", aggfunc="size") # Get count for y-axis label if needed
            ).reset_index()

            if not stats.empty:
                fig = go.Figure()
                fig.add_trace(go.Box(
                    x=stats["cross_product_name"], # Use cross_product_name for x-axis categories
                    q1=stats["q1"],
                    mean=stats["mean"],
                    median=stats["median"],
                    q3=stats["q3"],
                    lowerfence=stats["lowerfence"],
                    upperfence=stats["upperfence"],
                    name=product_name # Add name for potential legend if plotting multiple traces later
                ))
                fig.update_layout(
                     title_text=f"Cross PE Distribution for {product_name} (Effect OF other product price changes ON {product_name})",
                     xaxis_title="Product Whose Price Changed",
                     yaxis_title="Calculated Cross PE"
                )
                display(fig.show(renderer="png"))
            else:
                 print(f"  Skipping boxplot for {product_name}: No aggregated stats found.")
        except Exception as boxplot_err:
            print(f"Warning: Could not display Cross PE boxplot for {product_name}. Error: {boxplot_err}")

# COMMAND ----------

# DBTITLE 1,Cross PE
if cross_pe and not df_crosspe.empty:
    print("\nCross PE Descriptive Statistics:")
    try:
        # Add product names for better readability
        display(df_crosspe.groupby(["product_name","cross_product_name"])["pe"].describe().reset_index())
    except Exception as desc_err:
         print(f"Warning: Could not display Cross PE descriptive stats. Error: {desc_err}")

# COMMAND ----------



</file>

<file path="notebooks/other/accuracy_rsd_lgbm.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Calculate accuracy of the model in test data
# MAGIC
# MAGIC Predict volume sold at the observation level and compare to actual results.
# MAGIC The output is the accuracy at the global level but can also be computed at the state level.
# MAGIC For observation level results you need to modify the notebook to save that table.
# MAGIC
# MAGIC Accuracy here is actually 1 - MAPE.
# MAGIC

# COMMAND ----------

# dbutils.widgets.removeAll()
dbutils.widgets.text(
    "folder",
    "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/",
)
dbutils.widgets.text("run_id", "2023/08/vapor_cpu_cross_12periods/full")

# dbutils.widgets.text(
#     "export_path", 
#     "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/",
# )
dbutils.widgets.text("model_to_use", "all")
dbutils.widgets.text("max_predict_date", "2023-06-01") 
dbutils.widgets.text("config_path", "../config/vapor_full.json")

dbutils.widgets.text("delta", "0")
dbutils.widgets.text("industry_unique_id", "1")
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

# MAGIC %run ../utils.py

# COMMAND ----------

import json
from dateutil.relativedelta import relativedelta
import datetime
import calendar
import os 
import mlflow
from mlflow.entities import ViewType

folder = dbutils.widgets.get("folder")
run_id = dbutils.widgets.get("run_id")
import_data_path = folder + run_id
max_predict_date =  dbutils.widgets.get("max_predict_date")
delta_input = int(dbutils.widgets.get("delta"))
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))
export_suffix =  dbutils.widgets.get("export_suffix")

raw_config_path = dbutils.widgets.get("config_path")
print(f"Raw config_path from widget: {raw_config_path}")

current_notebook_path_full = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
current_notebook_dir = os.path.dirname(current_notebook_path_full)
print(f"Current notebook path full: {current_notebook_path_full}")
print(f"Current notebook directory: {current_notebook_dir}")

if not raw_config_path.startswith('/'): 
    resolved_path_relative_to_notebook = os.path.abspath(os.path.join(current_notebook_dir, raw_config_path))
    print(f"Resolved path relative to notebook: {resolved_path_relative_to_notebook}")

    if current_notebook_path_full.startswith('/Users/') and \
       resolved_path_relative_to_notebook.startswith('/Users/'):
        
        workspace_equivalent_path = "/Workspace" + resolved_path_relative_to_notebook
        print(f"Attempting Workspace equivalent path: {workspace_equivalent_path}")
        
        try:
            dbutils.fs.ls(workspace_equivalent_path) 
            print(f"Workspace equivalent path {workspace_equivalent_path} seems to exist. Using it.")
            config_path = workspace_equivalent_path
        except Exception:
            print(f"Workspace equivalent path {workspace_equivalent_path} not found. Falling back to resolved path relative to notebook: {resolved_path_relative_to_notebook}")
            config_path = resolved_path_relative_to_notebook # Fallback
    else:
        config_path = resolved_path_relative_to_notebook
else:
    config_path = raw_config_path
    print(f"Config_path from widget is already absolute: {config_path}")

print(f"Final config_path to be used: {config_path}")

try:
    with open(config_path, "r") as f:
        config = json.load(f)
except FileNotFoundError:
    print(f"ERROR: Config file not found at FINAL path: {config_path}")
    # Check if the other variant exists if one failed
    if config_path.startswith("/Workspace"):
        alternative_path = config_path.replace("/Workspace", "", 1) # /Users/...
        print(f"Attempting alternative path: {alternative_path}")
        try:
            with open(alternative_path, "r") as f_alt: config = json.load(f_alt)
            print(f"Success with alternative path: {alternative_path}")
        except:
            print(f"Alternative path {alternative_path} also failed.")
            raise
    elif config_path.startswith("/Users"):
        alternative_path = "/Workspace" + config_path # /Workspace/Users/...
        print(f"Attempting alternative path: {alternative_path}")
        try:
            with open(alternative_path, "r") as f_alt: config = json.load(f_alt)
            print(f"Success with alternative path: {alternative_path}")
        except:
            print(f"Alternative path {alternative_path} also failed.")
            raise
    else:
        raise

except Exception as e:
    print(f"Error loading config file {config_path}: {e}")
    raise



states = config.get("states")
products_config = config.get("products")
product_names = list(products_config.keys())
product_id_list = [d.get("id") for d in config.get("products").values()]

model_to_use = dbutils.widgets.get("model_to_use")

def get_previous_months(date1):
    """Return a list of the three months preceding ``date1``."""
    date1 = datetime.datetime.strptime(date1, "%Y-%m-%d")
    date_list = []
    for i in range(3):  # iterate backwards month by month
        date1 = date1.replace(day=1)
        date1 = date1 - relativedelta(months=1)
        date_list.append(date1.strftime("%Y-%m-%d"))
    return date_list

predict_dates = [max_predict_date]
predict_dates.extend(get_previous_months(max_predict_date))
predict_dates.sort() # Sort to have oldest first, newest last

print(f"Running for run_id: {run_id}")
print(f"Import data path (for train_vw): {import_data_path}")
print(f"Product names from config: {product_names}")
print(f"Product ids from config: {product_id_list}")
print(f"Predict dates for model loading/testing: {predict_dates}")
print(f"Model to use policy: {model_to_use}")
# print(f"Delta input: {delta_input}")


# COMMAND ----------

import numpy as np
import pandas as pd
from functools import reduce
from pyspark.sql import DataFrame
from mlflow.tracking.client import MlflowClient
print("Attempting to configure MLflow for Databricks environment...")
try:
    host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()
    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

    if host and token:
        os.environ['DATABRICKS_HOST'] = host
        os.environ['DATABRICKS_TOKEN'] = token
        mlflow.set_tracking_uri("databricks")
        print("MLflow tracking URI set to 'databricks'. DATABRICKS_HOST and DATABRICKS_TOKEN populated.")
    else:
        print("Warning: Could not retrieve Databricks host or token using dbutils. MLflow might rely on existing env vars or default config.")
        mlflow.set_tracking_uri("databricks")

except Exception as e:
    print(f"Error trying to get host/token with dbutils or set env vars: {e}")
    try:
        mlflow.set_tracking_uri("databricks")
    except Exception as e_set_uri:
        print(f"Could not even set tracking_uri: {e_set_uri}")


print(f"DATABRICKS_HOST (after attempting set): {os.environ.get('DATABRICKS_HOST')}")
print(f"DATABRICKS_TOKEN (after attempting set): {os.environ.get('DATABRICKS_TOKEN')}")

print("Initializing MlflowClient...")
try:
    client = MlflowClient()
    print("MlflowClient initialized successfully.")
except Exception as e:
    print(f"Error initializing MlflowClient: {e}")
    raise

# COMMAND ----------

def predict_sales(df):
    """Predicts with the model and compares to actual results."""
    global model, get_by_length_level, get_by_price_tier, get_by_menthol, current_cat_feats_for_udf

    product_lineup_sk = df["product_lineup_sk"].astype(int).values[0]
    pe_type = df["pe_type"].values[0]

    X = df.copy()
    if current_cat_feats_for_udf: 
        for cat_col in current_cat_feats_for_udf: 
            if cat_col in X.columns: 
                X[cat_col] = X[cat_col].astype("category")
            else:
                print(f"Warning in predict_sales UDF: Categorical feature '{cat_col}' not found in input DataFrame columns: {X.columns}")
    l_df = []

    columns = [
        "pe_type", "account_sk", "account_state", "product_lineup_sk",
        "sales_week_forecast", "price", "volume", 
        "rsd_price", "rsd_quantity", "rsd_quantity_industry", "rsd_price_industry",
        "rsd_price_cross", "rsd_quantity_cross",
    ]
    if get_by_length_level: columns.append("cig_length")
    if get_by_price_tier: columns.append("price_tier")
    if get_by_menthol: columns.append("menthol_non_menthol")


    if X.shape[0] == 0: 
        empty_data_for_schema = {
            "pe_type": [], "account_sk": [], "account_state": [], "product_lineup_sk": [],
            "sales_week_start_date": [], "sales_week_forecast": [],
            "actual_volume": [], "predicted_volume": [], "rsd_price": [],
            "rsd_quantity_roll4": [], "rsd_quantity_industry": [], "rsd_price_industry": []
        }
        if get_by_length_level: empty_data_for_schema["cig_length"] = []
        if get_by_price_tier: empty_data_for_schema["price_tier"] = []
        if get_by_menthol: empty_data_for_schema["menthol_non_menthol"] = []
        return pd.DataFrame(data=empty_data_for_schema)

    
    predicted_volume_values = model.predict(X) * df.loc[:, "rsd_quantity_roll4"].values
    
    data = {
        "pe_type": pe_type, 
        "account_sk": df.loc[:, "account_sk"].values,
        "account_state": df.loc[:, "account_state"].values,
        "product_lineup_sk": df.loc[:, "product_lineup_sk"].values,
        "sales_week_start_date": df.loc[:, "sales_week_start_date"].values,
        "sales_week_forecast": df.loc[:, "sales_week_forecast"].values,
        "actual_volume": df.loc[:, "rsd_quantity_y"].values,
        "predicted_volume": predicted_volume_values,
        "rsd_price": df.loc[:, "rsd_price_roll4"].values,
        "rsd_quantity_roll4": df.loc[:, "rsd_quantity_roll4"].values,
        "rsd_quantity_industry": df.loc[:, "rsd_quantity_industry_roll4"].values,
        "rsd_price_industry": df.loc[:, "rsd_price_industry_roll4"].values,
    }
    if get_by_length_level: # This global flag is set in compute_predictions
        data["cig_length"] = df.loc[:, "cig_length"].values
    if get_by_price_tier: # This global flag is set in compute_predictions
        data["price_tier"] = df.loc[:, "price_tier"].values
    if get_by_menthol: # This global flag is set in compute_predictions
        data["menthol_non_menthol"] = df.loc[:, "menthol_non_menthol"].values
    
    df_tmp = pd.DataFrame(data=data)
    l_df.append(df_tmp)

    return pd.concat(l_df)

# COMMAND ----------


from pyspark.sql.types import StructType, StructField, DateType, StringType, FloatType, IntegerType, TimestampType
from pyspark.sql.functions import lit
import pyspark.sql.functions as F


def compute_predictions(
    product_name_key,
    product_lineup_sk,
    l_product_lineup_sk,
    start_date,
    end_date,
    current_model_params,
):
    num_feats = current_model_params.get("num_feats", []) # Default to empty list
    cat_feats = current_model_params.get("cat_feats", []) # Default to empty list
    macroeconomics = int(current_model_params.get("include_macroeconomics", 0))
    get_by_length_level_flag = int(current_model_params.get("gb_cig_length", 0))
    get_by_price_tier_flag = int(current_model_params.get("gb_price_tier", 0))
    get_by_menthol_flag = int(current_model_params.get("gb_menthol", 0))
    cpu_or_serving = current_model_params.get("cpu_or_serving")
    price_threshold_val = int(current_model_params.get("price_threshold", 0)) # Provide a default

    field_list = [
        StructField("pe_type", StringType()),
        StructField("account_sk", IntegerType()),
        StructField("account_state", StringType()),
        StructField("product_lineup_sk", IntegerType()),
        StructField("sales_week_start_date", DateType()),
        StructField("sales_week_forecast", DateType()),
        StructField("actual_volume", FloatType()),
        StructField("predicted_volume", FloatType()),
        StructField("rsd_price", FloatType()),
        StructField("rsd_quantity_roll4", FloatType()),
        StructField("rsd_quantity_industry", FloatType()),
        StructField("rsd_price_industry", FloatType()),
    ]
    if get_by_length_level_flag: field_list += [StructField("cig_length", StringType())]
    if get_by_price_tier_flag: field_list += [StructField("price_tier", StringType())]
    if get_by_menthol_flag: field_list += [StructField("menthol_non_menthol", StringType())]
    result_schema = StructType(field_list)

    create_training_data_view(
        product_lineup_sk,
        l_product_lineup_sk,
        import_data_path,
        macroeconomics,
        price_threshold_val,
        gb_price_tier=get_by_price_tier_flag,
        gb_cig_length=get_by_length_level_flag,
        gb_menthol=get_by_menthol_flag,
        cpu_or_serving=cpu_or_serving,
        industry_unique_id=industry_unique_id
    )

    additional_cols_definition = ( # Renamed to avoid confusion with additional_cols list for select
        [
            "account_sk", # Needed by predict_sales
            # "product_lineup_sk", # This will be part of grouping_cols_for_select
            # "sales_week_start_date", # Needed by predict_sales
            # "sales_week_forecast", # This will be part of grouping_cols_for_select
            "rsd_quantity_y", # Needed by predict_sales for actual_volume
            "rsd_price_roll4", # Needed by predict_sales
            "rsd_quantity_roll4", # Needed by predict_sales
            "rsd_quantity_industry_roll4", # Needed by predict_sales
            "rsd_price_industry_roll4", # Needed by predict_sales
        ]
        + [f"{pid}_rsd_quantity_roll4" for pid in l_product_lineup_sk if l_product_lineup_sk] # Add if check
        + [f"{pid}_rsd_price_roll4" for pid in l_product_lineup_sk if l_product_lineup_sk] # Add if check
    )
    if (get_by_length_level_flag or get_by_price_tier_flag or get_by_menthol_flag):
        additional_cols_definition.append(f"{product_lineup_sk}_rsd_quantity_roll4")
        additional_cols_definition.append(f"{product_lineup_sk}_rsd_price_roll4")
    
    additional_cols_definition = sorted(list(set(additional_cols_definition)))


    next_month = str((pd.to_datetime(end_date) + pd.DateOffset(months=1)).date())
    filter_training_lgb = current_model_params.get("filter_training_lgb")
    if filter_training_lgb is None:
        filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
            and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
            and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
            and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
            and (data_points >= 10)
            and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""

    sql = f"""
    SELECT * 
    FROM train_vw
    WHERE sales_week_forecast>=date('{start_date}') AND sales_week_forecast<date('{next_month}')
    AND {filter_training_lgb}
    AND account_state IN {tuple(states) if len(states) > 1 else "('%s')" % ", ".join(map(repr, states))}
    """
    print(f"SQL for fetching training data for product {product_name_key}:\n{sql}")
    
    grouping_cols_for_select = ["product_lineup_sk", "sales_week_forecast", "account_state"]
    cols_for_udf_not_features = ["sales_week_start_date"] 

    if get_by_length_level_flag: grouping_cols_for_select.append("cig_length")
    if get_by_price_tier_flag: grouping_cols_for_select.append("price_tier")
    if get_by_menthol_flag: grouping_cols_for_select.append("menthol_non_menthol")
    
    all_selected_cols = sorted(list(set(num_feats + cat_feats + additional_cols_definition + grouping_cols_for_select + cols_for_udf_not_features)))
    
    print(f"Selecting columns for df: {all_selected_cols}")
    df_from_sql = spark.sql(sql)

    missing_cols_in_train_vw = [col for col in all_selected_cols if col not in df_from_sql.columns]
    if missing_cols_in_train_vw:
        print(f"Warning: The following columns specified for selection do not exist in 'train_vw' after SQL query: {missing_cols_in_train_vw}")
        all_selected_cols = [col for col in all_selected_cols if col in df_from_sql.columns]
        print(f"Revised columns for df: {all_selected_cols}")

    if not all_selected_cols:
        print("Error: No columns to select after checking. Aborting for this product.")
        return pd.DataFrame(), pd.DataFrame()

    df = df_from_sql.select(*all_selected_cols)


    decimals_cols = [
        x for x in df.columns if ("Decimal" in str(df.schema[x].dataType)) or ("Double" in str(df.schema[x].dataType)) # Fixed OR condition
    ]
    for column_to_cast in decimals_cols: # Renamed variable
        df = df.withColumn(column_to_cast, df[column_to_cast].cast(FloatType()))

    groups = grouping_cols_for_select # Use the explicitly defined grouping_cols_for_select

    print(f"Columns in df before groupBy: {df.columns}")
    print(f"Grouping by columns: {groups}")


    print("Processing Predictions")
    global get_by_length_level, get_by_price_tier, get_by_menthol # To modify globals for UDF
    get_by_length_level = get_by_length_level_flag
    get_by_price_tier = get_by_price_tier_flag
    get_by_menthol = get_by_menthol_flag
    
    # Create a temporary Delta table to break the lineage
    import uuid
    tmp_path = f"dbfs:/tmp/pe_accuracy_{product_lineup_sk}_{start_date}_{uuid.uuid4().hex[:8]}"

    results_temp = (
        df.withColumn("pe_type", lit("pe"))
        .filter( (F.col("rsd_quantity_roll4").isNotNull()) & (F.col("rsd_quantity_roll4") > 0) )
        .groupBy(groups)
        .applyInPandas(predict_sales, schema=result_schema)
    )

    # Write and read back to break lineage
    results_temp.write.mode("overwrite").format("delta").save(tmp_path)
    results = spark.read.format("delta").load(tmp_path)

    # Clean up temp table after processing (optional, add at the end of the function)
    # dbutils.fs.rm(tmp_path, recurse=True)

    safe_sum_actual_volume = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
    results_schema_cols = [field.name for field in result_schema] # Get column names from result_schema
    safe_actual_volume_for_mape = F.when(F.col("actual_volume").isNotNull() & (F.col("actual_volume") != 0), F.col("actual_volume")).otherwise(F.lit(None).cast(FloatType()))


    results_summarized_state = (
        results.groupBy("product_lineup_sk", "sales_week_forecast", "account_state")
        .agg(
            F.sum(F.abs(F.col("actual_volume") - F.col("predicted_volume"))).alias("sum_absolute_error"),
            F.sum("actual_volume").alias("sum_actual_volume"),
            F.sum("predicted_volume").alias("sum_predicted_volume"),
            F.countDistinct("account_sk").alias("nb_account"),
            F.mean(F.abs(F.col("actual_volume") - F.col("predicted_volume")) / safe_actual_volume_for_mape).alias("mape")
        )
        .withColumn("w_accuracy", 100 * (1 - F.col("sum_absolute_error") / safe_sum_actual_volume))
        .withColumn("w_accuracy_mape", 100 * (1 - F.col("mape")))
        .withColumn("w_accuracy_week_product_state", 100 * (1 - F.abs(F.col("sum_actual_volume") - F.col("sum_predicted_volume")) / safe_sum_actual_volume))
    )
    
    safe_total_actual_volume_national = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))

    results_summarized_national = (
        results_summarized_state.groupBy("product_lineup_sk")
        .agg(
            F.sum("sum_absolute_error").alias("total_sum_absolute_error"), # Alias to avoid conflict
            F.sum("sum_actual_volume").alias("total_sum_actual_volume"),   # Alias to avoid conflict
            F.sum("nb_account").alias("nb_account"),
            F.mean("w_accuracy_mape").alias("avg_w_accuracy_mape"), # Alias
            F.mean("w_accuracy_week_product_state").alias("avg_w_accuracy_week_product_state") #Alias
        )
        .withColumn( # Recalculate w_accuracy at national level
            "w_accuracy",
            100 * (1 - F.col("total_sum_absolute_error") / F.when(F.col("total_sum_actual_volume")!=0,F.col("total_sum_actual_volume")).otherwise(None) )
        )
    )
    return results_summarized_state.toPandas(), results_summarized_national.toPandas()

# COMMAND ----------

def find_mlflow_run_by_pl_sk(ml_runs, target_pl_sk):
    """
    Correctly searches for a product within a single MLflow run that contains multiple models.
    """
    for run in ml_runs:                           
        try:
            artifact_uri = run.info.artifact_uri
            # Load the entire parameter dictionary for the run
            all_params_in_run = mlflow.artifacts.load_dict(artifact_uri + "/parameters.json").get("parameters")
            if all_params_in_run is None:
                continue # Skip if the parameters file is missing or empty
            
            # **FIXED LOGIC**: Iterate through each model's parameters inside the file
            for model_key, product_params in all_params_in_run.items():
                if int(product_params.get("product_lineup_sk", -1)) == target_pl_sk:
                    # Found the matching product! 
                    # Return the parent run and the full dictionary of all parameters.
                    return run, all_params_in_run
        
        except Exception as e:
            # This can happen if parameters.json is missing, which we handle as a fallback.
            # We can make the warning less alarming.
            print(f"[INFO] Could not read 'parameters.json' for run {run.info.run_id}. This is expected if using the fallback. Details: {e}")
            continue

    # If the target_pl_sk was not found in any of the models within any of the runs
    return None, None

# COMMAND ----------



# COMMAND ----------

# DBTITLE 1,Helper to Reconstruct Parameters
def construct_parameters_from_config(config):
    """
    Reconstructs the 'parameters.json' dictionary from the main config file.
    This is a fallback for when the artifact is missing from an MLflow run.
    """
    print("--- Reconstructing model parameters from configuration file ---")
    reconstructed_params = {}
    products_config = config.get("products", {})
    
    # Get top-level parameters from the config
    include_macroeconomic = config.get("include_macroeconomic", 0)
    get_by_length_level = config.get("cig_length", 0)
    get_by_price_tier = config.get("price_tier", 0)
    get_by_menthol = config.get("menthol", 0)
    get_extra_num_feats_lgb = config.get("get_extra_num_feats_lgb", [])
    cpu_or_serving = config.get("cpu_or_serving")
    price_threshold = config.get("price_threshold")

    for product_name, product_details in products_config.items():
        product_lineup_sk = product_details.get("id")
        l_product_lineup_sk = product_details.get("other", [])
        
        # This is the crucial step: Call the same function used during training
        cat_feats, num_feats, _, _ = get_features_set(
            product_lineup_sk, 
            l_product_lineup_sk, 
            include_macroeconomic, 
            get_by_length_level, 
            get_by_price_tier, 
            get_by_menthol, 
            get_extra_num_feats_lgb
        )

        # Build the parameter dictionary for this specific product
        prod_params = {
            "product_lineup_sk": str(product_lineup_sk),
            "l_product_lineup_sk": l_product_lineup_sk,
            "cat_feats": cat_feats,
            "num_feats": num_feats,
            "include_macroeconomics": str(include_macroeconomic),
            "gb_cig_length": str(get_by_length_level),
            "gb_price_tier": str(get_by_price_tier),
            "gb_menthol": str(get_by_menthol),
            "cpu_or_serving": cpu_or_serving,
            "price_threshold": str(price_threshold)
            # Add any other parameters from the config that compute_predictions might need
        }
        
        # Use the same keying convention as the training script
        model_key = f"model_{product_name.lower().replace(' ', '').replace('/', '')}"
        reconstructed_params[model_key] = prod_params
        
    print(f"--- Successfully reconstructed parameters for {len(reconstructed_params)} products ---")
    return reconstructed_params

# COMMAND ----------

from pyspark.sql.functions import struct, col

predict_dates.sort() # Ensure sorted: oldest to newest
model = None # Initialize global model variable
get_by_length_level = 0
get_by_price_tier = 0
get_by_menthol = 0
current_cat_feats_for_udf = [] # This is the one causing the SyntaxError
results_national_all = []
results_state_all = []

if model_to_use == "all":
    for product in list(config["products"].keys()):
        print("-------------------------------")
        print("product", product)
        product_mlflow_key = f"model_{product.lower().replace(' ', '').replace('/', '')}"
        product_lineup_sk = config["products"][product]["id"]

        for predict_date in predict_dates:
            print("predict_date", predict_date)

            mlflow_run_name = f"{run_id}{export_suffix}/{predict_date}"
            print(f"Using specific model from MLflow run: {mlflow_run_name}")
            
            # Call our new helper function to have the reconstructed parameters ready
            reconstructed_parameters = construct_parameters_from_config(config)
            
            try:
                experiment_name_to_search = "/Shared/experiments/pe"
                experiments = client.search_experiments(filter_string=f"name='{experiment_name_to_search}'")
                if not experiments: 
                    raise ValueError(f"Experiment '{experiment_name_to_search}' not found.")
                target_experiment_id = experiments[0].experiment_id

                ml_runs = client.search_runs(
                    experiment_ids=[target_experiment_id],
                    filter_string=f"attributes.run_name = '{mlflow_run_name}'",
                    order_by=["start_time DESC"]
                )
                if not ml_runs:
                    raise ValueError(f"No MLflow run found with name '{mlflow_run_name}'")
            except Exception as e:
                print(f"Error fetching MLflow runs: {e}")

            selected_ml_run, parameters_for_single_model = find_mlflow_run_by_pl_sk(ml_runs, int(product_lineup_sk))
            if selected_ml_run is None:
                print(f"WARNING: No MLflow run found for product '{product_key_in_config}'. Skipping.")
                continue

            macroeconomics = int(parameters_for_single_model[product_mlflow_key].get("include_macroeconomics", 0))
            price_threshold_val = int(parameters_for_single_model[product_mlflow_key].get("price_threshold", 0))
            create_training_data_view(
                product_lineup_sk,
                config["products"][product].get("other", []),
                import_data_path,
                macroeconomics,
                price_threshold_val,
                gb_price_tier = int(parameters_for_single_model[product_mlflow_key].get("gb_price_tier", 0)),
                gb_cig_length = int(parameters_for_single_model[product_mlflow_key].get("gb_cig_length", 0)),
                gb_menthol = int(parameters_for_single_model[product_mlflow_key].get("gb_menthol", 0)),
                cpu_or_serving = parameters_for_single_model[product_mlflow_key].get("cpu_or_serving"),
                industry_unique_id = int(parameters_for_single_model[product_mlflow_key].get("price_threshold", 0))
            )
            next_month = str((pd.to_datetime(predict_date) + pd.DateOffset(months=1)).date())
            filter_training_lgb = parameters_for_single_model[product_mlflow_key].get("filter_training_lgb")
            if filter_training_lgb is None:
                filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
                    and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
                    and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
                    and (data_points >= 10)"""

            sql = f"""
            SELECT * 
            FROM train_vw
            WHERE sales_week_forecast>=date('{predict_date}') AND sales_week_forecast<date('{next_month}')
            AND {filter_training_lgb}
            AND account_state IN {tuple(states) if len(states) > 1 else "('%s')" % ", ".join(map(repr, states))}
            """
            print(sql)

            enable_features = parameters_for_single_model[product_mlflow_key]["num_feats"] + parameters_for_single_model[product_mlflow_key]["cat_feats"]
            all_selected_cols = list(set(enable_features + ["product_lineup_sk", "sales_week_forecast", "rsd_quantity_roll4", "account_sk", "rsd_quantity_y"]))
            df = spark.sql(sql).select(*all_selected_cols)
            decimals_cols = [
                x for x in df.columns if ("Decimal" in str(df.schema[x].dataType)) or ("Double" in str(df.schema[x].dataType)) # Fixed OR condition
            ]
            for column_to_cast in decimals_cols: # Renamed variable
                df = df.withColumn(column_to_cast, df[column_to_cast].cast(FloatType()))
            # for cat_col in parameters_for_single_model[product_mlflow_key]["cat_feats"]: 
            #     if cat_col in df.columns:
            #         df[cat_col] = df[cat_col].astype("category")

            # model = mlflow.pyfunc.load_model(f"runs:/{selected_ml_run.info.run_id}/main_model")
            loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=f"runs:/{selected_ml_run.info.run_id}/main_model")

            results = (
                df
                .filter( (F.col("rsd_quantity_roll4").isNotNull()) & (F.col("rsd_quantity_roll4") > 0) )
                .withColumn("predictions", loaded_model(struct(*map(col, df.columns))))
                .withColumn("predicted_volume", F.col("predictions") * F.col("rsd_quantity_roll4"))
                # rsd_quantity_y
            )
            results.cache()

            safe_actual_volume_for_mape = F.when(F.col("rsd_quantity_y").isNotNull() & (F.col("rsd_quantity_y") != 0), F.col("rsd_quantity_y")).otherwise(F.lit(None).cast(FloatType()))
            safe_sum_actual_volume = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
            results_summarized_state = (
                results.groupBy("product_lineup_sk", "sales_week_forecast", "account_state")
                .agg(
                    F.sum(F.abs(F.col("rsd_quantity_y") - F.col("predicted_volume"))).alias("sum_absolute_error"),
                    F.sum("rsd_quantity_y").alias("sum_actual_volume"),
                    F.sum("predicted_volume").alias("sum_predicted_volume"),
                    F.countDistinct("account_sk").alias("nb_account"),
                    F.mean(F.abs(F.col("rsd_quantity_y") - F.col("predicted_volume")) / safe_actual_volume_for_mape).alias("mape")
                )
                .withColumn("w_accuracy", 100 * (1 - F.col("sum_absolute_error") / safe_sum_actual_volume))
                .withColumn("w_accuracy_mape", 100 * (1 - F.col("mape")))
                .withColumn("w_accuracy_week_product_state", 100 * (1 - F.abs(F.col("sum_actual_volume") - F.col("sum_predicted_volume")) / safe_sum_actual_volume))
            )
            
            safe_total_actual_volume_national = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
            results_summarized_national = (
                results_summarized_state.groupBy("product_lineup_sk")
                .agg(
                    F.sum("sum_absolute_error").alias("total_sum_absolute_error"), # Alias to avoid conflict
                    F.sum("sum_actual_volume").alias("total_sum_actual_volume"),   # Alias to avoid conflict
                    F.sum("nb_account").alias("nb_account"),
                    F.mean("w_accuracy_mape").alias("avg_w_accuracy_mape"), # Alias
                    F.mean("w_accuracy_week_product_state").alias("avg_w_accuracy_week_product_state") #Alias
                )
                .withColumn( # Recalculate w_accuracy at national level
                    "w_accuracy",
                    100 * (1 - F.col("total_sum_absolute_error") / F.when(F.col("total_sum_actual_volume")!=0,F.col("total_sum_actual_volume")).otherwise(None) )
                )
            )
            results_summarized_state = results_summarized_state.toPandas()
            results_summarized_national = results_summarized_national.toPandas()
            results_summarized_state["model_date"], results_summarized_national["model_date"] = predict_date, predict_date
            results_summarized_state["predict_start"], results_summarized_national["predict_start"] = predict_date, predict_date
            results_summarized_state["predict_end"], results_summarized_national["predict_end"] = next_month, next_month
            results_summarized_state["prod_name"], results_summarized_national["prod_name"] = product, product
            results_state_all.append(results_summarized_state)
            results_national_all.append(results_summarized_national)
        print('------------------------')
        print()
    if results_national_all:
        results_national_pd = pd.concat(results_national_all)
        display(results_national_pd)
    else:
        results_national_pd = pd.DataFrame()
    
    results_national_pd_processed = (
        results_national_pd
            .rename(columns={"predict_start": "campaign_start_date", "predict_end": "campaign_end_date"}).groupby(["product_lineup_sk", "prod_name"]).agg({
                "campaign_start_date": "min",
                "campaign_end_date": "max",
                "total_sum_absolute_error": "sum",
                "total_sum_actual_volume": "sum",
                "avg_w_accuracy_mape": "mean",
                "avg_w_accuracy_week_product_state": "mean"
            })
    )
    results_national_pd_processed['w_accuracy'] = results_national_pd_processed.apply(
            lambda x: 100 * (1 - x.total_sum_absolute_error / x.total_sum_actual_volume) if x.total_sum_actual_volume and x.total_sum_actual_volume != 0 else None,
            axis=1
        )
    display(results_national_pd_processed.reset_index())
    accuracy_base_path = f"{folder}{run_id}{export_suffix}/other_files/accuracy_rsd/" 
    df_accuracy_spark = spark.createDataFrame(results_national_pd_processed.reset_index())

    # delta folder per product_lineup_sk
    product_keys = (df_accuracy_spark
                    .select("prod_name")
                    .distinct()
                    .collect())

    for row in product_keys:
        prod_name_val = row["prod_name"]
        folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

        per_prod_df = df_accuracy_spark.filter(F.col("prod_name") == F.lit(prod_name_val))
        per_product_path = f"{accuracy_base_path}{folder_name}"
        (per_prod_df
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .save(per_product_path))

    print(f"Per-product accuracy written under: {accuracy_base_path}")

# COMMAND ----------

if model_to_use == "all":
    dbutils.notebook.exit(json.dumps({"success": True}))

# COMMAND ----------

if model_to_use != "all":

    predict_dates.sort() # Ensure sorted: oldest to newest
    model = None # Initialize global model variable
    get_by_length_level = 0
    get_by_price_tier = 0
    get_by_menthol = 0
    current_cat_feats_for_udf = [] # This is the one causing the SyntaxError

    mlflow_run_name = f"{run_id}{export_suffix}/{model_to_use}"
    print(f"Using specific model from MLflow run: {mlflow_run_name}")
    
    # Call our new helper function to have the reconstructed parameters ready
    reconstructed_parameters = construct_parameters_from_config(config)
    
    try:
        experiment_name_to_search = "/Shared/experiments/pe"
        experiments = client.search_experiments(filter_string=f"name='{experiment_name_to_search}'")
        if not experiments: 
            raise ValueError(f"Experiment '{experiment_name_to_search}' not found.")
        target_experiment_id = experiments[0].experiment_id

        ml_runs = client.search_runs(
            experiment_ids=[target_experiment_id],
            filter_string=f"attributes.run_name = '{mlflow_run_name}'",
            order_by=["start_time DESC"]
        )
        if not ml_runs:
            raise ValueError(f"No MLflow run found with name '{mlflow_run_name}'")

    except Exception as e:
        print(f"Error fetching MLflow runs: {e}")
        results_state_all    = []
        results_national_all = []
        results_national_pd  = pd.DataFrame()

    else:
        results_state_all = []
        results_national_all = []

        for product_key_in_config, product_id in zip(product_names, product_id_list):

            # This function will still get the MLflow run, but 'parameters_for_single_model' might be None
            selected_ml_run, parameters_for_single_model = find_mlflow_run_by_pl_sk(ml_runs, int(product_id))

            if selected_ml_run is None:
                print(f"WARNING: No MLflow run found for product '{product_key_in_config}'. Skipping.")
                continue

            # ** THIS IS THE KEY CHANGE **
            # If parameters were not found in the artifact, use our reconstructed ones.
            if parameters_for_single_model is None:
                print(f"WARNING: 'parameters.json' not found in MLflow run for '{product_key_in_config}'. Using reconstructed parameters from config file.")
                parameters_for_single_model = reconstructed_parameters
            
            globals()['model'] = mlflow.pyfunc.load_model(f"runs:/{selected_ml_run.info.run_id}/main_model")

            # Now, 'parameters_for_single_model' is guaranteed to be populated (if the product is in the config)
            if parameters_for_single_model:
                for date_to_predict_on in predict_dates:
                    print(f"--- Predicting for date: {date_to_predict_on} using model from {model_to_use} ---")
                    print(f"Processing product: {product_key_in_config} for prediction date {date_to_predict_on}")
                    product_mlflow_key = f"model_{product_key_in_config.lower().replace(' ', '').replace('/', '')}"

                    if product_mlflow_key not in parameters_for_single_model:
                        print(f"Warning: Parameters for '{product_mlflow_key}' not found in reconstructed params. Skipping product.")
                        continue
                    
                    prod_specific_mlflow_params = parameters_for_single_model.get(product_mlflow_key)

                    if product_key_in_config not in products_config:
                        print(f"Warning: Product key '{product_key_in_config}' not found in main products_config. Skipping.")
                        continue
                    main_prod_config_details = products_config[product_key_in_config]
                    p_l_sk = int(main_prod_config_details.get("id"))
                    l_p_l_sk = main_prod_config_details.get("other", [])

                    globals()['current_cat_feats_for_udf'] = prod_specific_mlflow_params.get("cat_feats", [])
                    result_state_df, result_national_df = compute_predictions(
                        product_name_key=product_key_in_config,
                        product_lineup_sk=p_l_sk,
                        l_product_lineup_sk=l_p_l_sk,
                        start_date=date_to_predict_on,
                        end_date=date_to_predict_on,
                        current_model_params=prod_specific_mlflow_params
                    )
                    results_state_all.append(result_state_df)
                    results_national_all.append(result_national_df)
                if results_national_all:
                    results_national_pd = pd.concat(results_national_all)
                else:
                    results_national_pd = pd.DataFrame()
            else:
                print(f"Skipping predictions as parameters for model {mlflow_run_name} could not be loaded or reconstructed.")
                results_national_pd = pd.DataFrame()

# COMMAND ----------

if not products_config: # Check if products_config is empty or None
    print("Warning: products_config is empty. Cannot create names_df for merging.")
    names_df = pd.DataFrame(columns=["product_lineup_sk", "prod_name"])
else:
    names = []
    ids = []
    for name, content in products_config.items(): # Use products_config
        names.append(name) # This is the product key like "VUSE ALTO POD MEN 4CART"
        ids.append(content.get('id')) # This is the product_lineup_sk
    names_df = pd.DataFrame({ "product_lineup_sk": ids, "prod_name": names})

# COMMAND ----------

if 'results_national_pd' not in locals() or results_national_pd.empty:
    print("results_national_pd is not defined or empty. Skipping merge and display of national results.")
    results_national_pd_merged_display = pd.DataFrame() # Ensure it exists as an empty DF
else:
    # Ensure product_lineup_sk types match for merging
    results_national_pd['product_lineup_sk'] = results_national_pd['product_lineup_sk'].astype(int)
    if not names_df.empty: # names_df is defined in a previous cell
        names_df['product_lineup_sk'] = names_df['product_lineup_sk'].astype(int)
        results_national_pd_merged_display = results_national_pd.merge(names_df, on='product_lineup_sk', how='left').sort_values(['prod_name', 'product_lineup_sk'])
    else:
        results_national_pd_merged_display = results_national_pd.sort_values(['product_lineup_sk']).copy() # Sort by available key
        results_national_pd_merged_display['prod_name'] = 'Unknown (names_df empty)'

    if 'predict_dates' in locals() and predict_dates:
        datetime_predict_dates = pd.to_datetime(predict_dates)
        
        campaign_start_date = datetime_predict_dates.min()
        
        # The end date is the end of the month of the latest start date in predict_dates
        latest_month_start_in_campaign = datetime_predict_dates.max()
        campaign_end_date = latest_month_start_in_campaign + pd.offsets.MonthEnd(0)
        
        results_national_pd_merged_display['campaign_start_date'] = campaign_start_date.strftime('%Y-%m-%d')
        results_national_pd_merged_display['campaign_end_date'] = campaign_end_date.strftime('%Y-%m-%d')
        
        # Reorder columns to have dates appear earlier if desired
        cols = results_national_pd_merged_display.columns.tolist()
        if 'prod_name' in cols and 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            try:
                prod_name_idx = cols.index('prod_name')
                # Remove them
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                # Insert them after prod_name
                cols.insert(prod_name_idx + 1, 'campaign_end_date')
                cols.insert(prod_name_idx + 1, 'campaign_start_date')
                results_national_pd_merged_display = results_national_pd_merged_display[cols]
            except ValueError:
                # prod_name not found, just keep original order with new columns at the end
                pass
        elif 'product_lineup_sk' in cols and 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            # Fallback: place after product_lineup_sk if prod_name is not there
            try:
                pl_sk_idx = cols.index('product_lineup_sk')
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                cols.insert(pl_sk_idx + 1, 'campaign_end_date')
                cols.insert(pl_sk_idx + 1, 'campaign_start_date')
                results_national_pd_merged_display = results_national_pd_merged_display[cols]
            except ValueError:
                 pass 

    else:
        print("Warning: 'predict_dates' variable not found or empty. Cannot add campaign date columns to results_national_pd_merged_display.")
        results_national_pd_merged_display['campaign_start_date'] = None
        results_national_pd_merged_display['campaign_end_date'] = None

    display(results_national_pd_merged_display)

# COMMAND ----------

if 'results_national_pd_merged_display' in locals() and not results_national_pd_merged_display.empty:
    if 'prod_name' not in results_national_pd_merged_display.columns:
        print("Warning: 'prod_name' column missing in results_national_pd_merged_display. Adding placeholder.")
        results_national_pd_merged_display['prod_name'] = 'Unknown'
    
    aggregation_dict = {
        "total_sum_absolute_error": "sum",  # Use the aliased name from national results
        "total_sum_actual_volume": "sum",   # Use the aliased name from national results
        "avg_w_accuracy_mape": "mean",
        "avg_w_accuracy_week_product_state": "mean"
    }
    print(f"Aggregating with dict: {aggregation_dict}")
    print(f"Columns available in results_national_pd_merged_display: {results_national_pd_merged_display.columns.tolist()}")

    global_result = (
        results_national_pd_merged_display.groupby(["product_lineup_sk", "prod_name"])
        .agg(aggregation_dict) # Use the corrected dictionary
        .reset_index()
        .sort_values('prod_name')
    )
    global_result['w_accuracy'] = global_result.apply(
        lambda x: 100 * (1 - x.total_sum_absolute_error / x.total_sum_actual_volume) if x.total_sum_actual_volume and x.total_sum_actual_volume != 0 else None,
        axis=1
    )

    if 'predict_dates' in locals() and predict_dates:
        # Convert to datetime objects if they are strings
        datetime_predict_dates = pd.to_datetime(predict_dates)
        
        campaign_start_date = datetime_predict_dates.min()
        
        # The end date is the end of the month of the latest start date in predict_dates
        latest_month_start_in_campaign = datetime_predict_dates.max()
        campaign_end_date = latest_month_start_in_campaign + pd.offsets.MonthEnd(0)
        
        global_result['campaign_start_date'] = campaign_start_date.strftime('%Y-%m-%d')
        global_result['campaign_end_date'] = campaign_end_date.strftime('%Y-%m-%d')
        
        # Reorder columns to have dates appear earlier if desired
        cols = global_result.columns.tolist()
        # Example: move campaign dates after prod_name
        if 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            # Find index of prod_name
            try:
                prod_name_idx = cols.index('prod_name')
                # Remove them
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                # Insert them after prod_name
                cols.insert(prod_name_idx + 1, 'campaign_end_date')
                cols.insert(prod_name_idx + 1, 'campaign_start_date')
                global_result = global_result[cols]
            except ValueError:
                # prod_name not found, just keep original order with new columns at the end
                pass

    else:
        print("Warning: 'predict_dates' variable not found or empty. Cannot add campaign date columns.")
        global_result['campaign_start_date'] = None
        global_result['campaign_end_date'] = None
    # ---- END ADDED SECTION ----

    display(global_result)
else:
    print("No national results available to compute global_result.")
    global_result = pd.DataFrame()

if 'global_result' in locals() and not global_result.empty:
    print("Final Global Result (already displayed if computed):")
else:
    print("Global result is empty.")

# COMMAND ----------

if 'global_result' in locals() and not global_result.empty:
    accuracy_output_path = f"{folder}{run_id}/other_files/accuracy_rsd/"
    print(f"Writing final accuracy results to Delta table: {accuracy_output_path}")
    df_accuracy_spark = spark.createDataFrame(global_result)
    df_accuracy_spark.write.format("delta").mode('overwrite').option("overwriteSchema", "true").save(accuracy_output_path)
    print("Successfully wrote accuracy results.")
else:
    print("Skipping write operation because 'global_result' DataFrame is not available or is empty.")

</file>

<file path="notebooks/other/drift.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Drift Notebook
# MAGIC
# MAGIC Compute the pe drift and the data drift: number of states that drifted over the last 3 months per product.

# COMMAND ----------

import json
# dbutils.widgets.removeAll()
dbutils.widgets.text("folder", "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/")
dbutils.widgets.text("run_id", "2023/julie/reexecute_allprods/alloff") # Example run_id
dbutils.widgets.text("product_list", '[6]')
dbutils.widgets.text("industry_unique_id", "1")  # Example run_id


folder = dbutils.widgets.get("folder").strip() 
run_id = dbutils.widgets.get("run_id").strip()
industry_unique_id = dbutils.widgets.get("industry_unique_id")
product_list = json.loads(dbutils.widgets.get("product_list"))
dbutils.widgets.text("delta", "1") 
delta_input = int(dbutils.widgets.get("delta"))
dbutils.widgets.text("export_suffix", "") 
export_suffix = dbutils.widgets.get("export_suffix")

print(f"folder: {folder}")
print(f"run_id: {run_id}")
print(f"industry_unique_id: {industry_unique_id}")
print(f"product_list: {product_list}")

# COMMAND ----------

import plotly.express as px
import numpy as np
import pandas as pd
from pandas.tseries.offsets import MonthBegin
pd.options.plotting.backend = "plotly"

BAT_COLORS = [
    '#0e2b63',  
    '#004f9f',  
    '#00b1eb',  
    '#ef7d00',  
    '#ffbb00',  
    '#50af47',  
    '#afca0b',  
    '#5a328a',  
    '#e72582',  
]

# PE data
import plotly.express as px
import numpy as np
import pandas as pd
from pandas.tseries.offsets import MonthBegin
pd.options.plotting.backend = "plotly"

BAT_COLORS = [
    '#0e2b63',  
    '#004f9f',  
    '#00b1eb',  
    '#ef7d00',  
    '#ffbb00',  
    '#50af47',  
    '#afca0b',  
    '#5a328a',  
    '#e72582',  
]

# --- START OF MODIFIED "PE data" SECTION ---
# PE data
# pe_path = f"{folder}/{run_id}/*/computed_pe/" # This general path is less useful for specific iteration
base_run_path = f"{folder.rstrip('/')}/{run_id.rstrip('/')}/"

if delta_input == 1:
    print(f"DELTA MODE: Reading PE data from Delta sources under {base_run_path}<date_folder>/computed_pe{export_suffix}/")
    all_pe_spark_dfs_collected = [] 
    date_folders_found = []
    
    try:
        for item in dbutils.fs.ls(base_run_path):
            if item.isDir() and item.name[:-1].count('-') == 2 and len(item.name[:-1]) == 10:
                date_folders_found.append(item.path)
        
        if not date_folders_found:
            raise ValueError(f"No date-like subdirectories found under {base_run_path}")
        print(f"Found date folders for PE data: {date_folders_found}")

        for date_folder_path in date_folders_found:
            # Include export_suffix in the path
            pe_path_target = f"{date_folder_path.rstrip('/')}/computed_pe{export_suffix}/"
            print(f"  Processing date folder: {date_folder_path}, looking in {pe_path_target}")
            
            delta_tables_to_read_for_current_date = []
            try:
                for file_info in dbutils.fs.ls(pe_path_target):
                    if file_info.isDir() and file_info.name.endswith('_delta/'):
                        delta_tables_to_read_for_current_date.append(file_info.path)
                
                if not delta_tables_to_read_for_current_date:
                    print(f"    No '*_delta/' subdirectories found in {pe_path_target}. Skipping.")
                    continue

                df_list_for_current_date = []
                for p_delta_path in delta_tables_to_read_for_current_date:
                    print(f"      Reading Delta table: {p_delta_path}")
                    try:
                        df_list_for_current_date.append(spark.read.format("delta").load(p_delta_path))
                    except Exception as e_read_specific_delta:
                        print(f"      WARNING: Failed to read Delta table {p_delta_path}. Error: {e_read_specific_delta}")
                
                if df_list_for_current_date:
                    if len(df_list_for_current_date) == 1:
                        all_pe_spark_dfs_collected.append(df_list_for_current_date[0])
                    else:
                        from functools import reduce
                        from pyspark.sql import DataFrame
                        unioned_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), df_list_for_current_date)
                        all_pe_spark_dfs_collected.append(unioned_df)
            except Exception as e_ls_pe_target:
                print(f"    WARNING: Could not process {pe_path_target}. Error: {e_ls_pe_target}")

        if not all_pe_spark_dfs_collected:
            raise ValueError(f"No PE Delta tables successfully read from any date folder under {base_run_path}")

        if len(all_pe_spark_dfs_collected) == 1:
            final_pe_spark_df = all_pe_spark_dfs_collected[0]
        else:
            from functools import reduce
            from pyspark.sql import DataFrame
            print(f"Unioning {len(all_pe_spark_dfs_collected)} collected PE Spark DataFrames.")
            final_pe_spark_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), all_pe_spark_dfs_collected)
        
        final_pe_spark_df.createOrReplaceTempView("pe")
        print("Successfully created 'pe' view from combined Delta sources.")

    except Exception as e:
        raise Exception(f"Critical error during Delta PE data loading process. Error: {e}")

else: # delta_input == 0 (Parquet)
    # Include export_suffix in the Parquet path pattern
    pe_read_path = f"{base_run_path}*/computed_pe{export_suffix}/*.parquet"
    print(f"PARQUET MODE: Reading PE data from Parquet path pattern: {pe_read_path}")
    try:
        spark.read.parquet(pe_read_path).createOrReplaceTempView("pe")
        print("Successfully created 'pe' view from Parquet sources.")
    except Exception as e2:
         raise Exception(f"Failed to read PE data from Parquet path {pe_read_path}. Error: {e2}")
        
df_pe_check = spark.sql("SELECT 1 FROM pe LIMIT 1")
if df_pe_check.count() == 0:
    raise ValueError(f"No data loaded into 'pe' view from path: {base_run_path} (delta={delta_input})")

products = ", ".join(map(str, product_list))
df_pe = spark.sql(f"""select product_lineup_sk, simulated_price_product_lineup_sk, account_state, account_sk,
                  rsd_quantity, pe, predict_date, int(datediff(predict_date, max_week)/7) weeks
                  from pe
                  where product_lineup_sk in ({products})
                  """).toPandas()
if df_pe.empty:
    raise ValueError(f"Pandas DataFrame df_pe is empty after reading from 'pe' view (delta={delta_input}). Check source data.")
unique_predict_dates = sorted(df_pe.predict_date.unique())
if not unique_predict_dates:
    raise ValueError("No prediction dates found in the input data for drift analysis.")
elif len(unique_predict_dates) >= 4:
    start_date_drift = unique_predict_dates[-4]
else:
    start_date_drift = unique_predict_dates[0]
print(f"Using start_date_drift: {start_date_drift}")
data_s3_base_path = f"{base_run_path}data/"
        
df_pe_check = spark.sql("SELECT 1 FROM pe LIMIT 1")
if df_pe_check.count() == 0:
    raise ValueError(f"No data loaded into 'pe' view from path: {pe_path} (delta={delta_input})")

products = ", ".join(map(str, product_list))
df_pe = spark.sql(f"""select product_lineup_sk, simulated_price_product_lineup_sk, account_state, account_sk,
                  rsd_quantity, pe, predict_date, int(datediff(predict_date, max_week)/7) weeks
                  from pe
                  where product_lineup_sk in ({products})
                  """).toPandas()
if df_pe.empty:
    raise ValueError(f"Pandas DataFrame df_pe is empty after reading from 'pe' view (delta={delta_input}). Check source data.")
unique_predict_dates = sorted(df_pe.predict_date.unique())
if not unique_predict_dates:
    raise ValueError("No prediction dates found in the input data for drift analysis.")
elif len(unique_predict_dates) >= 4:
    start_date_drift = unique_predict_dates[-4]
else:
    start_date_drift = unique_predict_dates[0]
print(f"Using start_date_drift: {start_date_drift}")
data_s3_base_path = f"{base_run_path}data/" # Common base for product/industry data

if delta_input == 1:
    print(f"DELTA MODE: Reading product/industry data from Delta sources in {data_s3_base_path}delta/")
    df_product_delta_path_primary = f"{data_s3_base_path}delta/df_product"
    df_product_delta_path_alt = f"{data_s3_base_path}delta/df_product_delta"
    df_industry_delta_path_primary = f"{data_s3_base_path}delta/df_industry"
    df_industry_delta_path_alt = f"{data_s3_base_path}delta/df_industry_delta"

    try:
        print(f"  Reading product data from: {df_product_delta_path_primary}")
        spark.read.format("delta").load(df_product_delta_path_primary).filter(f"product_lineup_sk in ({products})").createOrReplaceTempView("df_product")
    except Exception:
        print(f"  Failed. Reading product data from alternative: {df_product_delta_path_alt}")
        spark.read.format("delta").load(df_product_delta_path_alt).createOrReplaceTempView("df_product")
    print(f"Successfully created 'df_product' view from Delta.")
    
    try:
        print(f"  Reading industry data from: {df_industry_delta_path_primary}")
        spark.read.format("delta").load(df_industry_delta_path_primary).filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("df_industry")
    except Exception:
        print(f"  Failed. Reading industry data from alternative: {df_industry_delta_path_alt}")
        spark.read.format("delta").load(df_industry_delta_path_alt).createOrReplaceTempView("df_industry")
    print(f"Successfully created 'df_industry' view from Delta.")

else: # delta_input == 0 (Parquet)
    df_product_parquet_path = f"{folder}/{run_id}/data/df_product.parquet"
    df_industry_parquet_path = f"{folder}/{run_id}/data/df_industry.parquet"

    print(f"PARQUET MODE: Reading product data from: {df_product_parquet_path}")
    spark.read.parquet(df_product_parquet_path).createOrReplaceTempView("df_product")
    print(f"PARQUET MODE: Reading industry data from: {df_industry_parquet_path}")
    spark.read.parquet(df_industry_parquet_path).createOrReplaceTempView("df_industry")
    print("Successfully created 'df_product' and 'df_industry' views from Parquet.")
unique_pl_sk_for_query = [str(e) for e in df_pe.product_lineup_sk.unique()]
if not unique_pl_sk_for_query:
    print("WARNING: No unique product_lineup_sk found in PE data. df_raw will be empty.")
    df_raw_schema = "account_state STRING, product_lineup_sk BIGINT, sales_week_start_date DATE, rsd_quantity DOUBLE, rsd_price DOUBLE, rsd_quantity_industry DOUBLE, rsd_price_industry DOUBLE"
    df_raw = spark.createDataFrame([], df_raw_schema).toPandas()
else:
    sql = f"""select account_state, df_product.product_lineup_sk, sales_week_start_date
    , sum(rsd_quantity) rsd_quantity
    , sum(rsd_price*rsd_quantity)/sum(rsd_quantity) rsd_price
    , sum(rsd_quantity_industry) rsd_quantity_industry
    , sum(rsd_price_industry*rsd_quantity_industry)/sum(rsd_quantity_industry) rsd_price_industry
    from df_product
    left join df_industry using (account_sk, sales_week_sk)
    where rsd_price>1 and rsd_quantity>0 and rsd_price_industry>1 and rsd_quantity_industry>0
    and df_product.product_lineup_sk in ({", ".join(unique_pl_sk_for_query)})
    group by account_state, df_product.product_lineup_sk, sales_week_start_date
    """
    df_raw = spark.sql(sql).toPandas()

l_c = ["rsd_quantity", "rsd_price", "rsd_quantity_industry", "rsd_price_industry"]
# Ensure columns exist before astype conversion, handle if df_raw is empty
for col_name in l_c:
    if col_name in df_raw.columns:
        df_raw[col_name] = df_raw[col_name].astype(float)
    else:
        if not df_raw.empty: 
             print(f"Warning: Column '{col_name}' not found in df_raw. Skipping astype conversion for it.")

df_name = spark.sql("select distinct product_lineup_sk, kpl_preferred_name from df_product").toPandas()

# COMMAND ----------

# DBTITLE 1,Function (same as pe-enhancement repo)
class Drift():
    
    def __init__(self, start_date_drift="2021-01-01"):
        self.start_date_drift = start_date_drift
                
    def _drift_calculator(self, df, feature):

        # Build rolling intervals to detect significant deviations
        if feature=="pe__":
            df[f"{feature} - sd"] = df["pe_roll"] - df[f"sd_{feature}_interval"] # current interval
            df[f"{feature} + sd"] = df["pe_roll"] + df[f"sd_{feature}_interval"] # current interval
        else:
            df[f"{feature} - sd"] = df[f"{feature}"] - df[f"sd_{feature}_interval"] # current interval
            df[f"{feature} + sd"] = df[f"{feature}"] + df[f"sd_{feature}_interval"] # current interval
        df.loc[:, f"{feature}_min_interval"] = np.nan # will be equal to previous intervals if possible
        df.loc[:, f"{feature}_max_interval"] = np.nan # will be equal to previous intervals if possible
        df.loc[:, f"{feature}_drift"] = False # indicates if there is a drift or not

        l_drift_dates = [
            df.loc[(~df[f"sd_{feature}_interval"].isna()) & (df.predict_date >= self.start_date_drift), "predict_date"].min()
        ]
        
        while(len(l_drift_dates)>0):    
            drift_date = l_drift_dates[0] # take the first date where the value is outside the range

            df.loc[df.predict_date > drift_date, [f"{feature}_min_interval", f"{feature}_max_interval"]] = (
                df.loc[df.predict_date == drift_date, [f"{feature} + sd",f"{feature} - sd"]].values
            )
            df.loc[df.predict_date == drift_date, f"{feature}_drift"] = True

            loc = (
                (df["predict_date"] > drift_date)
                & ( (df[feature] > df[f"{feature}_min_interval"]) | (df[feature] < df[f"{feature}_max_interval"]) )
            )
            l_drift_dates = df.loc[loc].predict_date.values

        return df
    
    @staticmethod
    def _lambda_drift_to_month(x):
        if x == 0:
            return 12 
        elif x > 4 / 12:
            return 1
        elif x > 2 / 12:
            return 3
        else:
            return 6

    def compute_drift(self, df, gb, feature):
        
        def process_feature(df):
            return self._drift_calculator(df, feature)
        # df = pd.concat(applyParallel(df.groupby(gb), process_feature, 1))
        df = df.groupby(gb).apply(process_feature)

        return df, df[df.predict_date>self.start_date_drift].groupby(gb).apply(lambda x: pd.Series({
            f"{feature}_drift": self._lambda_drift_to_month(x[f"{feature}_drift"].mean()),
            f"{feature}": x[f"{feature}"].mean(),
            f"{feature}_avg_interval": np.abs(
                x[f"{feature}_max_interval"].mean()-x[f"{feature}_min_interval"].mean()
            ),
        }))

def pe_drift(pe, product_lineup_sk, account_state, gb, display_data=True, constant_sd=2, start_date_drift="2021-01-01"):
    print(f"Selection: {pe}, {product_lineup_sk}, {account_state}, {gb}")

    if pe == "pe":
        df_tmp = df_pe[(df_pe.product_lineup_sk == df_pe.simulated_price_product_lineup_sk)] # select the pe numbers
    else:
        df_tmp = df_pe[(df_pe.product_lineup_sk != df_pe.simulated_price_product_lineup_sk)] # select the cross-pe numbers

    # filter the pe by widget selection
    if product_lineup_sk != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.product_lineup_sk == product_lineup_sk), :]
    if account_state != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.account_state == account_state), :]
    #if account_county != "ALL": df_tmp = df_tmp[(df_tmp.account_county==account_county)]

    df_tmp = (
        df_tmp
        .query("rsd_quantity>=5")
        .query("weeks<=25")
        .groupby(list(gb)+["predict_date"]).apply(lambda x: pd.Series({
            "pe": (x.pe * x.rsd_quantity).sum() / x.rsd_quantity.sum(),
        }))
        .reset_index()
    )
    df_tmp["pe_roll"] = df_tmp.groupby(list(gb))["pe"].rolling(3, min_periods=1).mean().reset_index(drop=True)
    df_tmp.predict_date = df_tmp.predict_date.astype(str)
    df_tmp["sd_pe"] = df_tmp.groupby(list(gb))[["pe"]].transform(lambda x: x.rolling(12, min_periods=3).std())
    df_tmp["sd_pe_interval"] = constant_sd * df_tmp["sd_pe"]
    
    if pe == "pe": # adjust the inerval for pe, not for cross-pe
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe"].apply(lambda x: np.max([x, 0.1])) # do not retrain if the variation of pe is less than 0.5
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe_interval"].apply(lambda x: np.min([x, 0.1])) # retrain if the variation of pe is more than 0.1
    else:
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe"].apply(lambda x: np.max([x, 0.01]))

    drift = Drift(start_date_drift) # we are interested by the pe drift after start_date_drift
    df_tmp, df_res = drift.compute_drift(df_tmp, list(gb), "pe")
    
    if display_data:
        display(df_res.reset_index())
    
    return df_res, df_tmp

def data_drift(product_lineup_sk, account_state, gb, constant_sd=2, start_date_drift="2020-01-01"):
    
    df_tmp = df_raw.copy()[df_raw.sales_week_start_date >= pd.to_datetime(start_date_drift)]
    
    if product_lineup_sk != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.product_lineup_sk == product_lineup_sk), :]
    if account_state != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.account_state == account_state), :]
    
    df_tmp = (
        df_tmp.groupby(list(gb)+["sales_week_start_date"]).apply(lambda x: pd.Series({
            "rsd_quantity": x.rsd_quantity.sum(),
            "rsd_price": (x.rsd_price*x.rsd_quantity).sum() / x.rsd_quantity.sum(),
            "rsd_quantity_industry": x.rsd_quantity_industry.sum(),
            "rsd_price_industry": (x.rsd_price_industry*x.rsd_quantity_industry).sum() / x.rsd_quantity_industry.sum(),
        }))
        .reset_index()
    )
    df_tmp["rsd_price_perc"] = df_tmp["rsd_price"] / df_tmp["rsd_price_industry"]
    df_tmp["som"] = 100 * (df_tmp["rsd_quantity"] / df_tmp["rsd_quantity_industry"])
    
    # group data per month
    df_tmp["sales_week_start_date"] = pd.DatetimeIndex(df_tmp["sales_week_start_date"])
    df_tmp = df_tmp.set_index(["sales_week_start_date"]).groupby(list(gb)).resample('M')[["rsd_quantity", "rsd_price",
       "rsd_quantity_industry", "rsd_price_industry", "rsd_price_perc", "som"]].mean()
    df_tmp.reset_index(inplace=True)
    df_tmp["predict_date"] = df_tmp["sales_week_start_date"] + MonthBegin(1)
    
    drift = Drift(start_date_drift) # we are interested by the drift after start_date_drift
    
    print("start drift computation")
    l_results = []
    # We want the drift for each of the fetaures in the list
    for feature in ["rsd_quantity","rsd_price","rsd_quantity_industry","rsd_price_industry","rsd_price_perc","som"]:
        df_tmp[f"sd_{feature}"] = df_tmp.groupby(list(gb))[feature].transform(lambda x: x.rolling(12, min_periods=3).std())
        df_tmp[f"sd_{feature}_interval"] = constant_sd*df_tmp[f"sd_{feature}"]
        if feature=="rsd_price_perc":
            df_tmp[f"sd_{feature}_interval"] = 0.1
        elif feature=="rsd_price":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 5% of price variation
        elif feature=="rsd_price_industry":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 5% of price variation
        elif feature=="som":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 10% of som variation
        df_tmp, df_res = drift.compute_drift(df_tmp, list(gb), feature)
        l_results.append(df_res)
    print("end drift computation")
                
    return df_tmp, pd.concat(l_results, ignore_index=False, axis=1)

# COMMAND ----------

# DBTITLE 1,PE Drift
df_res, df_details = pe_drift("pe", product_lineup_sk="ALL", account_state="ALL", gb=["product_lineup_sk", "account_state"], start_date_drift=start_date_drift, display_data=False)
df_drift = df_details[(df_details.predict_date > start_date_drift) & (df_details.pe_drift == True)][["product_lineup_sk", "account_state", "predict_date", "pe_drift"]]

df_drift_counts = (
    df_drift
    .groupby(["product_lineup_sk", "account_state"])
    .size() # Use size() for counting rows in groups, more efficient than count() on a specific column
    .reset_index(name='count_per_state') # Get product_lineup_sk, account_state, count_per_state
    .query("count_per_state >= 2")
    .groupby("product_lineup_sk")
    .size() # Count how many states meet the criteria per product
    .reset_index(name="pe_drift") # Get product_lineup_sk, pe_drift (count of states)
)

df_all_products = df_details[["product_lineup_sk"]].drop_duplicates()

df_merged_counts = pd.merge(
    df_all_products,
    df_drift_counts,
    on="product_lineup_sk",
    how="left" # Keep all products from df_all_products
)

if 'product_lineup_sk' not in df_name.columns:
    df_name_reset = df_name.reset_index()
else:
    df_name_reset = df_name.copy()


df_tmp = pd.merge(
    df_merged_counts,
    df_name_reset[['product_lineup_sk', 'kpl_preferred_name']], # Select only necessary columns from df_name
    on="product_lineup_sk",
    how="inner" # Keep only products that have names
).fillna({'pe_drift': 0}) # Fill only the count column with 0

if df_drift.empty:
    print("No PE drift details to display (df_drift is empty).")
else:
    display(df_drift)

df_tmp["product_lineup_sk"] = df_tmp["product_lineup_sk"].astype(str)
fig = px.bar(df_tmp, x="kpl_preferred_name", y="pe_drift", color_discrete_sequence=BAT_COLORS, title="Number of instances of pe drift")
fig.update_yaxes(range=[0,55])
fig.show()


# COMMAND ----------

df_pe_drift_state = (
    df_drift
    .groupby(["product_lineup_sk","account_state"])
    .size() # Count occurrences per group
    .reset_index(name='pe_drift_count') # Rename the count column
    .query("pe_drift_count >= 2") # Filter based on the count
)
if df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty).")
else:
    display(df_pe_drift_state)

# COMMAND ----------

if df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty).")
else:
    display(
        df_tmp.loc[:, ['kpl_preferred_name', 'pe_drift']]
        .sort_values('kpl_preferred_name')
    )

# COMMAND ----------

import plotly.express as px
import pandas as pd

if 'df_pe_drift_state' not in locals() or df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty or not defined). Thus, no PE plot for drifted states will be generated.")
else:
    print(f"Plotting PE for {len(df_pe_drift_state)} drifted product-state combinations.")
    
    # Select only the product_lineup_sk and account_state from df_pe_drift_state
    drifted_states_identifiers = df_pe_drift_state[['product_lineup_sk', 'account_state']].drop_duplicates()
    
    # Filter df_details to get PE time series for these drifted states
    # df_details contains the 'pe' column and 'predict_date' needed for plotting
    df_plot_data = pd.merge(
        df_details, 
        drifted_states_identifiers, 
        on=['product_lineup_sk', 'account_state'], 
        how='inner'
    )
    
    if df_plot_data.empty:
        print("No PE data found in df_details for the identified drifted states.")
    else:
        # Merge with df_name to get kpl_preferred_name for richer labels
        # Ensure df_name has the required columns; df_name was created earlier in the notebook
        if 'product_lineup_sk' not in df_name.columns:
             # If product_lineup_sk is an index
            df_name_to_merge = df_name.reset_index()
        else:
            df_name_to_merge = df_name.copy()

        df_plot_data = pd.merge(
            df_plot_data,
            df_name_to_merge[['product_lineup_sk', 'kpl_preferred_name']],
            on='product_lineup_sk',
            how='left'
        )
        
        # Create a comprehensive legend label
        # Fill missing kpl_preferred_name with product_lineup_sk for robustness
        df_plot_data['kpl_preferred_name_filled'] = df_plot_data['kpl_preferred_name'].fillna(
            'SK: ' + df_plot_data['product_lineup_sk'].astype(str)
        )
        df_plot_data['legend_label'] = df_plot_data['kpl_preferred_name_filled'] + ' - ' + df_plot_data['account_state']
        
        # Ensure predict_date is in datetime format for plotting
        df_plot_data['predict_date'] = pd.to_datetime(df_plot_data['predict_date'])
        
        # Sort data for consistent line plotting
        df_plot_data = df_plot_data.sort_values(by=['legend_label', 'predict_date'])
        
        # Generate the plot
        fig_drifted_pe = px.line(
            df_plot_data,
            x='predict_date',
            y='pe',  # Plotting the 'pe' column as requested
            color='legend_label',
            markers=True,
            title='PE Over Time for States with Significant Drift (pe_drift_count >= 2)'
        )
        
        fig_drifted_pe.update_layout(
            xaxis_title='Prediction Date',
            yaxis_title='Price Elasticity (pe)',
            legend_title='Product - State'
        )
        
        fig_drifted_pe.show()

# COMMAND ----------

#Saving PE drift results per products
from pyspark.sql import functions as F
pe_drift_base_path  = f"{folder}{run_id}/other_files/pe_drift{export_suffix}/" 
print(f"Writing PE drift results to Delta table: {pe_drift_base_path}")
df_pe_drift_spark = spark.createDataFrame(df_tmp)

product_names = (df_pe_drift_spark.select("kpl_preferred_name").distinct().collect())

for row in product_names:
    prod_name_val = row["kpl_preferred_name"]
    folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

    per_prod_df = df_pe_drift_spark.filter(F.col("kpl_preferred_name") == F.lit(prod_name_val))
    per_product_path = f"{pe_drift_base_path}{folder_name}"

    (per_prod_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(per_product_path))

print(f"Per-product PE drift written under: {pe_drift_base_path}")


# COMMAND ----------

# DBTITLE 1,Data Drift
df_details_data, df_res_data = data_drift(product_lineup_sk="ALL", account_state="ALL", gb=["product_lineup_sk", "account_state"], start_date_drift=start_date_drift) # Renamed df_details to avoid conflict
df_drift_data = df_details_data[(df_details_data.predict_date>start_date_drift) & ( # Use df_details_data
    (df_details_data.rsd_quantity_drift==True) | (df_details_data.rsd_price_drift==True) |
    (df_details_data.rsd_quantity_industry_drift==True) | (df_details_data.rsd_price_industry_drift==True) |
    (df_details_data.rsd_price_perc_drift==True) | (df_details_data.som_drift==True)
)][["product_lineup_sk", "account_state", "predict_date"]+[c for c in df_details_data.columns if "_drift" in c]] # Use df_details_data

if df_drift_data.empty:
    print("No Data drift details to display (df_drift_data is empty).")
else:
    display(df_drift_data) # Use df_drift_data


l_data_drift = []
for field in ["rsd_quantity_drift", "rsd_price_drift", "rsd_quantity_industry_drift",
              "rsd_price_industry_drift", "som_drift"]:
    # "rsd_price_perc_drift" not needed
    l_data_drift.append(
        df_drift_data # Use df_drift_data
        .query(f"{field}==True")
        .groupby(["product_lineup_sk","account_state"])
        .size() # Use size()
        .reset_index(name='count_per_state') # Use size()
        .query(f"count_per_state>=2") # Adjusted query
        .groupby("product_lineup_sk")
        .size() # Count products meeting criteria
        .rename(field) # Rename the series to the field name
    )

df_combined_data_drift = pd.concat(l_data_drift, axis=1) # Concatenate Series into a DataFrame, index is product_lineup_sk

df_all_products_data = df_details_data[["product_lineup_sk"]].drop_duplicates()

df_merged_data_counts = pd.merge(
    df_all_products_data,
    df_combined_data_drift,
    left_on="product_lineup_sk",
    right_index=True, # Join left column with right index
    how="left" # Keep all products
)

df_melted = df_merged_data_counts.melt(
    id_vars="product_lineup_sk",
    var_name="variable",
    value_name="value"
)

if 'product_lineup_sk' not in df_name.columns:
    df_name_reset = df_name.reset_index()
else:
    df_name_reset = df_name.copy()


df_tmp = pd.merge(
    df_melted,
    df_name_reset[['product_lineup_sk', 'kpl_preferred_name']], # Select necessary columns
    on="product_lineup_sk",
    how="inner" # Keep only products that have names
).fillna({'value': 0}) # Fill only the count column 'value' with 0


df_tmp["product_lineup_sk"] = df_tmp["product_lineup_sk"].astype(str)
df_tmp = df_tmp.query('~kpl_preferred_name.isin(["NJOY ACE PUK", "NJOY ACE DEVICE"])')
fig = px.bar(df_tmp, x="kpl_preferred_name", y="value", color="variable", color_discrete_sequence=BAT_COLORS[1:],
       barmode = "group", labels={"value": "Number of instances of data drift"},
       title="Number of instances of data drift")
fig.update_yaxes(range=[0,55])
fig.show()

# COMMAND ----------

# --- Weighted PE (wPE) time series for all products (own-price only) ---

import pandas as pd
import plotly.express as px

# Optional: respect the widget-provided product_list if it exists
product_filter = ""
try:
    if 'product_list' in locals() and product_list:
        product_filter = f"AND product_lineup_sk IN ({', '.join(map(str, product_list))})"
except NameError:
    pass

sql = f"""
with agg as (
    select predict_date, pe_type, product_lineup_sk, simulated_price_product_lineup_sk, count(*) nb_rows
    , sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
    , sum(rsd_quantity_cross) rsd_quantity_cross
    , count(distinct account_sk) n_acccount
    from pe 
    where product_lineup_sk = simulated_price_product_lineup_sk
    {product_filter}
    group by predict_date, pe_type, product_lineup_sk, simulated_price_product_lineup_sk
),
names AS (
  SELECT DISTINCT product_lineup_sk, kpl_preferred_name
  FROM df_product
)
SELECT
    a.product_lineup_sk,
    a.predict_date,
    a.wpe,
    COALESCE(n.kpl_preferred_name, CAST(a.product_lineup_sk AS STRING)) AS product_name
FROM agg a
LEFT JOIN names n USING (product_lineup_sk)
ORDER BY product_name, predict_date
"""

df_ts = spark.sql(sql).toPandas()
if df_ts.empty:
    print("No data found for wPE time series.")
else:
    df_ts["predict_date"] = pd.to_datetime(df_ts["predict_date"])
    df_ts = df_ts.sort_values(["product_name", "predict_date"])

    fig = px.line(
        df_ts,
        x="predict_date",
        y="wpe",
        color="product_name",
        markers=True,
        title="Weighted PE over time (own-price)"
    )
    fig.update_layout(
        xaxis_title="Predict date",
        yaxis_title="Weighted PE (wPE)",
        xaxis=dict(rangeslider=dict(visible=True))
    )
    try:
        display(fig.show(renderer="png"))
    except Exception:
        display(fig)

# COMMAND ----------

# Saving Data drift results per product
data_drift_base_path  = f"{folder}{run_id}/other_files/data_drift{export_suffix}/" 
print(f"Writing Data drift results to Delta table: {data_drift_base_path}")
df_data_drift_spark = spark.createDataFrame(df_tmp)


product_names_dd = (df_data_drift_spark.select("kpl_preferred_name").distinct().collect())

for row in product_names_dd:
    prod_name_val = row["kpl_preferred_name"]
    folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

    per_prod_df = df_data_drift_spark.filter(F.col("kpl_preferred_name") == F.lit(prod_name_val))
    per_product_path = f"{data_drift_base_path}{folder_name}"

    (per_prod_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(per_product_path))

print(f"Per-product Data drift written under: {data_drift_base_path}")


</file>

<file path="notebooks/utils.py.py">
# Databricks notebook source
delta = 1

# COMMAND ----------

# DBTITLE 1,delta helpers
import fnmatch
from typing import List, Union
from pyspark.sql import SparkSession, DataFrame
from functools import reduce

def list_all_dirs(path: str) -> List[str]:
    """
    Recursively list all directories under a given path using dbutils.fs.
    """
    all_dirs = []

    def recurse(p):
        try:
            items = dbutils.fs.ls(p)
        except Exception as e:
            print(f"⚠️ Cannot access {p}: {e}")
            return

        for item in items:
            if item.isDir():
                all_dirs.append(item.path)
                recurse(item.path)

    recurse(path)
    return all_dirs

def find_matching_delta_paths(base_path: str, patterns: List[str]) -> List[str]:
    """
    From all subdirs under base_path, return those that:
    - Match any pattern in patterns (using fnmatch)
    - Contain a `_delta_log/` subfolder (i.e., a Delta table)
    """
    all_dirs = list_all_dirs(base_path)
    matching_paths = []

    for dir_path in all_dirs:
        # Check if dir_path matches any pattern
        if any(fnmatch.fnmatch(dir_path.rstrip('/'), pattern.rstrip('/')) for pattern in patterns):
            # Check if _delta_log exists inside
            try:
                _ = dbutils.fs.ls(f"{dir_path.rstrip('/')}/_delta_log")
                matching_paths.append(dir_path.rstrip('/'))
            except:
                continue

    return matching_paths

def load_delta_tables_with_wildcards(

    spark: SparkSession,
    pattern_paths: List[str],
    union_all: bool = True,
    verbose: bool = True
) -> Union[DataFrame, List[DataFrame]]:
    """
    Load Delta tables from a list of wildcard patterns using dbutils.fs + fnmatch.
    """
    if not pattern_paths:
        raise ValueError("pattern_paths list is empty")

    # Extract base path from first pattern (everything before first '*')
    base_paths = set()
    for pattern in pattern_paths:
        split_index = pattern.find("*")
        if split_index == -1:
            base_path = pattern
        else:
            base_path = pattern[:split_index].rpartition("/")[0] + "/"
        base_paths.add(base_path)

    delta_paths = []
    for base_path in base_paths:
        delta_paths.extend(find_matching_delta_paths(base_path, pattern_paths))

    if not delta_paths:
        raise ValueError(f"No Delta tables found matching patterns: {pattern_paths}")

    dfs = []
    for path in delta_paths:
        try:
            df = spark.read.format("delta").load(path)
            dfs.append(df)
            if verbose:
                print(f"✔ Loaded: {path}")
        except Exception as e:
            if verbose:
                print(f"✘ Skipped: {path} — {e}")

    if not dfs:
        raise RuntimeError("No Delta tables could be loaded.")

    if union_all:
        return reduce(DataFrame.unionByName, dfs)
    else:
        return dfs
    
def load_delta_tables_explicit(
    spark: SparkSession,
    paths: List[str],
    union_all: bool = True,
    verbose: bool = True
) -> Union[DataFrame, List[DataFrame]]:
    """
    Load and optionally union multiple explicit Delta table paths (no wildcards).
    """
    if not paths:
        raise ValueError("paths list is empty")

    valid_paths = []
    for path in paths:
        try:
            _ = dbutils.fs.ls(path.rstrip("/") + "/_delta_log")
            valid_paths.append(path.rstrip("/"))
            if verbose:
                print(f"✔ Valid Delta table: {path}")
        except:
            if verbose:
                print(f"✘ Skipped (no _delta_log): {path}")

    if not valid_paths:
        raise ValueError("No valid Delta tables found in provided paths.")

    dfs = []
    for path in valid_paths:
        try:
            df = spark.read.format("delta").load(path)
            dfs.append(df)
        except Exception as e:
            if verbose:
                print(f"✘ Error loading {path}: {e}")

    if not dfs:
        raise RuntimeError("No Delta tables could be loaded.")

    return reduce(DataFrame.unionByName, dfs) if union_all else dfs

# COMMAND ----------

# DBTITLE 1,ML model - SQL queries
def create_training_data_view(product_lineup_sk, l_product_linup_sk, export_path, macroeconomics, price_threshold, gb_price_tier=0, gb_cig_length=0, gb_menthol=0, cpu_or_serving='cpu', industry_unique_id=0):

    """ Create a training data view `train_vw` to use as the training data for the lgb model and for the simulation model that is using lgb
    inputs: 
     - product_lineup_sk: the product we are interested in, each product has it's own model
     - l_product_linup_sk: the competitors of the product product_lineup_sk
    """
    if industry_unique_id == 0:
      print("missing industry_unique_id")
      return 0
    if delta == 1:
      spark.read.format("delta").load(f"{export_path}/data/delta/df_industry").filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("industry")
      spark.read.format("delta").load(f"{export_path}/data/delta/df_product").createOrReplaceTempView("raw_product")
      if macroeconomics:
        spark.read.format("delta").load(f"{export_path}/data/delta/df_macrolevels_processed").createOrReplaceTempView("macrolevels")
    else:
      spark.read.parquet(f"{export_path}/data/df_industry.parquet").filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("industry")
      spark.read.parquet(f"{export_path}/data/df_product.parquet").createOrReplaceTempView("raw_product")
      if macroeconomics:
        spark.read.parquet(f"{export_path}/data/df_macrolevels_processed.parquet").createOrReplaceTempView("macrolevels")

    # if cpu_or_serving == 'cpu':
    #     price_threshold = 1
    # if cpu_or_serving == 'serving':
    #     price_threshold = 0.2
    # if cpu_or_serving == 'cartridge':
    #     price_threshold = 1


    # group the product data depending if we want to group it by price_tier, cig_length, menthol
    sql = f"""create or replace temp view product as (
                    select account_sk
                    , product_lineup_sk
                    , kpl_preferred_name
                    , sales_week_sk
                    , {"price_tier" if gb_price_tier else "NULL"} price_tier
                    , {"cig_length" if gb_cig_length else "NULL"} cig_length
                    , {"menthol_non_menthol" if gb_menthol else "NULL"} menthol_non_menthol
                    , sum(rsd_quantity) rsd_quantity
                    , sum(rsd_price*rsd_quantity)/sum(rsd_quantity) rsd_price
                    from raw_product
                    group by 1,2,3,4,5,6,7
                    )"""
    spark.sql(sql)

    # clean the product data prices/volumes
    sql = f"""
    create or replace temp view product_clean_data as (
        with interquartile_price as ( -- there is a lof of wrong prices in the db, we are trying to clean most of them with this query
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol
            , percentile(case when rsd_price >= {price_threshold} then rsd_price else null end, 0.1) q10
            , percentile(case when rsd_price >= {price_threshold} then rsd_price else null end, 0.9) q90
            from product
            where product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
            group by product_lineup_sk, price_tier, cig_length, menthol_non_menthol
        ),
        interquartile_price_range as (
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol
            , q10
            , q90
            , (q90 - q10) * 1.5 cut_off -- interquartile range = q90 - q10
            from interquartile_price
        ),
        interquartile_price_thresholds as (
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol

            , q10 - cut_off lower
            , q90 + cut_off upper
            from interquartile_price_range
        )
        select account_sk
        , sales_week_sk
        , product.product_lineup_sk
        , kpl_preferred_name
        , product.price_tier
        , product.cig_length
        , product.menthol_non_menthol
        , case when rsd_quantity >= 0 then rsd_quantity else 0 end rsd_quantity
        , case when rsd_price >= {price_threshold} and rsd_price > lower and rsd_price < upper then rsd_price else null end rsd_price
        from product 
        inner join interquartile_price_thresholds i
            on product.product_lineup_sk=i.product_lineup_sk
            {"and product.price_tier=i.price_tier" if gb_price_tier else ""}
            {"and product.cig_length=i.cig_length" if gb_cig_length else ""}
            {"and product.menthol_non_menthol=i.menthol_non_menthol" if gb_menthol else ""}
    )
    """
    spark.sql(sql)
    
    sql_prod = f"""
    --create or replace temp view train_vw as 
    with product_vw as (
    select account_sk
      , sales_week_sk
      , product_clean_data.product_lineup_sk
      , kpl_preferred_name
      , product_clean_data.price_tier
      , product_clean_data.cig_length
      , product_clean_data.menthol_non_menthol
      , rsd_quantity
      , rsd_price
    from product_clean_data
    where product_clean_data.product_lineup_sk={product_lineup_sk}
    ),
    competitors_vw as (
    select * from (
      select product_clean_data.product_lineup_sk
        , sales_week_sk
        , account_sk
        , sum(rsd_quantity) rsd_quantity
        , sum(case when rsd_price is not null then rsd_price*rsd_quantity else 0 end)/sum(case when rsd_price is not null then rsd_quantity else 0 end) rsd_price
      from product_clean_data
      where product_clean_data.product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
      group by product_clean_data.product_lineup_sk, sales_week_sk, account_sk
    ) pivot (
      first(rsd_quantity) rsd_quantity, first(rsd_price) rsd_price for product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
    )
    ),
    min_sk as (
    select account_sk, min(sales_week_start_date) min_sales_week_start_date
    from industry
    group by account_sk
    ),
    joined as (
    select to_timestamp(industry.sales_week_sk) sales_week_sk
      , to_timestamp(sales_week_start_date) Time
      , industry.sales_week_start_date
      , case when datediff(industry.sales_week_start_date, min_sales_week_start_date)/7 <= 52 then datediff(industry.sales_week_start_date, min_sales_week_start_date)/7 else 52 end data_points -- data_points for the account_sk
      , industry.account_sk
      , industry.account_state
      , industry.l1_account_name
      , industry.l1_account_code
      , case when industry.rsd_quantity_industry >= 0 then industry.rsd_quantity_industry else 0 end rsd_quantity_industry
      , case when industry.rsd_price_industry >= {price_threshold} then industry.rsd_price_industry else null end rsd_price_industry
      , product_vw.kpl_preferred_name
      , product_vw.product_lineup_sk
      , product_vw.price_tier
      , product_vw.cig_length
      , product_vw.menthol_non_menthol
      , coalesce(product_vw.rsd_quantity, 0) rsd_quantity
      , product_vw.rsd_price
      {", " if len(l_product_linup_sk)>0 else ""} {", ".join([f"coalesce(competitors_vw.{pid}_rsd_quantity, 0) {pid}_rsd_quantity" for pid in l_product_linup_sk])}
      {", " if len(l_product_linup_sk)>0 else ""} {", ".join([f"competitors_vw.{pid}_rsd_price" for pid in l_product_linup_sk])}

      -- in case we want to compute a price elasticity for a product by price_tier or cig_length,
      -- we need to have this product with the other price_tier and cig_length as competitor
      -- because there can be cross-correlations between the two
      , coalesce(competitors_vw.{product_lineup_sk}_rsd_quantity - product_vw.rsd_quantity, 0) {product_lineup_sk}_rsd_quantity
      , (competitors_vw.{product_lineup_sk}_rsd_quantity*competitors_vw.{product_lineup_sk}_rsd_price - product_vw.rsd_quantity*product_vw.rsd_price)/(competitors_vw.{product_lineup_sk}_rsd_quantity - product_vw.rsd_quantity) {product_lineup_sk}_rsd_price
    
    from industry 
    left join product_vw on industry.account_sk = product_vw.account_sk and industry.sales_week_sk = product_vw.sales_week_sk
    left join competitors_vw on industry.account_sk = competitors_vw.account_sk and industry.sales_week_sk = competitors_vw.sales_week_sk
    left join min_sk on industry.account_sk = min_sk.account_sk
    ),
    ffill as (
    select joined.*
      , last_value(rsd_price, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) rsd_price_ffill
      , last_value(rsd_price_industry, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) rsd_price_industry_ffill
      , {", ".join([f"last_value({pid}_rsd_price, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) {pid}_rsd_price_ffill" for pid in l_product_linup_sk+[product_lineup_sk]])}
      from joined
    ),
    compute_revenues as (
    select ffill.*
      , coalesce(rsd_quantity * rsd_price_ffill, 0) rsd_revenue
      , coalesce(rsd_quantity_industry * rsd_price_industry_ffill, 0) rsd_revenue_industry
      , {", ".join([f"coalesce({pid}_rsd_quantity * {pid}_rsd_price_ffill, 0) {pid}_rsd_revenue" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from ffill
    ),
    rolled as (
    select sales_week_sk, Time, sales_week_start_date
      , data_points
      , account_sk, account_state
      , l1_account_name, l1_account_code
      , rsd_quantity_industry
      , rsd_price_industry_ffill rsd_price_industry
      , kpl_preferred_name
      , price_tier
      , cig_length
      , menthol_non_menthol
      , product_lineup_sk
      , rsd_quantity
      , rsd_price_ffill rsd_price
      , {", ".join([f"{pid}_rsd_quantity" for pid in l_product_linup_sk+[product_lineup_sk]])}
      , {", ".join([f"{pid}_rsd_price_ffill {pid}_rsd_price" for pid in l_product_linup_sk+[product_lineup_sk]])}
      
      -- mean revenue with min periods
      , mean(rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_revenue_roll4
      , mean(rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_revenue_roll13
      , mean(rsd_revenue_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_revenue_industry_roll4
      , {", ".join([f"mean({pid}_rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) {pid}_rsd_revenue_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}
      
      -- mean quantity
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 1 weeks preceding and current row) rsd_quantity_roll2
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_quantity_roll4
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 7 weeks preceding and current row) rsd_quantity_roll8
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_quantity_roll13
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 25 weeks preceding and current row) rsd_quantity_roll26
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 1 weeks preceding and current row) rsd_quantity_industry_roll2
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_quantity_industry_roll4
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 7 weeks preceding and current row) rsd_quantity_industry_roll8
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_quantity_industry_roll13
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 25 weeks preceding and current row) rsd_quantity_industry_roll26
      , {", ".join([f"mean({pid}_rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) {pid}_rsd_quantity_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}

      from compute_revenues
    ),
    ffill_price as (
    select rolled.*
      , case when rsd_revenue_roll4 / rsd_quantity_roll4 > 0 then rsd_revenue_roll4 / rsd_quantity_roll4 else null end rsd_price_roll4
      , case when rsd_revenue_industry_roll4 / rsd_quantity_industry_roll4 > 0 then rsd_revenue_industry_roll4 / rsd_quantity_industry_roll4 else null end  rsd_price_industry_roll4
      , {", ".join([f"case when {pid}_rsd_revenue_roll4 / {pid}_rsd_quantity_roll4 > 0 then {pid}_rsd_revenue_roll4 / {pid}_rsd_quantity_roll4 else null end {pid}_rsd_price_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from rolled
    ),
    lags as (
    select ffill_price.*
      , mean(rsd_revenue_roll4/rsd_quantity_roll4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_price_roll4_roll13
      , lag(rsd_quantity_roll4, 52) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_roll4_prev_y
      , lag(sales_week_start_date, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) sales_week_forecast
      , lag(rsd_quantity_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_y
      , lag(rsd_price_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_price_y
      , lag(rsd_quantity_industry_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_industry_y
      , lag(rsd_price_industry_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_price_industry_y
      , {", ".join([f"lag({pid}_rsd_price_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) {pid}_rsd_price_y" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from ffill_price
    ),
    all_data as (
    select lags.*
        , rsd_price_y / rsd_price_roll4 rsd_price_perc
        , rsd_price_y - rsd_price_roll4 rsd_price_diff
        , rsd_quantity_y / rsd_quantity_roll4 rsd_quantity_perc
        , rsd_quantity_industry_y / rsd_quantity_industry_roll4 rsd_quantity_ind_perc
        , rsd_price_industry_y / rsd_price_industry_roll4 rsd_price_ind_perc
        , rsd_price_roll4 / rsd_price_industry_roll4 rsd_price_perc_ind
        , rsd_quantity_roll2 / rsd_quantity_industry_roll2 rsd_quantity_roll2_perc
        , rsd_quantity_roll4 / rsd_quantity_industry_roll4 rsd_quantity_roll4_perc
        , {", ".join([f"{pid}_rsd_quantity_roll4 / rsd_quantity_industry_roll4 {pid}_rsd_quantity_roll4_perc" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , rsd_quantity_roll8 / rsd_quantity_industry_roll8 rsd_quantity_roll8_perc
        , rsd_quantity_roll13 / rsd_quantity_industry_roll13 rsd_quantity_roll13_perc
        , rsd_quantity_roll26 / rsd_quantity_industry_roll26 rsd_quantity_roll26_perc
        , rsd_quantity_industry_roll2 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll2_perc
        , rsd_quantity_industry_roll4 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll4_perc
        , rsd_quantity_industry_roll8 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll8_perc
        , rsd_quantity_industry_roll13 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll13_perc
        , rsd_quantity_industry_roll26 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll26_perc
        , {", ".join([f"coalesce({pid}_rsd_price_y / {pid}_rsd_price_roll4, 1) {pid}_rsd_price_perc" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , {", ".join([f"coalesce({pid}_rsd_price_y - {pid}_rsd_price_roll4, 0) {pid}_rsd_price_diff" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , {", ".join([f"{pid}_rsd_price_roll4 - rsd_price_roll4 {pid}_rsd_price_ref" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , (rsd_revenue_roll4/rsd_quantity_roll4) / (rsd_price_roll4_roll13) relative_price_hist
        , sin((month(sales_week_start_date)-1)*(2.*pi()/12)) month_sin
        , cos((month(sales_week_start_date)-1)*(2.*pi()/12)) month_cos
        , log(1 + rsd_quantity_roll4/5) weight
    from lags
    )
    select all_data.*
    from all_data
    where rsd_price_perc is not null 
     and rsd_quantity_roll4 is not null 
     and rsd_quantity_roll4 > 1 
     and rsd_price_roll4 >= 0
     and product_lineup_sk is not null
    order by account_sk, cig_length, price_tier, menthol_non_menthol, sales_week_start_date

    """
    spark.sql(sql_prod).createOrReplaceTempView('final_product_data')
    
    # if macroeconomic features are required
    if macroeconomics:
        indicator_names = ['Unemployment rate', 'Inflation, consumer price index - % year-on-year', 'Consumer spending, nominal, US$ - Total consumer spending',
                          'Consumer spending, nominal, US$ - Tobacco', 'Gasoline price, retail, regular grade', 'Consumer price index']
        
#         sql_macro = f"""with process_date as ( 
#                    select *
#                    , case when macro_economic_quarter like '%1' then to_date(replace(macro_economic_quarter, 'Q1', '-01-01'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%2' then to_date(replace(macro_economic_quarter, 'Q2', '-04-01'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%3' then to_date(replace(macro_economic_quarter, 'Q3', '-07-01'), 'yyyy-MM-dd')
#                           else to_date(replace(macro_economic_quarter, 'Q4', '-10-01'), 'yyyy-MM-dd') end quarter_start_date
#                   , case when macro_economic_quarter like '%1' then to_date(replace(macro_economic_quarter, 'Q1', '-03-31'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%2' then to_date(replace(macro_economic_quarter, 'Q2', '-06-30'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%3' then to_date(replace(macro_economic_quarter, 'Q3', '-09-30'), 'yyyy-MM-dd')
#                           else to_date(replace(macro_economic_quarter, 'Q4', '-12-31'), 'yyyy-MM-dd') end quarter_end_date
#             from macrolevels
#         ),
#         macro_pivoted as (
#              select quarter_start_date
#                    , quarter_end_date
#                    , cast(`Unemployment rate` as float) unemployment_rate
#                    , cast(`Inflation, consumer price index - % year-on-year` as float) inflation_yoy
#                    , cast(`Consumer spending, nominal, US$ - Total consumer spending` as float) consumer_spending_total
#                    , cast(`Consumer spending, nominal, US$ - Tobacco` as float) consumer_spending_tobacco
#                    , cast(`Gasoline price, retail, regular grade` as float) gasoline_price
#                    , cast(`Consumer price index` as float) consumer_price_index
#              from (
#                  select *
#                 from process_date
#              ) pivot (
#                  first(agg_value) agg_value for indicator_name in {tuple(indicator_names)}
#            )
#         )
#         select * 
#         from final_product_data
#           left join macro_pivoted
#         on final_product_data.sales_week_start_date between macro_pivoted.quarter_start_date and macro_pivoted.quarter_end_date
#         """

        sql_macro = f"""with process_date as ( 
                    select *
                     from macrolevels
         ),
        macro_pivoted as (
              select macro_economic_week
                    , cast(`Unemployment rate` as float) unemployment_rate
                    , cast(`Inflation, consumer price index - % year-on-year` as float) inflation_yoy
                    , cast(`Consumer spending, nominal, US$ - Total consumer spending` as float) consumer_spending_total
                    , cast(`Consumer spending, nominal, US$ - Tobacco` as float) consumer_spending_tobacco
                    , cast(`Gasoline price, retail, regular grade` as float) gasoline_price
                    , cast(`Consumer price index` as float) consumer_price_index

              from (
                  select *
                 from process_date
              ) pivot (
                  first(agg_value) agg_value for indicator_name in {tuple(indicator_names)}
            )
        ),
        
        lags_macros as(
            select macro_economic_week,
                lag(unemployment_rate, 4) over ( order by macro_economic_week) unemployment_rate_prev_m,
                lag(inflation_yoy, 4) over ( order by macro_economic_week) inflation_yoy_prev_m,
                lag(consumer_spending_total, 4) over ( order by macro_economic_week) consumer_spending_total_prev_m,
                lag(consumer_spending_tobacco, 4) over ( order by macro_economic_week) consumer_spending_tobacco_prev_m,
                lag(gasoline_price, 4) over ( order by macro_economic_week) gasoline_price_prev_m,
                lag(consumer_price_index, 4) over (order by macro_economic_week) consumer_price_index_prev_m
            from macro_pivoted    
        ),
        all_macros as (
            select macro.*, 
                lag_macros.unemployment_rate_prev_m,
                lag_macros.inflation_yoy_prev_m,
                lag_macros.consumer_spending_total_prev_m,
                lag_macros.consumer_spending_tobacco_prev_m ,
                lag_macros.gasoline_price_prev_m ,
                lag_macros.consumer_price_index_prev_m ,
                case when macro.unemployment_rate / lag_macros.unemployment_rate_prev_m > 0 then macro.unemployment_rate / lag_macros.unemployment_rate_prev_m - 1 else null end unemployment_rate_change,
                case when macro.inflation_yoy / lag_macros.inflation_yoy_prev_m > 0 then macro.inflation_yoy / lag_macros.inflation_yoy_prev_m - 1 else null end inflation_yoy_change,
                case when macro.consumer_spending_total / lag_macros.consumer_spending_total_prev_m > 0 then macro.consumer_spending_total / lag_macros.consumer_spending_total_prev_m - 1 else null end consumer_spending_total_change,
                case when macro.consumer_spending_tobacco / lag_macros.consumer_spending_tobacco_prev_m > 0 then macro.consumer_spending_tobacco / lag_macros.consumer_spending_tobacco_prev_m - 1 else null end consumer_spending_tobacco_change,
                case when macro.gasoline_price / lag_macros.gasoline_price_prev_m > 0 then macro.gasoline_price / lag_macros.gasoline_price_prev_m - 1 else null end gasoline_price_change,
                case when macro.consumer_price_index / lag_macros.consumer_price_index_prev_m > 0 then macro.consumer_price_index / lag_macros.consumer_price_index_prev_m - 1 else null end consumer_price_index_change
            from   macro_pivoted macro
            join  lags_macros lag_macros
            on macro.macro_economic_week = lag_macros.macro_economic_week
        )
        
        select final_product_data.*, 
                all_macros.*

        from final_product_data
            left join all_macros 
                on final_product_data.sales_week_start_date = all_macros.macro_economic_week
            """
        spark.sql(sql_macro).createOrReplaceTempView('final_product_data')

    sql = f"""create or replace temp view train_vw as
                select * from final_product_data
                -- where condition as NewportportMenXSS has no more data on CA in 2023 due to a ban on menthol brands
                where (product_lineup_sk != 63261) or 
                      (product_lineup_sk == 63261 and (account_state != 'CA' or (account_state=='CA' and sales_week_forecast < '2022-11-01')))
                order by account_sk,  
                cig_length,
                price_tier,
                menthol_non_menthol,
                sales_week_start_date
           """
    spark.sql(sql)

# COMMAND ----------

def get_extra_info(export_path):
  if delta == 1:
    spark.read.format("delta").load(f"{export_path}/data/delta/df_product").createOrReplaceTempView("product")
  else:
      spark.read.parquet(f"{export_path}/data/df_product.parquet").createOrReplaceTempView("product")
  
  sql = """
    create or replace temp view extra_info_vw as
      select distinct account_sk, product_lineup_sk, kpl_preferred_name
      , price_tier, cig_length, menthol_non_menthol
      from product
  """
  spark.sql(sql)

# COMMAND ----------

# DBTITLE 1,Load functions for kpi view
# MAGIC %run ./utils_kpi.py

# COMMAND ----------

# DBTITLE 1,List of kpi variables per industry
from typing import List


def kpi_variables_industry(industry: str) -> List:
    if industry == "fmc":
        kpi_variables = [
            "latitude_norm", "longitude_norm",
            "VOL_TC_norm",
            "IDX_TC_norm",
            "share_of_BR_MARLBORO",
            "share_of_BR_NATURAL_AMERICAN_SPIRIT",
            "share_of_BR_NEWPORT",
            "share_of_BR_PALL_LUCKYSTR",
            "share_of_BR_CAMEL_CRUSH",
            "share_of_BR_CAMEL_EX_CRUSH",
            "share_of_MFR_ITG_CIG_Pro_forma",
            "share_of_MFR_NON_BIG_3_Pro_forma",
        ]
    elif industry == "vapor":
        kpi_variables = [
            "latitude_norm", "longitude_norm",
            # "VOL_TV_norm",
            # "IDX_TV_norm",
            "share_of_FLV_MEN",
            "share_of_FLV_NM",
            "share_of_FLV_XM_NM",
            "share_of_br_blu",
            "share_of_br_juul",
            "share_of_br_logic",
            "share_of_br_njoy",
            "share_of_br_vuse",
            "share_of_br_others",
            # "share_of_strg_2",
            # "share_of_strg_2_4",
            # "share_of_strg_4",
            # "share_of_cart_2",
            # "share_of_cart_2_3",
            # "share_of_cart_4",
            # "share_of_sub_liq_bott",
            # "share_of_sub_kits",
            # "share_of_sub_rpl_cart",
            # "share_of_sub_disposables",
            # "share_of_mfr_non_big5",
            # "share_of_mfr_blu",
            # "share_of_mfr_juul",
            # "share_of_mfr_logic",
            # "share_of_mfr_rjr",
            # "share_of_mfr_sottera",
            # "IDX_FLV_MEN_norm",
            # "IDX_FLV_NM_norm",
            # "IDX_FLV_XM_NM_norm",
            # "share_of_mfr_others",
        ]
    elif industry == "mo":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TN_norm",
            # "IDX_TN_norm",
            # "IDX_FLV_MEN_norm",
            # "IDX_FLV_NM_norm",
            "share_of_FLV_MEN",
            "share_of_FLV_NM",
            "share_of_br_velo",
            "share_of_br_zyn_nt",
            "share_of_br_on",
            "share_of_br_rogue",
            # "share_of_mfr_swd",
            # "share_of_mfr_modoral",
            # "share_of_mfr_rogue",
            # "share_of_mfr_helix",
            "share_of_str_2mg_6mg",
            "share_of_str_2mg",
            "share_of_str_6mg",
            "share_of_sub_cat_lozenges",
            "share_of_sub_cat_pouches",
            "share_of_sub_cat_pouches_lozenges",
            "share_of_br_others",
            # "share_of_mfr_others",
        ]
    elif industry == "to":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TM_norm",
            # "IDX_TM_norm",
            # "IDX_FLV_MINT_norm",
            # "IDX_FLV_NATURAL_norm",
            # "IDX_FLV_STRAIGHT_norm",
            # "IDX_FLV_WINTERGREEN_norm",
            # "IDX_PR_BV_norm",
            # "IDX_PR_LV_norm",
            # "IDX_PR_PP_norm",
            # "IDX_PR_PFP_norm",
            # "IDX_FLV_OTHER_norm",
            "share_of_br_copenhagen",
            "share_of_br_grizzly",
            "share_of_br_kodiak",
            "share_of_br_longhorn",
            "share_of_br_red_seal",
            "share_of_br_skoal",
            "share_of_br_stokers",
            "share_of_br_others",
            "share_of_mfr_asc",
            "share_of_mfr_other",
            "share_of_mfr_swd_match",
            "share_of_mfr_usst",
            # "share_of_pt_branded_value",
            "share_of_pt_low_end",
            "share_of_pt_popular_price",
            # "share_of_pt_premium_full_price",
            "share_of_flv_mint_wintergreen",
            "share_of_flv_natural_straight",
            "share_of_flv_other",
            # "share_of_flv_mint",
            # "share_of_flv_natural",
            # "share_of_flv_straight",
            # "share_of_flv_wintergreen",
        ]
    elif industry == "snus":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TS_norm",
            # "IDX_TS_norm",
            "share_of_br_camel",
            "share_of_br_general",
            "share_of_br_grizzly",
            "share_of_br_skoal",
            # "share_of_mfr_asc",
            # "share_of_mfr_rjrt",
            # "share_of_mfr_swd_match",
            # "share_of_mfr_ustt",
            "share_of_flv_mint",
            "share_of_flv_nt",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NT"
        ]
    return kpi_variables

# COMMAND ----------

# DBTITLE 1,STR - SQL Queries
from typing import Optional
from pyspark.sql import functions as F, DataFrame

# ---------------------------------------------------------------------------
def _path_exists(path: str) -> bool:
    try:
        dbutils.fs.ls(path)
        return True
    except Exception:
        return False
# ---------------------------------------------------------------------------

def create_str_training_view(
    pe_file: str,
    export_path: str,
    date: str,
    industry: str,
    gb_price_tier: bool = False,
    gb_cig_length: bool = False,
    gb_menthol: bool = False,
    use_delta: bool = True,
    mech_delta_path: Optional[str] = None,
) -> None:

    # ----------------------------------------------------------------------
    # 0 · Load raw STR
    # ----------------------------------------------------------------------
    str_path_delta   = f"{export_path}/data/delta/df_str"
    str_path_parquet = f"{export_path}/data/df_str.parquet"

    if use_delta:
        spark.read.format("delta").load(str_path_delta) \
             .createOrReplaceTempView("raw_df_str")
    else:
        spark.read.parquet(str_path_parquet) \
             .createOrReplaceTempView("raw_df_str")

    # ----------------------------------------------------------------------
    # 1 · PE‑RSD – union all *_delta folders exported by the PE notebook
    # ----------------------------------------------------------------------
    (
        load_delta_tables_with_wildcards(spark, [f"{pe_file}/*"], union_all=True)
        .createOrReplaceTempView("raw_pe_rsd")
    )

    # ----------------------------------------------------------------------
    # 2 · Mechanical‑Price Delta
    # ----------------------------------------------------------------------
    if mech_delta_path is None:
        mech_delta_path = f"{export_path}/data/delta/mechanical_price"

    mech_df = (
        spark.read.format("delta").load(mech_delta_path)
        .withColumnRenamed("state",  "account_state")
        .withColumnRenamed("price",  "str_price")
        .withColumnRenamed("product","product_name")
    )
    mech_df.createOrReplaceTempView("mechanical_price_contract")

    # ----------------------------------------------------------------------
    # 3 · Build mechanical_price_outlet_contract on the fly
    # ----------------------------------------------------------------------
    spark.sql(f"""
        WITH account_cbo AS (
            SELECT
                account_sk,
                FIRST(CAST(regexp_extract(
                           UPPER(program),
                           'CBO[^0-9]*([0-9]+)', 1) AS INT)) AS cbo  
            FROM   rai_qa_uc_gm.d_core_pace.account_contract_map
            WHERE  program LIKE '%CBO%'
              AND  start_date <= DATE '{date}'
              AND  end_date   >= DATE '{date}'
            GROUP BY account_sk                              
        ),
        account_component AS (               
            SELECT
                account_sk,
                FIRST(
                    CASE
                        WHEN component IN ('MENTHOL BASE', 'PORTFOLIO BASE')
                             THEN 'Base'        
                        WHEN component =  'PORTFOLIO VLP'
                             THEN 'LA'           
                        ELSE component           
                    END
                ) AS component
            FROM   rai_qa_uc_gm.d_core_pace.account_contract_map
            WHERE  component IN ('MENTHOL BASE','MENTHOL EDLP',
                                 'PORTFOLIO BASE','PORTFOLIO EDLP','PORTFOLIO VLP')
              AND  start_date <= DATE '{date}'
              AND  end_date   >= DATE '{date}'
            GROUP BY account_sk
        ),
        acc_meta AS (
            SELECT DISTINCT
                account_sk,
                account_name_and_code,
                account_state
            FROM   raw_df_str                       
        )
        SELECT  ac.account_sk,
                am.account_name_and_code,
                am.account_state,
                ac.component,
                CONCAT('CBO ', ab.cbo) AS contract
        FROM    account_component ac
        LEFT JOIN account_cbo ab USING (account_sk)
        LEFT JOIN acc_meta   am USING (account_sk)
    """).createOrReplaceTempView("mechanical_price_outlet_contract")

    # ----------------------------------------------------------------------
    # 4 · KPI view
    # ----------------------------------------------------------------------
    create_kpi_view(industry, export_path)
    kpi_vars = [v.replace("_norm", "") for v in kpi_variables_industry(industry) if v not in ["account_longitude_norm","account_latitude_norm"]]

    # ----------------------------------------------------------------------
    # 5 · Normalized PE‑RSD
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW pe_rsd AS (
            SELECT
                account_sk,
                product_lineup_sk,
                {"price_tier"          if gb_price_tier  else "''"} AS price_tier,
                {"cig_length"          if gb_cig_length  else "''"} AS cig_length,
                {"menthol_non_menthol" if gb_menthol     else "''"} AS menthol_non_menthol,

                -- ♦ quantity-weighted PE and price
                SUM(pe  * rsd_quantity) / SUM(rsd_quantity) AS pe,
                SUM(rsd_price * rsd_quantity) / SUM(rsd_quantity) AS rsd_price,

                SUM(rsd_quantity) AS rsd_quantity
            FROM   raw_pe_rsd
            WHERE  pe_type = 'pe'
            GROUP  BY 1,2,3,4,5
        )
    """)

    # ----------------------------------------------------------------------
    # 6 · Aggregated STR
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW df_str AS
        SELECT account_sk, scan_validated_flag, product_lineup_sk,
               account_state, account_msa_fips, account_county_fips,
               account_name_and_code, product_name,
               {"price_tier" if gb_price_tier else "''"}       AS price_tier,
               {"cig_length" if gb_cig_length else "''"}       AS cig_length,
               {"menthol_non_menthol" if gb_menthol else "''"} AS menthol_non_menthol,
               cig_meeting_competition_contracted_flag_2017,
               location_sp_edlp_menthol_flag,
               location_sp_edlp_balanced_flag,
               location_sp_base_flag,
               location_sp_edlp_value_flag,
               location_mp_edlp_value_flag,
               location_sp_edlp_premium_flag,
               location_mp_edlp_premium_flag,
               location_mp_edlp_balanced_flag,
               location_mp_base_flag,
               location_mpe_edlp_balanced_flag,
               location_mpe_edlp_premium_flag,
               location_mpe_edlp_value_flag,
               account_status,
               float(account_latitude) account_latitude,
               float(account_longitude) account_longitude,
               SUM(str_quantity) /
                 (DATEDIFF(MAX(max_week_sales), MIN(min_week_sales)) / 7 + 1)
                 AS str_weekavg_quantity,
               FIRST(str_quantity_industry /
                     (DATEDIFF(max_week_sales_industry,
                               min_week_sales_industry) / 7 + 1))
                 AS str_weekavg_quantity_industry,
               MAX(max_week_sales) AS max_week_sales
        FROM   raw_df_str
        GROUP  BY all
    """)

    # ----------------------------------------------------------------------
    # 7 · Final view str_train_vw
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW str_train_vw AS
        WITH product_lineup AS (
            SELECT DISTINCT product_lineup_sk FROM pe_rsd
        ),
        base AS (
            SELECT  d.*, p.pe, p.rsd_price, p.rsd_quantity
            FROM    df_str d
            LEFT JOIN product_lineup pL
                   ON pL.product_lineup_sk = d.product_lineup_sk
            LEFT JOIN pe_rsd p
                   ON p.account_sk = d.account_sk
                  AND p.product_lineup_sk = d.product_lineup_sk
                  {"AND p.price_tier = d.price_tier"             if gb_price_tier  else ""}
                  {"AND p.cig_length = d.cig_length"             if gb_cig_length  else ""}
                  {"AND p.menthol_non_menthol = d.menthol_non_menthol"
                                                               if gb_menthol     else ""}
            WHERE   d.str_weekavg_quantity_industry IS NOT NULL
              AND   d.str_weekavg_quantity           IS NOT NULL
              AND   d.str_weekavg_quantity           > 0
              AND   d.account_status                 = 'Active'
        ),
        with_kpi AS (
            SELECT  b.*,
                    kpi_vw.bsns_sk,
                    {', '.join(kpi_vars)},
                    COALESCE(b.str_weekavg_quantity /
                             b.str_weekavg_quantity_industry, 0) AS str_weekavg_som
            FROM    base b
            LEFT JOIN kpi_vw ON kpi_vw.bsns_sk = b.account_sk
        ),
        with_contract AS (
            SELECT  w.*,
                    m_out.contract,
                    m_out.component,
                    CASE WHEN m_out.contract IS NOT NULL
                        AND  m_out.component IS NOT NULL
                        THEN 1 ELSE 0 END          AS bool_price_outlet_contract
            FROM    with_kpi w
            LEFT JOIN mechanical_price_outlet_contract m_out
                ON  m_out.account_name_and_code = w.account_name_and_code
                AND m_out.account_state         = w.account_state
        ),
        m_price_uni AS (
            SELECT  contract,
                    component,
                    product_name,
                    FIRST(str_price, TRUE) AS str_price
            FROM    mechanical_price_contract
            GROUP BY 1,2,3
        ),
        ranked AS (
                SELECT  wc.*,
                        ROW_NUMBER() OVER (
                            PARTITION BY wc.account_sk, wc.product_lineup_sk
                            ORDER BY wc.str_weekavg_quantity DESC,
                                    wc.product_lineup_sk
                        ) AS rn
                FROM    with_contract wc
            )

        SELECT  r.*,
                m.str_price,
                CASE WHEN m.str_price IS NOT NULL THEN 1 ELSE 0 END AS bool_price_contract
        FROM    ranked r
        LEFT JOIN m_price_uni m
            ON m.contract     = r.contract
            AND m.component    = r.component
            AND m.product_name = r.product_name
        WHERE   r.rn = 1;
    """)

# COMMAND ----------

# DBTITLE 1,ML model - dataset utils
def get_features_set(product_lineup_sk, l_product_lineup_sk, macroeconomics, get_by_length_level, get_by_price_tier, get_by_menthol, get_extra_num_feats_lgb):
  cat_feats = ["account_state"]
  num_feats = (
    ["rsd_price_perc"] + # rsd_price_perc expected to be the first in the list
    [f"{pid}_rsd_price_perc" for pid in l_product_lineup_sk] + # {pid}_rsd_price_perc list expected to be the second in the list
    # this cross product is adding too much in the pe elasticity
    #([f"{product_lineup_sk}_rsd_price_perc"] if get_by_length_level or get_by_price_tier else []) +
    [f"rsd_quantity_roll{r}_perc" for r in [2,4,8,13,26]] +
    ["relative_price_hist", "month_sin", "month_cos", "data_points"] +
    # ["rsd_price_ind_perc"] +
    [f"{pid}_rsd_quantity_roll4_perc" for pid in l_product_lineup_sk] +
    [f"{pid}_rsd_price_ref" for pid in l_product_lineup_sk]
    # this cross product is adding too much in the pe elasticity
    #([f"{product_lineup_sk}_rsd_quantity_roll4_perc"] if get_by_length_level or get_by_price_tier else [])
  )
  if macroeconomics:
    num_feats += ["unemployment_rate", "inflation_yoy", "consumer_spending_tobacco",  "gasoline_price", "consumer_price_index",
    "unemployment_rate_change" ,"inflation_yoy_change",  "consumer_spending_tobacco_change", "gasoline_price_change", "consumer_price_index_change"]
  num_feats += get_extra_num_feats_lgb
  if get_by_length_level:
    cat_feats += ["cig_length"]
  if get_by_price_tier:
    cat_feats += ["price_tier"]
  if get_by_menthol:
    cat_feats += ["menthol_non_menthol"]

  
  additional_cols = ["sales_week_start_date", "sales_week_forecast", "no_outlier", "weight", "rsd_quantity_roll4", "rsd_quantity_y", "rsd_price_y", "rsd_price_roll4", "account_sk", "rsd_quantity", "rsd_price"]    
  target_col = "rsd_quantity_perc"
  
  return cat_feats, num_feats, additional_cols, target_col

def split_train_test(train_input_df, train_start_date, train_end_date):
  print(f"Train start date: {train_start_date}")
  print(f"Train end date: {train_end_date}")

  # Split train/test - in the old code we used sales_week_forecast but we should use sales_week_start_date
  train_mask = (train_input_df.sales_week_forecast >= train_start_date) & \
                 (train_input_df.sales_week_forecast < train_end_date) & train_input_df.no_outlier
  test_mask = train_input_df.sales_week_forecast >= train_end_date
  test_clean_mask = (train_input_df.sales_week_forecast >= train_end_date) & train_input_df.no_outlier
  
  return train_mask, test_mask, test_clean_mask

# COMMAND ----------

# DBTITLE 1,STR KNN Helpers
import numpy as np
import pandas as pd
from pyspark.sql.functions import pandas_udf, PandasUDFType
from scipy.spatial import distance
from pyspark.sql import DataFrame
from pyspark.sql.types import FloatType

contracts = [
    "cig_meeting_competition_contracted_flag_2017",
    "location_sp_edlp_menthol_flag",
    "location_sp_edlp_balanced_flag",
    "location_sp_base_flag",
    "location_sp_edlp_value_flag",
    "location_mp_edlp_value_flag",
    "location_sp_edlp_premium_flag",
    "location_mp_edlp_premium_flag",
    "location_mp_edlp_balanced_flag",
    "location_mp_base_flag",
    "location_mpe_edlp_balanced_flag",
    "location_mpe_edlp_premium_flag",
    "location_mpe_edlp_value_flag",
    "no_contract",
]


@pandas_udf(returnType="float", functionType=PandasUDFType.SCALAR)
def custom_norm(x):
    xlog = np.log(x + 0.001)
    return (xlog - np.mean(xlog)) / np.std(xlog)


def custom_norm2(df: DataFrame) -> DataFrame:  # old method of normalization
    for feature in [
        "str_weekavg_quantity",
        "str_weekavg_quantity_industry",
        "str_weekavg_som",
    ]:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))
        result_schema = df.schema

        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm(df):
            xlog = np.log(df[feature] + 0.001)
            xlog_rsd = np.log(df[~df.pe.isna()][feature] + 0.001)
            df[f"{feature}_norm"] = (xlog - np.mean(xlog_rsd)) / np.std(xlog_rsd)
            df[f"{feature}_norm"] = df[f"{feature}_norm"].clip(-3.5, 3.5)
            return df

        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm)
    return df


def minmax_scaling(
    df: DataFrame, industry: str
) -> DataFrame:  # new method of normalization
    if industry == "fmc":
        features_name = ["VOL_TC", "IDX_TC"]
    elif industry == "vapor":
        features_name = [
            # "VOL_TV",
            # "IDX_TV",
            # "IDX_FLV_MEN",
            # "IDX_FLV_NM",
            # "IDX_FLV_XM_NM",
        ]
    elif industry == "mo":
        features_name = [
            # "VOL_TN",
            # "IDX_TN",
            # "IDX_FLV_MEN",
            # "IDX_FLV_NM",
        ]
    elif industry == "to":
        features_name = [
            # "VOL_TM",
            # "IDX_TM",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NATURAL",
            # "IDX_FLV_STRAIGHT",
            # "IDX_FLV_WINTERGREEN",
            # "IDX_PR_BV",
            # "IDX_PR_LV",
            # "IDX_PR_PP",
            # "IDX_PR_PFP",
            # "IDX_FLV_OTHER"
        ]
    elif industry == "snus":
        features_name = [
            # "VOL_TS",
            # "IDX_TS",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NT"
        ]

    for feature in features_name:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))

        result_schema = df.schema

        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm(df):
            xlog = np.log(df[feature] + 0.001)
            minx = np.min(xlog)
            maxx = np.max(xlog)

            df[f"{feature}_norm"] = (xlog - minx) / (maxx - minx)
            return df

        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm)
    
    for feature in ["account_longitude", "account_latitude"]:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))
        result_schema = df.schema
        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm_min_max(df):
            minx = np.min(df[feature])
            maxx = np.max(df[feature])
            df[f"{feature}_norm"] = (df[feature] - minx) / (maxx - minx)
            return df
        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm_min_max)
    return df

def distance_by_industry(df: pd.DataFrame, row: pd.Series, industry: str) -> pd.Series:
    # create Series of 0
    distance = pd.Series(0, index=df.index)
    # get list of kpi variables
    kpi_variables = kpi_variables_industry(industry)
    # loop over selected variables for given industry
    for kpi_variable in kpi_variables:
        distance = distance + (abs(row[kpi_variable] - df[kpi_variable])) ** 2

    return distance


def custom_knn(
    df: pd.DataFrame, row: pd.Series, cluster_size: int, industry: str
) -> pd.Series:
    if df.shape[0] == 0:
        return pd.Series({"pe": np.nan, "nearest_account_sk": [], "distance": []})

    df["distance"] = distance_by_industry(df, row, industry)
    df["rsd_price"] = df["rsd_price"].round(decimals=2)
    row["str_price"] = round(row["str_price"],2)
    # We are using the price point elasticity method to compute a new pe for the rsd store as if it was using the str_price
    # We are using the following formulas:
    # pe_rsd = ((Q_str-Q_rsd)/Q_rsd) / ((P_str-P_rsd)/P_rsd),
    # we supose pe_rsd = current pe for the rsd store (this is a price point elasticity)
    # pe_str = ((Q_rsd-Q_str)/Q_str) / ((P_rsd-P_str)/P_str)
    # source: https://pressbooks.bccampus.ca/uvicecon103/chapter/4-2-elasticity/
    # we first compute Q_str = Q_rsd + pe_rsd*((P_str-P_rsd)/P_rsd)*Q_rsd
    # then the new pe_str using the above formula
    Q_rsd, P_rsd, P_str, pe_rsd = (
        df["rsd_quantity"],
        df["rsd_price"],
        row["str_price"],
        df["pe"],
    )

    # we don't want price variations greater than 10%
    P_str = pd.Series([P_str] * len(P_rsd), index=df.index.values)
    P_str[P_str / P_rsd > 1.1] = P_rsd[P_str / P_rsd > 1.1] * 1.1
    P_str[P_str / P_rsd < 1 / 1.1] = P_rsd[P_str / P_rsd < 1 / 1.1] * 1 / 1.1

    # Using linear curve and point elasticity
    # Q_str = Q_rsd + pe_rsd*((P_str-P_rsd)/P_rsd)*Q_rsd
    # df["pe_str"] = ((Q_rsd-Q_str)/Q_str) / ((P_rsd-P_str)/P_str)
    # df.loc[df.pe_str.isna(), "pe_str"] = df.loc[df.pe_str.isna(), "pe"]

    # Using log curve and point elasticity
    Q_str = Q_rsd * (P_str / P_rsd) ** pe_rsd
    df["pe_str"] = ((Q_rsd - Q_str) / Q_str) / ((P_rsd - P_str) / P_str)
    df.loc[df.pe_str.isna(), "pe_str"] = df.loc[df.pe_str.isna(), "pe"]

    # cap floor pe difference, no more than 0.1 pe difference
    # df.loc[df.pe_str-df.pe>0.1, "pe_str"] = df.loc[df.pe_str-df.pe>0.1, "pe"]+0.1
    # df.loc[df.pe-df.pe_str>0.1, "pe_str"] = df.loc[df.pe-df.pe_str>0.1, "pe"]-0.1

    df["distance"] = np.sqrt(df["distance"].astype(float))

    # we want to map str stores to rsd stores having a lower price
    if df[df["rsd_price"] <= row["str_price"]].shape[0] > 0:
        df = df[df["rsd_price"] <= row["str_price"]]

    df = df.sort_values(
        by=[
            "distance",
            "rsd_quantity",
            "str_weekavg_quantity",
            "str_weekavg_quantity_industry",
        ],
        ascending=[True, False, False,False],
    )

    return pd.Series(
        {
            "estimated_pe": (
                (
                    df.head(cluster_size).pe_str * df.head(cluster_size).rsd_quantity
                ).sum()
                / df.head(cluster_size).rsd_quantity.sum()
            ),
            "estimated_pe_rsd": (
                (df.head(cluster_size).pe * df.head(cluster_size).rsd_quantity).sum()
                / df.head(cluster_size).rsd_quantity.sum()
            ),
            "nearest_account_sk": list(df.head(cluster_size).account_sk),
            "distance": list(df.head(cluster_size).distance),
            "nearest_account_pe": list(df.head(cluster_size).pe),
            "nearest_account_price": list(df.head(cluster_size).rsd_price),
            "nearest_account_pe_str": list(df.head(cluster_size).pe_str),
        }
    )


def create_knn_spark(
    industry: str = "fmc", use_contract: bool = True, cluster_size: int = 5
):
    # function used to calculate pe_str using knn
    def knn_spark(df: pd.DataFrame) -> pd.DataFrame:
        f = df.knn_computed == 0
        df[["estimated_pe", "estimated_pe_rsd"]] = np.nan, np.nan
        df.loc[f, "estimated_pe"], df.loc[f, "estimated_pe_rsd"] = (
            df.loc[f, "pe"],
            df.loc[f, "pe"],
        )
        l_col = [
            "nearest_account_sk",
            "distance",
            "nearest_account_pe",
            "nearest_account_price",
            "nearest_account_pe_str",
        ]
        for c in l_col:
            df[c] = [None] * df.shape[0]
        if df[f].shape[0] > 0:
            if not use_contract:
                df.loc[~f, ["estimated_pe", "estimated_pe_rsd"] + l_col] = df.loc[
                    ~f
                ].apply(
                    lambda row: custom_knn(
                        df[(df.knn_computed == 0)].copy(), row, cluster_size, industry
                    ),
                    axis=1,
                )
            else:
                for contract in df.contract.unique():
                    f_contract = df.contract == contract
                    f_ref = [True] * df.shape[0]
                    if industry == "fmc":
                        if "EDLP" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                        if "Base" in contract:
                            f_ref = df["contract"].str.contains("Base")
                        if "VLP" in contract:
                            f_ref = df["contract"].str.contains("VLP")
                        if "No Contract" in contract:
                            f_ref = df["contract"].str.contains("Base")
                        if "Limited Assortment" in contract:
                            f_ref = df["contract"].str.contains("Limited Assortment")
                        if "CAMEL PGA" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                        if "NEWPORT PGA" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                    if industry in ["vapor", "vapr", "mo", "to", "snus"]:
                        # for these industries direct mapping is used
                        f_ref = df.contract == contract

                    if (
                        df[f & f_ref].shape[0] > 0
                    ):  # if the restrictions on the contract are not too restrictive
                        df.loc[
                            (~f) & f_contract,
                            ["estimated_pe", "estimated_pe_rsd"] + l_col,
                        ] = df.loc[(~f) & f_contract].apply(
                            lambda row: custom_knn(
                                df[f & f_ref].copy(), row, cluster_size, industry
                            ),
                            axis=1,
                        )
                    else:  # we remove the filter on the contract
                        df.loc[
                            (~f) & f_contract,
                            ["estimated_pe", "estimated_pe_rsd"] + l_col,
                        ] = df.loc[(~f) & f_contract].apply(
                            lambda row: custom_knn(
                                df[f].copy(), row, cluster_size, industry
                            ),
                            axis=1,
                        )
        return df

    return knn_spark

# COMMAND ----------

# DBTITLE 1,Setting DB parameters
try: 
    scope = "spi-eap-ds-rgm"
    host = dbutils.secrets.get(scope=scope, key="host")
    password = dbutils.secrets.get(scope=scope, key="password")
    user = dbutils.secrets.get(scope=scope, key="user")
    execution_role = dbutils.secrets.get(scope=scope, key="execution_role")
    database = dbutils.secrets.get(scope=scope, key="database")
    tempdir = dbutils.secrets.get(scope=scope, key="tempdir")

    rs_url = "jdbc:redshift://" + host + ":5439/" + database + "?user=" + user + "&password=" + password + "&ssl=true&sslFactory=com.amazon.redshift.ssl.NonValidatingFactory"
except Exception as e:
    print("No access to Redshift secrets. Skipping Redshift config.")
    rs_url = None
    tempdir = None

# COMMAND ----------

# DBTITLE 1,Dynamic dates selection
from typing import Tuple


def get_dynamic_start_end_dates(no_years: int = 2) -> Tuple[str, str]:
    """
    function returns start and end dates using dynamic dates
    for selected number of years
    """
    if no_years == 1:
        period = '(4024)'
    elif no_years == 2:
        period = (4024, 4041)
    else:
        raise ValueError("no_years parameter should be equal to 1 or 2")

    query = f"""select  min(a11.sales_week_start_date) sales_week_start_date,
    max(a11.sales_week_end_date) sales_week_end_date
    from D_RPT_FALCON_V.calendar_sales_lookup a11
    left outer join D_RPT_FALCON_V.calendar_sales_dynamic_weeks_lookup a12
    on (a11.sales_week_sk = a12.sales_week_sk)
    where a12.dynamic_date_str_id in {period}
    """

    df = (
        spark.read.format("com.databricks.spark.redshift")
        .option("url", rs_url)
        .option("tempdir", tempdir)
        .option("aws_iam_role", execution_role)
        .option("query", query)
        .load()
        .toPandas()
    )
    # display(dates_query2)
    start_date = str(df.sales_week_start_date[0])
    end_date = str(df.sales_week_end_date[0])
    return start_date, end_date

</file>

</files>
</repository_contents>

suggest what options can we do:
Optimizations and things to keep in mind:
the filter_training_lgb is set so we discard no more than 5% sales in the 02 - ML Model
the wpe on the 06 - Evaluate notebook can't be changed
I need ideas to improve the accuracy
do not output code
<repository_contents>
<file path="grizzly_velo.json">
{
  "_comment_readme": "This configuration file is being used for the PE Remodel project as of June 2025",

  "_comment_macroeconomic": "1 to use macroeconomics data in lgb model, otherwise put 0",
  "include_macroeconomic": 1,

  "catalog": "rai_prod_uc",

  "_comment_extra_num_feats_lgb": "list of optional numerical features",
  "extra_num_feats_lgb": ["rsd_quantity_ind_perc"],
  "extra_num_feats_lgb_industry": 1,
  
  "_comment_variables": "put the following variables to 1 if you want a pe for each of those groups, otherwise put 0",
  "cig_length": 0,
  "price_tier": 0,
  "menthol": 0,
  
  "price_threshold": 1,
  "cpu_or_serving": "cpu",
  "industry_id": [1],
  "industry_unique_id": 1,
  "industry": "fmc",

  "_comment_states": "states for which we want PE numbers",
  "states": ["AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DE", "FL",
     "GA", "ID", "IL", "IN", "IA", "HI", "KS", "KY", "LA", "ME", "MD", "MA", "MI",
     "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", "NM", "NY", "NC", "ND",
     "OH", "OK", "OR", "PA", "RI", "SC", "SD", "TN", "TX", "UT", "VT", "VA",
     "WA", "WV", "WI", "WY"],

  "_comment_products": "list of products and competitors by product_lineup_sk",
    "products": 
{"GRIZZLY MO X 7MG": {"id": 1053221, "other": [], "filter_training_lgb": "(rsd_quantity_y / rsd_quantity_roll4) <= 6 and (rsd_quantity_roll4 / rsd_quantity_y) <= 6 and (rsd_price_perc < 2) and (rsd_price_perc > 0.5) and (data_points >= 10)", "hyperparameters": {"min_delta": 0.0001}}, "VELO PLUS": {"id": 1052079, "other": [], "filter_training_lgb": "(rsd_quantity_y / rsd_quantity_roll4) <= 5 and (rsd_quantity_roll4 / rsd_quantity_y) <= 5 and (rsd_price_perc < 2) and (rsd_price_perc > 0.5) and (data_points >= 10)", "hyperparameters": {"min_delta": 0.0001}}},
  "_comment_simulation_price_variation": "we are going to simulate bump of -5%, -2.5%, 0%, 2.5% and 5% of the price and see what is the impact on the volume",
  "simulation_price_variation": [0.95, 0.975, 1, 1.025, 1.05],

  "_comment_min_weekly_rsd_quantity_for_pe_computation": "we won't compute a PE for (store,product) that have less than this amount of weekly qauntity",
  "min_weekly_rsd_quantity_for_pe_computation": 1
}
</file>

<file path="run_crosspe_fmc_mo_to">
import json
export_path = "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/"
export_path_for_reynolds = "s3://rai-eap-rgm-qa-us-east-1/insiteai/outputs"

max_predict_date = "2025-06-01"
main_folder = f"{max_predict_date[0:4]}/{max_predict_date[5:7]}"
def run_main(exp_name_suffix, config_name, run_download=0, run_density=0, run_eda=0, test=0, compute_cross_pe=1, export_suffix="", max_predict_date=max_predict_date, number_of_predictions = 1):
    dbutils.notebook.run("../notebooks/00 - PE Full Pass", 0, {
        "run_id": f"{main_folder}/{exp_name_suffix}",
        "export_path": export_path,
        "config_path": f"""/Workspace{"/".join(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[:-2])}/config/crosspe_fmc_mo_to/{config_name}.json""",
        "max_predict_date": max_predict_date,
        "number_of_predictions": number_of_predictions, # put several to run a stability analysis
        "compute_cross_pe": compute_cross_pe,
        "future_steps": 0,
        "run_pe_str": 0,
        "run_download": run_download,
        "run_density": run_density,
        "run_eda": run_eda,
        "crosspe_category_project": 1,
        "export_suffix": export_suffix,
    })

# COMMAND ----------


# FMC Manuel
exp_name_suffix = "fmc"
run_main(
    exp_name_suffix=exp_name_suffix,
    config_name="grizzly_velo",
    run_download=0,
    run_density=0,
    run_eda=0,
    export_suffix="_pe_manuel",
    compute_cross_pe=0,
    max_predict_date="2025-06-01",
    number_of_predictions=6,
)

</file>

<files>
<file path="notebooks/00 - PE Full Pass.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # PE Full Pass Notebook
# MAGIC
# MAGIC This notebook is the main notebook used to generate PE for FMC, Vapor, Modern Oral and Traditional Oral PE (MO and TO to be added).
# MAGIC
# MAGIC The main steps are:
# MAGIC  - "Get data": download RSD, STR, econometric, KPI data from Redshift
# MAGIC  - "Train ML model": Train the machine learning model and save it to mlflow on databricks
# MAGIC  - "Curve Simulation": Create simulations of price variations
# MAGIC  - "PE RSD Pass": Compute a PE for each RSD account
# MAGIC  - "Delete Curve Simulation files": Remove temporary files to keep the storage on s3 low
# MAGIC  - "PE STR Pass": Compute PE STR
# MAGIC  - "Future PE": Compute forecasted RSD PE

# COMMAND ----------

dbutils.widgets.removeAll()

# COMMAND ----------

dbutils.widgets.text("run_id", "2023/test") # id for the run, this should be unique or it will overwrite an existing run
dbutils.widgets.text("export_path", "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/")
dbutils.widgets.text("config_path", "../config/fmc_test.json")
dbutils.widgets.text("max_predict_date", "2023-03-01") # date for which we generate a pe (has to be a historical data, not a future one)
dbutils.widgets.text("number_of_predictions", "1") # '1' by default, this will compute pe for the max_predict_date, if we want to compute more historical PE increase this number (2 will compute pe for max_predict_date + the month before)
dbutils.widgets.text("compute_cross_pe", "0") # '1' to enable cross pe computation, '0' to disable
dbutils.widgets.text("future_steps", "0") # number of months for which to forecast pe
dbutils.widgets.text("run_pe_str", "1")
dbutils.widgets.text("run_download", "1")
dbutils.widgets.text("run_modeling", "1")
dbutils.widgets.text("run_density", "1")
dbutils.widgets.text("run_eda", "1")
dbutils.widgets.text("run_accuracy", "0")
dbutils.widgets.text("run_drift", "0")
dbutils.widgets.text("run_histogram", "0")
dbutils.widgets.text("crosspe_category_project", "0")
dbutils.widgets.text("dynamic_dates_alignment","0") # for period equal to max_predict date use dynamic dates to select data ranges
dbutils.widgets.text("delta", "1") # Add delta parameter, 1 = skip cleaning step 04.2
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

# MAGIC %run ./utils.py 

# COMMAND ----------

import json
import numpy as np
import pandas as pd

run_id = dbutils.widgets.get("run_id")
export_path = dbutils.widgets.get("export_path")
config_path = dbutils.widgets.get("config_path")
max_predict_date = dbutils.widgets.get("max_predict_date")
number_of_predictions = int(dbutils.widgets.get("number_of_predictions"))
compute_cross_pe = dbutils.widgets.get("compute_cross_pe")
future_steps = dbutils.widgets.get("future_steps")
run_pe_str = int(dbutils.widgets.get("run_pe_str"))
run_download = int(dbutils.widgets.get("run_download"))
run_density = int(dbutils.widgets.get("run_density"))
run_eda = int(dbutils.widgets.get("run_eda"))
run_histogram = int(dbutils.widgets.get("run_histogram"))
run_modeling = int(dbutils.widgets.get("run_modeling"))
run_accuracy = int(dbutils.widgets.get("run_accuracy"))
run_drift = int(dbutils.widgets.get("run_drift"))
crosspe_category_project = dbutils.widgets.get("crosspe_category_project")
dynamic_dates_alignment = int(dbutils.widgets.get("dynamic_dates_alignment"))
delta = int(dbutils.widgets.get("delta"))
export_suffix = dbutils.widgets.get("export_suffix")

predict_dates = pd.date_range(end=max_predict_date, periods=number_of_predictions, freq="MS", closed="right").date.astype(str)
with open(config_path, "r") as f:
    config = json.load(f)
history_start_date = config.get("history_start_date", "none") # first date from training dataset, 'none' for two years of history
# When dynamic alignment is enabled, derive history range from available data
if dynamic_dates_alignment == 1:
    history_start_date, dynamic_end_date = get_dynamic_start_end_dates(no_years=2)

elif history_start_date=='none':
    history_start_date = str((pd.to_datetime(max_predict_date)-pd.DateOffset(years=2)).date())
simulation_price_variation = config.get("simulation_price_variation")
min_weekly_rsd_quantity_for_pe_computation = config.get("min_weekly_rsd_quantity_for_pe_computation")
states = config.get("states")
products = config.get("products")
all_product_id = config.get(
    "rsd_product_id_to_download", # new method to download the list of products
    list(np.concatenate([[d.get("id")] + d.get("other") for d in config.get("products").values()]).flat) # old method to download the list of products
)
main_product_id = list(np.unique(np.array([[d.get("id")] for d in config.get("products").values()]).flat))
product_names = list(products.keys())
generate_data = config.get("generate_data", 0)
include_macroeconomic = config.get("include_macroeconomic") # Should be set to 1 in config file if macroeconomic features need to be included
get_by_length_level = config.get("cig_length", 0)
get_by_price_tier = config.get("price_tier", 0)
get_by_menthol = config.get("menthol", 0)
get_extra_num_feats_lgb = config.get("extra_num_feats_lgb", [])
get_extra_num_feats_lgb_industry = config.get("extra_num_feats_lgb_industry", 1)
if get_extra_num_feats_lgb_industry==1:
    get_extra_num_feats_lgb = (
        get_extra_num_feats_lgb +
        [f"rsd_quantity_industry_roll{r}_perc" for r in [2,4,8,13,26]] + [f"rsd_quantity_industry_roll2"]
    )
group_target_products = config.get("group_target_products", 0)
grouping = config.get("grouping", "")
grouped_targets = config.get('grouped_targets', "")
group_rsd_data = config.get("group_rsd_data", {})
cpu_or_serving = config.get("cpu_or_serving")
price_threshold = config.get("price_threshold")
hyperparameters_file = config.get("hyperparameters_file", "")
industry_id = config.get("industry_id")
industry_unique_id = config.get("industry_unique_id")
industry = config.get("industry")
additional_filters_redshift = config.get("additional_filters_redshift", "")
density_use_all_products = config.get("density_use_all_products")
density_cpu_threshold = config.get("density_cpu_threshold")
density_weeks_threshold = config.get("density_weeks_threshold")
catalog = config.get("catalog", 0)


print(f"States: {states}")
print(f"Predict date: {predict_dates}")
print(f"History start date: {history_start_date}")
print(f"Compute cross pe: {compute_cross_pe}")
print(f"Future steps: {future_steps}")
print(f"Include Macroeconomics: {include_macroeconomic}")
print(f"group_target_products: {group_target_products}")
print(f"grouped targets: {grouped_targets}")
print(f"cpu_or_serving: {cpu_or_serving}")
print(f"get_by_length_level: {get_by_length_level}")
print(f"get_by_price_tier: {get_by_price_tier}")
print(f"get_by_menthol: {get_by_menthol}")
print(f"get_extra_num_feats_lgb: {get_extra_num_feats_lgb}")
print(f"price_threshold: {price_threshold}")
print(f"industry_id: {industry_id}")
print(f"additional_filters_redshift: {additional_filters_redshift}")
print(f"industry: {industry}")
print(f"Dynamic dates alignment : {dynamic_dates_alignment}")
print(f"Delta: {delta}")

# COMMAND ----------

# DBTITLE 1,Get data
if run_download==1:
    if dynamic_dates_alignment == 1:
        start_date_1y, end_date = get_dynamic_start_end_dates(no_years = 1)
    else:
        start_date_1y = 'none'
        end_date = max_predict_date
        
    dbutils.notebook.run("01 - Get Data", 0, {
        "export_path": export_path,
        "run_id": run_id,
        "states": json.dumps(states),
        "rsd_product_ids": str(all_product_id),
        "str_product_ids": str(all_product_id),
        "start_date": history_start_date,
        "str_start_date": start_date_1y,
        "end_date": end_date, # Used in STR query - PE STR runs only for the last (max) date in the predict_dates list
        "generate_data": generate_data,
        "include_macroeconomic": include_macroeconomic,
        "group_target_products": group_target_products,
        "grouping": json.dumps(grouping),
        "group_rsd_data": json.dumps(group_rsd_data),
        "cpu_or_serving": cpu_or_serving,
        "industry_id": json.dumps(industry_id),
        "additional_filters_redshift": json.dumps(additional_filters_redshift),
        "industry": industry,
        "crosspe_category_project": crosspe_category_project,
        "catalog": catalog,
        "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,eda
if run_eda==1:
  dbutils.notebook.run("01.2 - EDA Data", 0, {
      "export_path": export_path,
      "run_id": run_id,
      "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,Regenerate product-competitor Dict in case of grouped data
if group_target_products:
    product_names = grouped_targets

    new_products = {}
    # Loop through low level producsts
    for relation in products.values():
        # Loop through groups
        for group, rel in grouping.items():
            # Look if the low level products is in this parent
            if relation['id'] in list(rel.values())[0]:
                # If parent already in the new dictionary just append the competitors
                if group in new_products.keys():
                    new_products[group]['other'] += relation['other']
                # If parent not in dict add it with it's competitors
                else:
                    new_products[group] = {'id' : [int(x) for x in list(rel.keys())][0], 'other' : relation['other']}
                    if "filter_training_lgb" in relation.keys():
                        new_products[group]["filter_training_lgb"] = relation.get("filter_training_lgb")
    # Remove duplicates in competitors
    for group in new_products.values():
        group['other'] = list(set(group['other']))
    
    products = new_products.copy()
    print(products)

    
    # reverse_grouping
    reverse_grouping = {}

    # Loop through new id and old ids that need to be agg
    for mapping in grouping.values():
        for group, old_prods in mapping.items():
            for old_prod in old_prods:
                if old_prod not in reverse_grouping.keys():
                    reverse_grouping[old_prod] = group
                else:
                    print("WARNING: A PROD IS IN MULTIPLE GROUPS")

    
    # Change competitors in the new products with the aggregated ones
    new_products = products.copy()

    # Loop through new id and old ids that need to be agg
    for mapping in new_products.values():
        new_list = []
        for competitor in mapping['other']:
            new_list.append(int(reverse_grouping.get(competitor, competitor)))
        mapping['other'] = list(set(new_list))

    products = new_products.copy()
    print(products)

# COMMAND ----------

# DBTITLE 1,Data density
# Use default values of parameters 
# if they are not provided in config
if density_cpu_threshold is None:
   density_cpu_threshold = 30
if density_weeks_threshold is None:
   density_weeks_threshold = 30 
if density_use_all_products is None:
   density_use_all_products = 0
if industry is None:
   industry = "None"

# # if run_density==1:
# #     # dbutils.notebook.run("01.3 - Data Density", 0, {
# #     #     "end_date": max_predict_date,
# #     #     "cpu_threshold": density_cpu_threshold,
# #     #     "weeks_threshold":density_weeks_threshold, 
# #     #     "main_products": str(main_product_id),
# #     #     "all_products": str(all_product_id),
# #     #     "group_target_products": group_target_products,
# #     #     "products_grouped":str(grouping).replace("\'", "\""),
# #     #     "industry": industry,
# #     #     "use_all_products": density_use_all_products   
# #     # })

if run_density==2:
    dbutils.notebook.run("01.3 - Data Density 2", 0, {
        "data_path": f"{export_path}{run_id}/data",
        "end_date": max_predict_date,
        "cpu_threshold": density_cpu_threshold,
        "weeks_threshold": density_weeks_threshold,
        "industry": industry,
        "all_products": str(all_product_id),
        "delta": delta
    })

# COMMAND ----------

# DBTITLE 1,Price histograms
if run_histogram==1:
    dbutils.notebook.run("01.5 - EDA Histogram", 0, {
        "export_path": export_path,
        "run_id": f"{run_id}/{max_predict_date}",
        "delta": delta
    })

# COMMAND ----------

if run_eda==1:
    try:
        print(f"  - Passing export_path: {export_path}")
        print(f"  - Passing run_id: {run_id}")
        dbutils.notebook.run("01.4 - EDA Macrovariables", 0, {
            "export_path": export_path,
            "run_id": run_id,
            "delta": delta
        })
    except Exception as e:
        print(f"ERROR running Macrovariable EDA notebook: {e}")
        # raise e # Optionally uncomment to stop execution on error

# COMMAND ----------

# DBTITLE 1,Train ML model
result_ml_model_train = {}

for predict_date in predict_dates:
 model_name_date = predict_date
 if (dynamic_dates_alignment == 1) & (predict_date == max_predict_date):
   # When aligning dynamically for the most recent month, preserve the
   # historical start date but move the end date to the detected window
   predict_date = dynamic_end_date
 train_res = dbutils.notebook.run("02 - ML Model", 0, {
                                           "import_data_path": f"{export_path}{run_id}",
                                           "run_id": f"{run_id}{export_suffix}/{predict_date}",
                                           "product_names": json.dumps(product_names),
                                           "products": json.dumps(products),
                                           "industry_unique_id": industry_unique_id,
                                           "predict_date": predict_date,
                                           "history_start_date": history_start_date,
                                           "include_macroeconomic": include_macroeconomic,
                                           "price_threshold": price_threshold,
                                           "get_by_length_level": get_by_length_level,
                                           "get_by_price_tier": get_by_price_tier,
                                           "cpu_or_serving": cpu_or_serving,
                                           "hyperparameters_file": hyperparameters_file,
                                           "get_by_menthol": get_by_menthol,
                                           "get_extra_num_feats_lgb": json.dumps(get_extra_num_feats_lgb),
                                         })
 result_ml_model_train[model_name_date] = train_res
print(result_ml_model_train)

# COMMAND ----------

# DBTITLE 1,Combined step for pe computation

for predict_date in predict_dates:
    # The model URI is retrieved from the results of the ML model training step
    model_uri = json.loads(result_ml_model_train[predict_date]).get("model_uri")

    dbutils.notebook.run("../notebooks/03.1 Simulation RSD", 0, {
        # Common Parameters
        "run_id": f"{run_id}/{predict_date}",
        "export_path": export_path,
        "product_names": json.dumps(product_names),
        "states": json.dumps(states),
        "get_by_length_level": get_by_length_level,
        "get_by_price_tier": get_by_price_tier,
        "get_by_menthol": get_by_menthol,
        "delta": delta,
        "industry_unique_id": industry_unique_id,

        # Parameters from original "03 - Simulation"
        "import_data_path": f"{export_path}{run_id}",
        "model_uri": model_uri,
        "compute_cross_pe": compute_cross_pe,
        "simulation_price_variation": json.dumps(simulation_price_variation),
        "cpu_or_serving": cpu_or_serving,
        "price_threshold": price_threshold,
        "dynamic_dates_alignment": dynamic_dates_alignment if predict_date == max_predict_date else 0,

        # Parameters from original "04 - PE RSD"
        "products": json.dumps(products),
        "min_weekly_rsd_quantity": min_weekly_rsd_quantity_for_pe_computation,
        "predict_date": predict_date,
        "export_suffix": export_suffix
    })

# COMMAND ----------

# DBTITLE 1,Curve Simulation
# for predict_date in predict_dates:
#  model_uri = json.loads(result_ml_model_train[predict_date]).get("model_uri")

#  dbutils.notebook.run("03 - Simulation", 0, {
#      "import_data_path": f"{export_path}{run_id}",
#      "run_id": f"{run_id}/{predict_date}",
#      "product_names": json.dumps(product_names),
#      "states": json.dumps(states), # Not used at the moment
#      "export_path": export_path,
#      "model_uri": model_uri,
#      "compute_cross_pe": compute_cross_pe,
#      "simulation_price_variation": json.dumps(simulation_price_variation),
#      "get_by_length_level": get_by_length_level,
#      "get_by_price_tier": get_by_price_tier,
#      "cpu_or_serving": cpu_or_serving,
#      "price_threshold": price_threshold,
#      "get_by_menthol": get_by_menthol,
#      "dynamic_dates_alignment": dynamic_dates_alignment if predict_date == max_predict_date else 0,
#      "delta": delta 
#    })

# COMMAND ----------

# DBTITLE 1,PE RSD Pass
# for predict_date in predict_dates:
#  dbutils.notebook.run("04 - PE RSD", 0, {
#      "states": json.dumps(states),
#      "product_names": json.dumps(product_names),
#      "predict_date": predict_date,
#      "export_path": export_path,
#      "run_id": f"{run_id}/{predict_date}",
#      "products": json.dumps(products),
#      "min_weekly_rsd_quantity": min_weekly_rsd_quantity_for_pe_computation,
#      "get_by_length_level": get_by_length_level,
#      "get_by_price_tier": get_by_price_tier,
#      "get_by_menthol": get_by_menthol,
#      "delta": delta 
#    })

# COMMAND ----------

# DBTITLE 1,Delete Curve Simulation files (taking a lot of space on s3)
# # Conditionally run the cleaning step based on delta
# if delta != 1: 
#     dbutils.notebook.run("04.2 - Cleaning Curve Simulation Files", 0, {
#          "path_to_search": f"{export_path}{run_id}/"
#        })
# else: 
#     print("Skipping 04.2 - Cleaning Curve Simulation Files because delta=1")

# COMMAND ----------

# DBTITLE 1,PE STR Pass
if run_pe_str==1:
   for predict_date in predict_dates:
       dbutils.notebook.run("05 - PE STR", 0, {
           "import_data_path": f"{export_path}{run_id}",
           "run_id": f"{run_id}/{predict_date}",
           "export_path": export_path,
           "states": json.dumps(states), # Not used at the moment
           "product_names": json.dumps(product_names),
           "products": json.dumps(products),
           "predict_date": predict_date,
           "get_by_length_level": get_by_length_level,
           "get_by_price_tier": get_by_price_tier,
           "get_by_menthol": get_by_menthol,
           "industry": industry
           })

# COMMAND ----------

# DBTITLE 1,Evaluate
for predict_date in predict_dates:
 dbutils.notebook.run("06 - Evaluate", 0, {
     "pe_str": f"{export_path}{run_id}/{predict_date}/str_pe",
     "pe_rsd": f"{export_path}{run_id}/{predict_date}/computed_pe{export_suffix}",
     "data_path": f"{export_path}{run_id}/data/df_product.parquet",
     "cross_pe": compute_cross_pe,
     "delta": delta ,
     "industry_unique_id": industry_unique_id
   })

# COMMAND ----------

# DBTITLE 1,Visualize PE Trend
if len(predict_dates)>1 and run_pe_str==1:
 dbutils.notebook.run("07 - Visualize Trend", 0, {
     "pe_str": f"{export_path}/{run_id}{export_suffix}/*/str_pe/*.parquet"
   })

# COMMAND ----------

# DBTITLE 1,Future PE
if len(predict_dates)>=12:
 dbutils.notebook.run("08 - Future PE", 0, {
     "product_names": json.dumps(product_names),
     "export_path": export_path,
     "run_id": run_id,
     "products": json.dumps(products),
     "pe_file": f"{export_path}/{run_id}/*/computed_pe/*.parquet",
     "test_length": "0", # setup 1 to run it with 1 test date
     "future_steps": future_steps
   })

# COMMAND ----------

# DBTITLE 1,Generate Report
if len(predict_dates)>=12:
    dbutils.notebook.run("09 - Generate Report", 0, {
          "import_data_path": f"{export_path}{run_id}",
          "future_pe_file": f"{export_path}/{run_id}/forecasted_pe/*.parquet",
          "cross_pe_file": f"{export_path}/{run_id}/*/computed_pe/*.parquet",
          "min_weekly_quantity": min_weekly_rsd_quantity_for_pe_computation,
          "products": json.dumps(products),
          "predict_date": max_predict_date
        })

# COMMAND ----------

# DBTITLE 1,Accuracy
if run_accuracy == 1:
    try:
        dbutils.notebook.run(
            "../notebooks/other/accuracy_rsd_lgbm", 0,
            {
                "folder":          export_path,
                "run_id":          run_id,                 
                "max_predict_date":max_predict_date,
                "model_to_use":    "all",       
                "config_path":     config_path,
                "delta" : delta,
                "export_suffix" : export_suffix,
                "industry_unique_id" : industry_unique_id             
            }
        )
        print("accuracy_rsd_lgbm completed")
    except Exception as e:
        print("accuracy_rsd_lgbm failed:", e)

# COMMAND ----------

if run_drift == 1: 
    try:
        dbutils.notebook.run(
            "../notebooks/other/drift", 0,
            {
                "folder":      export_path,
                "run_id":      run_id,
                "product_list": str([d.get("id") for d in config.get("products").values()]),
                "industry_unique_id": industry_unique_id,
                "delta_input": 1,
                "export_suffix": export_suffix                        
            }
        )
        print("drift notebook completed")
    except Exception as e:
        print("drift notebook failed:", e)

</file>

<file path="notebooks/02 - ML Model.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # ML Model Notebook
# MAGIC
# MAGIC Train a machine learning model for each product of interest and save it to mlflow on databricks.
# MAGIC
# MAGIC The goal of those models are to learn the dynamics of price vs quantity for each product at account level.

# COMMAND ----------

# dbutils.widgets.removeAll()
# dbutils.widgets.text("import_data_path", "/FileStore/pe/pe_model_dev")
# dbutils.widgets.text("run_id", "vapor_group_data_test")
# dbutils.widgets.text("product_names", '["VUSE ALTO POD MEN 4CART", "VUSE ALTO POD GOLDEN TOBACCO 2CART"]')
# dbutils.widgets.text("industry_unique_id", "1") 
# dbutils.widgets.text("predict_date", "2022-11-01") 
# dbutils.widgets.text("history_start_date", "2022-03-01")    # first date from training dataset, 'none' for one year history
# dbutils.widgets.text("products", """{"VUSE ALTO POD MEN 4CART": {"id": 99991, "other": [93932,83587, 61964]}, "VUSE ALTO POD GOLDEN TOBACCO 2CART": {"id": 99995, "other": [61957, 70336, 71957]}}""") 
# dbutils.widgets.text("include_macroeconomic", "0")
# dbutils.widgets.text("get_by_length_level", "0")
# dbutils.widgets.text("get_by_price_tier", "0")
# dbutils.widgets.text("get_by_menthol", "0")
# dbutils.widgets.text("cpu_or_serving", "cpu")
# dbutils.widgets.text("hyperparameters_file", "none")
# dbutils.widgets.text("get_extra_num_feats_lgb", "")
# dbutils.widgets.text("price_threshold", "1")

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

spark.conf.set("spark.sql.execution.arrow.enabled", "true")
spark.conf.set("spark.sql.execution.arrow.fallback.enabled", "false")

# COMMAND ----------

import json

import_data_path = dbutils.widgets.get("import_data_path")
run_id = dbutils.widgets.get("run_id")
product_names = json.loads(dbutils.widgets.get("product_names"))
predict_date = dbutils.widgets.get("predict_date")
history_start_date = dbutils.widgets.get("history_start_date")
products = json.loads(dbutils.widgets.get("products")) 
industry_unique_id = int(dbutils.widgets.get("industry_unique_id")) 
macroeconomics = int(dbutils.widgets.get("include_macroeconomic"))
get_by_length_level = int(dbutils.widgets.get("get_by_length_level")) 
get_by_price_tier = int(dbutils.widgets.get("get_by_price_tier")) 
cpu_or_serving = dbutils.widgets.get("cpu_or_serving")
price_threshold = dbutils.widgets.get("price_threshold")
hyperparameters_file = dbutils.widgets.get("hyperparameters_file")
get_by_menthol = int(dbutils.widgets.get("get_by_menthol"))
get_extra_num_feats_lgb = json.loads(dbutils.widgets.get("get_extra_num_feats_lgb"))

print(f"Import path data: {import_data_path}")
print(f"Run id: {run_id}")
print(f"Products: {product_names}")
print(f"Predict date: {predict_date}")
print(f"History start date: {history_start_date}")
print(f"Products: {products}")
print(f"Hyperparameters_file: {hyperparameters_file}")
print(f"price_threshold: {price_threshold}")

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

# MAGIC %run ./utils_plot.py

# COMMAND ----------

import pandas as pd

train_end_date = pd.to_datetime(predict_date)

# Compute the start date for training dataset.
if history_start_date != 'none':
  train_start_date = pd.to_datetime(history_start_date)
else:
  train_start_date = pd.to_datetime(str((train_end_date - pd.Timedelta("2y")).date()))

# hyperparameters
if len(hyperparameters_file) > 0 and hyperparameters_file != "none":
    with open(f"../config/{hyperparameters_file}.json") as hyperparameter_file:
        hyperparameters = json.load(hyperparameter_file)
else:
    hyperparameters = None

default_hyperparameters = {'metric': 'mae',
          'verbosity': -1,
          'boosting_type': 'gbdt',
          'n_estimators': 600,
          'learning_rate': 0.1,
          'bagging_fraction': 1,
          'feature_fraction': 0.8,
          # 'num_threads': -1,
          'num_threads': 10,
          'monotone_constraints_method': 'advanced',
          'seed': 42}


# COMMAND ----------

# DBTITLE 1,Train Model
import lightgbm as lgb
import mlflow.lightgbm
from mlflow.tracking import MlflowClient
from pyspark.sql.types import FloatType
from pyspark.sql.functions import col, when, log1p
from functools import reduce
from pyspark.sql import DataFrame
import plotly.express as px

class LGBMWrapper(mlflow.pyfunc.PythonModel):
  """
  Wrapper to store multiple models in one pyfunc model
  """
  def __init__(self, models):
    self.models = models

  def predict(self, context, model_input):
    results = []
    
    for model in self.models:
      product_lineup_sk = model['product_lineup_sk']
      estimator = model['estimator']
      enable_features = model['enable_features']
      cat_features = model['categorical_features']
      
      product_model_input = (model_input[model_input['product_lineup_sk'] == product_lineup_sk])
      
      if product_model_input.empty:
        continue

      for c, categories in cat_features.items():
          product_model_input[c] = (product_model_input[c].astype("category")).cat.set_categories(categories)
        
      product_model_input['forecast'] = estimator.predict(product_model_input[enable_features])
      results.append(product_model_input[['forecast']])

    results_df = pd.concat(results)
    model_input = model_input.join(results_df, how='left')

    return model_input['forecast'].to_numpy()

# set experiment workspace
mlflow.set_experiment(f'/Shared/experiments/pe')

#open mlflow.autolog() to log everythings default during the training
mlflow.autolog(disable=True, log_models=False)

with mlflow.start_run(run_name=run_id) as run:
  """ Modeling step.
  Goes product by product.  
  Creates the training dataset based on the raw data.  
  Trains the LGBoost model and stores it with its metadata in mlflow. 
  """
  mlflow.log_param("run_id", run_id)

  estimators = []
  est_parameters = {}
  est_importance = {}

  # Initial Set Up
  for product_name in product_names:
    print(f"Processing {product_name}")
    name = product_name.lower().replace(" ","").replace("/", "")
    product_lineup_sk = products.get(product_name).get("id")
    l_product_lineup_sk = products.get(product_name).get("other")
    filter_training_lgb = products.get(product_name).get("filter_training_lgb")
    if filter_training_lgb==None:
        filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
            and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
            and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
            and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
            and (data_points >= 10)
            and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""
    if "hyperparameters" in products.get(product_name).keys() and "min_data_in_leaf" in products.get(product_name).get("hyperparameters"):
        min_data_in_leaf = products.get(product_name).get("hyperparameters").get("min_data_in_leaf")
    else:
        min_data_in_leaf = False
    print("min_data_in_leaf", min_data_in_leaf)
    estimator_name = f"model_{name}"

    #Features
    
    cat_feats, num_feats, additional_cols, target_col = get_features_set(product_lineup_sk, l_product_lineup_sk, macroeconomics, get_by_length_level, get_by_price_tier, get_by_menthol, get_extra_num_feats_lgb)
    if "remove_feature" in products.get(product_name).keys():  # add feature in the config file if you want to not use that feature
      for f in products.get(product_name).get("remove_feature"):
        num_feats = [c for c in num_feats if c!=f]
        cat_feats = [c for c in cat_feats if c!=f]
        if f not in additional_cols:
          additional_cols.append(f)
    enable_features = num_feats + cat_feats    

    number_of_cross_products = len(l_product_lineup_sk)

    #Creating Data
    create_training_data_view(product_lineup_sk, l_product_lineup_sk, import_data_path, 
        macroeconomics, price_threshold, gb_price_tier=get_by_price_tier, gb_cig_length=get_by_length_level, cpu_or_serving=cpu_or_serving, gb_menthol=get_by_menthol, industry_unique_id=industry_unique_id)
    columns = list(dict.fromkeys(cat_feats + num_feats + additional_cols + [target_col]))


    train_input_df = (
        spark.sql(f"select *, case when {filter_training_lgb} then 1 else 0 end no_outlier from train_vw where sales_week_forecast>='{train_start_date}'")
        .select(*columns)
    )

    decimals_cols = [x for x in train_input_df.columns if 'Decimal' in str(train_input_df.schema[x].dataType)]
    for columns in decimals_cols:
        train_input_df = train_input_df.withColumn(columns, train_input_df[columns].cast(FloatType()))

    agg_features = None 
    if "agg_features" in products.get(product_name).keys():
      agg_features = products[product_name]["agg_features"]
      for new_col, expr_template in agg_features.items():
        train_input_df = train_input_df.withColumn(new_col, eval(expr_template))
        enable_features.append(new_col)
    
    # Ensure the conversion to Pandas happens BEFORE this block
    train_input_df = train_input_df.toPandas()

    # if product_name == "MODERN ORAL INDUSTRY":
    #     # Create smoothed price roll4
    #     train_input_df['rsd_price_smooth_roll4'] = (
    #         train_input_df
    #           .groupby(['account_sk'])['rsd_price']
    #           .transform(lambda x: x.ewm(span=8).mean().rolling(window=4, min_periods=1).mean())
    #     )

    #     # Calculate smoothed percentage change
    #     rsd_price_smooth_perc = (
    #         train_input_df['rsd_price_smooth_roll4'] /
    #         train_input_df.groupby(['account_sk'])['rsd_price_smooth_roll4'].shift(4)
    #     )
        
    #     # Revert to your original logic by REMOVING .fillna(1.0)
    #     # This will keep the NaNs in the data
    #     train_input_df['rsd_price_perc'] = rsd_price_smooth_perc

    #     # Drop the temporary column
    #     train_input_df.drop('rsd_price_smooth_roll4', axis=1, inplace=True)

    #     # Clip values to within the 10th–90th state quantiles
    #     # This now operates on data with NaNs
    #     q_low  = train_input_df.groupby('account_state')['rsd_price_perc']\
    #                 .transform(lambda s: s.quantile(0.10))
    #     q_high = train_input_df.groupby('account_state')['rsd_price_perc']\
    #                 .transform(lambda s: s.quantile(0.90))
    #     train_input_df['rsd_price_perc'] = train_input_df['rsd_price_perc'].clip(lower=q_low, upper=q_high)
    # else:
    #     pass

    categorical_features = {}
    for c in cat_feats:
        train_input_df[c] = train_input_df[c].astype("category")
        categorical_features[c] = list(train_input_df[c].cat.categories.values)

    # enable_features = enable_features + ["price_tier_int","price_tier_int_log"]

    print(enable_features)

    # The hyperparameter tuning of the model was done with Optuna 
    # (An open source hyperparameter optimization framework to automate hyperparameter search)
    
    if get_by_price_tier ==1:
      monotone_dict = {
      "price_tier_int": 1
      } 
    else:
      monotone_dict = {}

    params = {'metric': 'mae',
          'verbosity': -1,
          'boosting_type': 'gbdt',
          'monotone_constraints': f"-1,{','.join(['1']*number_of_cross_products)},{','.join([str(monotone_dict.get(feat, 0)) for feat in enable_features[number_of_cross_products + 1:]])}",
          'monotone_constraints_method': 'advanced',
          'seed': 42}
    
    print(f"-1,{','.join(['1']*number_of_cross_products)},{','.join([str(monotone_dict.get(feat, 0)) for feat in enable_features])}")
    
    if hyperparameters:
        params.update(hyperparameters.get(estimator_name,default_hyperparameters))
    else:
        params.update(default_hyperparameters)

    # Weight up competitor products prices and weight down other features
    if "hyperparameters" in products.get(product_name).keys() and "features_contri" in products.get(product_name).get("hyperparameters"):
      features_contri = [1.0] * len(enable_features)
      for feat in enable_features:
        if not(feat.endswith("rsd_price_perc")):
          features_contri[enable_features.index(feat)] = products.get(product_name).get("hyperparameters")["features_contri"]
      params["feature_contri"] = features_contri

    #for c in enable_features+[target_col]: # this rounding makes the new PE closer to the old one
    #    train_input_df[c] = train_input_df[c].astype("float32").round(2)
    train_input_df["sales_week_start_date"] = pd.to_datetime(train_input_df["sales_week_start_date"])

    # Plot data
    df_plot_full = aggregate_data_for_plot(train_input_df, no_outliers=False)
    create_quantity_price_perc_plot(df_plot_full, title=f"{product_name} - Quantity change vs price change - All Data")
    df_plot_filtered = aggregate_data_for_plot(train_input_df, no_outliers=True)
    create_quantity_price_perc_plot(df_plot_filtered, title=f"{product_name} - Quantity change vs price change - No Outliers")
    df_plot_filtered = train_input_df.loc[train_input_df.no_outlier >= 1].groupby("sales_week_forecast")[["rsd_price_perc", "rsd_quantity_perc"]].mean().reset_index()
    create_quantity_price_perc_plot(df_plot_filtered, title=f"{product_name} - Quantity change vs price change (mean aggregation) - No Outliers")


    train_mask, test_mask, test_clean_mask = split_train_test(train_input_df, train_start_date, train_end_date)
    print(train_input_df[train_mask].shape, train_input_df.shape)
    print(train_input_df[train_mask][["sales_week_forecast","sales_week_start_date"]].min())
    print(f"Percentage of rows discarded: {int(100*(1-train_input_df[train_mask].shape[0]/train_input_df[(train_input_df.sales_week_forecast < train_end_date) & (train_input_df.sales_week_forecast >= train_start_date)].shape[0]))}%")
    print(f"Percentage of sales discarded: {int(100*(1-train_input_df[train_mask]['rsd_quantity_roll4'].sum()/train_input_df[(train_input_df.sales_week_forecast < train_end_date) & (train_input_df.sales_week_forecast >= train_start_date)]['rsd_quantity_roll4'].sum()))}%")

    print("size of testing dataset", train_input_df[test_clean_mask].shape, train_input_df[test_mask].shape)
    
    if train_input_df.shape[0]==0:
      continue
    

    # Uncomment to inspect basic statistics of the training subset
    # print(train_input_df[train_mask][["rsd_price_perc", "rsd_quantity_perc"]].describe())

    # for better stability and more accurate results in the model, we add more regularization for the following products
    # 3308, 3448, 30348, 30360, 32399, 36746, 63261, 78746
    # if train_input_df[train_mask].shape[0]>1000000 and (product_lineup_sk in [] or min_data_in_leaf):
    if min_data_in_leaf:
        print("add param min_data_in_leaf")
        if "hyperparameters" in products.get(product_name).keys() and "perc_min_data_in_leaf" in products.get(product_name).get("hyperparameters"):
          params["min_data_in_leaf"] = int(products.get(product_name).get("hyperparameters")["perc_min_data_in_leaf"]*train_input_df[train_mask].shape[0])
        else:
          params["min_data_in_leaf"] = int(0.05*train_input_df[train_mask].shape[0])
        print("params[min_data_in_leaf]", params["min_data_in_leaf"])
    if "hyperparameters" in products.get(product_name).keys():
      for p in products.get(product_name).get("hyperparameters"):
        if p in ["bagging_fraction", "n_estimators", "learning_rate", "feature_fraction","early_stopping_rounds", "max_depth", "min_gain_to_split", "min_data_per_group", "num_threads", "min_split_gain", "lambda_l2", "lambda_l1", "boosting_type", "metric", "huber_delta", "num_leaves", "max_bin", "min_data_in_bin", "drop_rate", "max_drop", "skip_drop", "bagging_freq",
          "feature_fraction_bynode", "path_smooth"]:
          params[p] = products.get(product_name).get("hyperparameters")[p]

    if "hyperparameters" in products.get(product_name).keys() and "low_weight" in products.get(product_name).get("hyperparameters"):
      train_input_df["deviation"] = np.maximum(train_input_df[target_col], 1/train_input_df[target_col])
      k = train_input_df[(train_input_df.no_outlier==True)]["deviation"].quantile(products.get(product_name).get("hyperparameters").get("low_weight"))
      train_input_df.loc[(train_input_df["deviation"] <= k) & (train_mask), "weight"] = 0.1
    if "hyperparameters" in products.get(product_name).keys() and "clip_target" in products.get(product_name).get("hyperparameters"):
      c = products.get(product_name).get("hyperparameters").get("clip_target")
      train_input_df.loc[train_mask, "rsd_quantity_perc"] = train_input_df.loc[train_mask, "rsd_quantity_perc"].clip(lower=1/c, upper=c)
    
    print(params)
    print(enable_features)
    
    train_data = lgb.Dataset(train_input_df[train_mask][enable_features],
                               label=train_input_df[train_mask][target_col],
                               weight=train_input_df[train_mask]["weight"],
                               categorical_feature=cat_feats,
                               free_raw_data=False)

    test_data = lgb.Dataset(train_input_df[test_mask][enable_features],
                               label=train_input_df[test_mask][target_col],
                               #weight=train_input_df[test_mask]["weight"], # In the old code we don't set weights for test dataset
                               categorical_feature=cat_feats,
                               free_raw_data=False)

    test_clean_data = lgb.Dataset(train_input_df[test_clean_mask][enable_features],
                               label=train_input_df[test_clean_mask][target_col],
                               #weight=train_input_df[test_clean_mask]["weight"], # In the old code we don't set weights for test dataset
                               categorical_feature=cat_feats,
                               free_raw_data=False)
    
    callbacks = [lgb.log_evaluation(period=25)]
    if "hyperparameters" in products.get(product_name).keys() and "min_delta" in products.get(product_name).get("hyperparameters"):
      callbacks.append(
        lgb.early_stopping(stopping_rounds=25, first_metric_only=False, min_delta=products.get(product_name).get("hyperparameters").get("min_delta"))
      )

    estimator = lgb.train(params,
                        train_data,
                        valid_sets = [train_data, test_data, test_clean_data] if test_data.data.shape[0]>0 else [train_data],
                        # verbose_eval = 25,
                        callbacks = callbacks
                        )
   

    parameters = {
      "product_lineup_sk": str(product_lineup_sk),
      "l_product_lineup_sk": l_product_lineup_sk,
      "train_start_date": str(train_start_date.date()),
      "train_end_date": str(train_end_date.date()),
      "model_params": params,
      "cat_feats": cat_feats,
      "num_feats": num_feats,
      "remove_feats": products.get(product_name).get("remove_feature"),
      "agg_feats" : agg_features,
      "additional_cols": additional_cols,
      "target_col": target_col,
      "include_macroeconomics": str(macroeconomics),
      "cpu_or_serving": cpu_or_serving,
      "price_threshold": str(price_threshold) , 
      "gb_price_tier": str(get_by_price_tier), 
      "gb_cig_length": str(get_by_length_level), 
      "gb_menthol": str(get_by_menthol),
      "stat" : {
        "mean(rsd_quantity_roll4)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].mean()), 
        "mean(rsd_price_perc)" : str(train_input_df[train_mask]["rsd_price_perc"].mean()),
        "rsd_quantity_roll4_quantile(10)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].quantile(0.1)),
        "rsd_quantity_roll4_quantile(90)" : str(train_input_df[train_mask]["rsd_quantity_roll4"].quantile(0.9)),
        "rsd_price_perc_quantile(10)" : str(train_input_df[train_mask]["rsd_price_perc"].quantile(0.1)),
        "rsd_price_perc_quantile(90)" : str(train_input_df[train_mask]["rsd_price_perc"].quantile(0.9)),
      }
    }

    mlflow.lightgbm.log_model(estimator, estimator_name)

    estimators.append({"estimator": estimator,
                       "model_name": estimator_name,
                       "enable_features": enable_features,
                       "categorical_features": categorical_features,
                       "product_lineup_sk": product_lineup_sk,
                       "l_product_lineup_sk": l_product_lineup_sk})
    est_parameters[estimator_name] = parameters

    df_importance = pd.DataFrame({
        "feature": estimator.feature_name(),
        "importance_gain": estimator.feature_importance(importance_type="gain"),
        "importance_split": estimator.feature_importance(importance_type="split"),
    }).sort_values("importance_gain", ascending=False)
    display(df_importance)
    est_importance[estimator_name] = df_importance.to_json(orient="records")

    # Get feature importance
    importance = estimator.feature_importance(importance_type='gain')  # or 'split'
    features = estimator.feature_name()
    # Create a DataFrame
    fi_df = pd.DataFrame({
        'Feature': features,
        'Importance': importance
    }).sort_values(by='Importance', ascending=False)
    # Plot with Plotly
    fig = px.bar(
        fi_df,
        x='Importance',
        y='Feature',
        orientation='h',
        title='LightGBM Feature Importance (by Gain)',
        labels={'Importance': 'Importance Score', 'Feature': 'Feature'},
        height=600
    )
    fig.update_layout(yaxis={'categoryorder':'total ascending'})
    fig.show()

    
  mlflow.log_dict({
    "parameters": est_parameters
  }, "parameters.json")
  mlflow.log_dict({
    "importance": est_importance
  }, "importance.json")

  wrappedModel = LGBMWrapper(estimators)
  mlflow.pyfunc.log_model(artifact_path="main_model", python_model=wrappedModel)

# COMMAND ----------

# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns
# import numpy as np
# from pyspark.sql.types import FloatType # For casting if needed again

# FMC_PRODUCT_ID_FOR_PLOT = 1  # CIG INDUSTRY
# REQUESTED_OUTLIER_THRESHOLD_RATIO = 2.0
# MAX_PLOTS_TO_GENERATE = 5

# print(f"--- Preparing data for Ad-hoc Plotting for Product ID: {FMC_PRODUCT_ID_FOR_PLOT} ---")
# target_product_name_for_plot = None
# target_product_details = None

# for p_name, p_details in products.items():
#     if p_details.get("id") == FMC_PRODUCT_ID_FOR_PLOT:
#         target_product_name_for_plot = p_name
#         target_product_details = p_details
#         break

# if target_product_details:
#     print(f"Target product: {target_product_name_for_plot}")
#     current_l_product_lineup_sk = target_product_details.get("other", [])
    
#     create_training_data_view(
#         product_lineup_sk=FMC_PRODUCT_ID_FOR_PLOT, 
#         l_product_linup_sk=current_l_product_lineup_sk, 
#         export_path=import_data_path, # Using import_data_path as it's used for reading
#         macroeconomics=macroeconomics, 
#         price_threshold=price_threshold, 
#         gb_price_tier=get_by_price_tier, 
#         gb_cig_length=get_by_length_level, 
#         gb_menthol=get_by_menthol, 
#         cpu_or_serving=cpu_or_serving
#     )
#     print(f"Re-created train_vw for Product ID {FMC_PRODUCT_ID_FOR_PLOT}")

#     # Re-run get_features_set for the target product ---
#     current_cat_feats, current_num_feats, current_additional_cols, current_target_col = get_features_set(
#         product_lineup_sk=FMC_PRODUCT_ID_FOR_PLOT, 
#         l_product_lineup_sk=current_l_product_lineup_sk, 
#         macroeconomics=macroeconomics, 
#         get_by_length_level=get_by_length_level, 
#         get_by_price_tier=get_by_price_tier, 
#         get_by_menthol=get_by_menthol,
#         get_extra_num_feats_lgb=get_extra_num_feats_lgb # from notebook params
#     )
    
#     # Re-run Spark SQL to get the DataFrame for the target product ---
#     # Get the filter_training_lgb specific to this product
#     current_filter_training_lgb = target_product_details.get("filter_training_lgb")
#     if current_filter_training_lgb is None: # Default filter if not specified
#         current_filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
#             and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
#             and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
#             and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
#             and (data_points >= 10)
#             and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""

#     # train_start_date should be available from the main notebook parameters
#     adhoc_train_df_spark = (
#         spark.sql(f"select *, case when {current_filter_training_lgb} then 1 else 0 end no_outlier from train_vw where sales_week_forecast>='{train_start_date}'")
#         .select(*current_cat_feats, *current_num_feats, *current_additional_cols, current_target_col)
#     )
    
#     adhoc_decimals_cols = [x for x in adhoc_train_df_spark.columns if 'Decimal' in str(adhoc_train_df_spark.schema[x].dataType)]
#     for col_name in adhoc_decimals_cols: # Renamed 'columns' to 'col_name'
#         adhoc_train_df_spark = adhoc_train_df_spark.withColumn(col_name, adhoc_train_df_spark[col_name].cast(FloatType()))

#     current_product_df_for_plot = adhoc_train_df_spark.toPandas()
    
#     if "sales_week_start_date" in current_product_df_for_plot.columns:
#         current_product_df_for_plot["sales_week_start_date"] = pd.to_datetime(current_product_df_for_plot["sales_week_start_date"])
#     else:
#         print("CRITICAL ERROR: 'sales_week_start_date' is missing after re-generating data. Plotting cannot proceed.")
#         current_product_df_for_plot = pd.DataFrame() # Empty df to skip plotting

#     print(f"Re-generated Pandas DataFrame for {target_product_name_for_plot}. Shape: {current_product_df_for_plot.shape}")

#     # Proceed with plotting logic (copied from previous iteration) ---
#     if not current_product_df_for_plot.empty and \
#        all(col in current_product_df_for_plot.columns for col in ['rsd_quantity_y', 'rsd_quantity_roll4', 'account_sk', 'sales_week_start_date']):
        
#         # Ensure numeric types (should be handled by toPandas and casting, but good for safety)
#         current_product_df_for_plot['rsd_quantity_y'] = pd.to_numeric(current_product_df_for_plot['rsd_quantity_y'], errors='coerce')
#         current_product_df_for_plot['rsd_quantity_roll4'] = pd.to_numeric(current_product_df_for_plot['rsd_quantity_roll4'], errors='coerce')
#         current_product_df_for_plot.dropna(subset=['rsd_quantity_y', 'rsd_quantity_roll4'], inplace=True)

#         plot_df_for_ratio_calc = current_product_df_for_plot[current_product_df_for_plot['rsd_quantity_roll4'] > 0.001].copy()
#         print(f"Shape of plot_df_for_ratio_calc (after rsd_quantity_roll4 > 0.001 filter & dropna): {plot_df_for_ratio_calc.shape}")

#         if not plot_df_for_ratio_calc.empty:
#             plot_df_for_ratio_calc['quantity_ratio'] = plot_df_for_ratio_calc['rsd_quantity_y'] / plot_df_for_ratio_calc['rsd_quantity_roll4']
            
#             stores_with_potential_outliers = plot_df_for_ratio_calc[
#                 plot_df_for_ratio_calc['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO
#             ]['account_sk'].unique()
#             print(f"Found {len(stores_with_potential_outliers)} store(s) with data points where quantity_ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO}.")
            
#             good_candidates_for_plotting = []
            
#             if len(stores_with_potential_outliers) > 0:
#                 print("Evaluating them against heuristic criteria...")
#                 for store_key_candidate in stores_with_potential_outliers:
#                     store_specific_data = plot_df_for_ratio_calc[plot_df_for_ratio_calc['account_sk'] == store_key_candidate].copy()
#                     outlier_data_points = store_specific_data[store_specific_data['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO]
#                     non_outlier_data_points = store_specific_data[store_specific_data['quantity_ratio'] <= REQUESTED_OUTLIER_THRESHOLD_RATIO]
                    
#                     MIN_TOTAL_WEEKS_CRITERIA = 20; MIN_OUTLIER_WEEKS_CRITERIA = 1
#                     MAX_OUTLIER_FRACTION_CRITERIA = 0.3; MAX_CV_NON_OUTLIERS_CRITERIA = 0.5 

#                     if (len(store_specific_data) >= MIN_TOTAL_WEEKS_CRITERIA and
#                         len(outlier_data_points) >= MIN_OUTLIER_WEEKS_CRITERIA and
#                         (len(outlier_data_points) / len(store_specific_data)) <= MAX_OUTLIER_FRACTION_CRITERIA and
#                         not non_outlier_data_points.empty and 'rsd_quantity_y' in non_outlier_data_points.columns):
#                         mean_qty_non_outlier = non_outlier_data_points['rsd_quantity_y'].mean()
#                         std_qty_non_outlier = non_outlier_data_points['rsd_quantity_y'].std()
#                         coeff_variation = float('inf'); is_heuristic_match = False
#                         if pd.notna(mean_qty_non_outlier) and pd.notna(std_qty_non_outlier):
#                             if mean_qty_non_outlier > 0: 
#                                 coeff_variation = std_qty_non_outlier / mean_qty_non_outlier
#                                 if coeff_variation < MAX_CV_NON_OUTLIERS_CRITERIA: is_heuristic_match = True
#                             elif mean_qty_non_outlier == 0 and std_qty_non_outlier == 0 : 
#                                 coeff_variation = 0.0; is_heuristic_match = True
#                         if is_heuristic_match:
#                             good_candidates_for_plotting.append({"account_sk": store_key_candidate, "cv": coeff_variation, 
#                                                                  "total_weeks": len(store_specific_data), "outlier_weeks": len(outlier_data_points)})
            
#             good_candidates_for_plotting.sort(key=lambda x: (x['cv'], -x['total_weeks']))
#             selected_stores_to_plot_info = good_candidates_for_plotting[:MAX_PLOTS_TO_GENERATE] 

#             if not selected_stores_to_plot_info:
#                 print(f"  Could not find any stores that meet the specified heuristic criteria for plotting for {target_product_name_for_plot} (ID: {FMC_PRODUCT_ID_FOR_PLOT}).")
#             else:
#                 print(f"\nFound {len(selected_stores_to_plot_info)} suitable store(s). Plotting top {len(selected_stores_to_plot_info)}:")
#                 for store_info in selected_stores_to_plot_info:
#                     selected_store_account_key_for_plot = store_info['account_sk']
#                     print(f"\n--- Plotting Store (account_sk): {selected_store_account_key_for_plot} ---")
#                     print(f"    CV: {store_info['cv']:.2f}, Total Weeks: {store_info['total_weeks']}, Outlier Weeks: {store_info['outlier_weeks']}")
#                     store_to_plot_df = plot_df_for_ratio_calc[plot_df_for_ratio_calc['account_sk'] == selected_store_account_key_for_plot].copy()
#                     store_to_plot_df.sort_values('sales_week_start_date', inplace=True)
#                     store_to_plot_df['is_outlier_for_plot'] = store_to_plot_df['quantity_ratio'] > REQUESTED_OUTLIER_THRESHOLD_RATIO
#                     plt.figure(figsize=(18, 9))
#                     sns.lineplot(data=store_to_plot_df, x='sales_week_start_date', y='rsd_quantity_y', marker='o', linestyle='-', label='Weekly Sales (rsd_quantity_y)', color='dodgerblue', zorder=1, markersize=5)
#                     outlier_points_for_plot_display = store_to_plot_df[store_to_plot_df['is_outlier_for_plot']]
#                     if not outlier_points_for_plot_display.empty:
#                         sns.scatterplot(data=outlier_points_for_plot_display, x='sales_week_start_date', y='rsd_quantity_y', color='red', s=150, edgecolor='black', label=f'Outlier (ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO})', zorder=3)
#                     sns.lineplot(data=store_to_plot_df, x='sales_week_start_date', y='rsd_quantity_roll4', marker='.', linestyle='--', label='4-Week Rolling Avg Sales (rsd_quantity_roll4)', color='grey', zorder=2, markersize=4)
#                     plt.title(f"Weekly Sales for Store {selected_store_account_key_for_plot} | Product: {target_product_name_for_plot} (ID {FMC_PRODUCT_ID_FOR_PLOT})\nHighlighting Ratio > {REQUESTED_OUTLIER_THRESHOLD_RATIO} | CV (non-outlier): {store_info['cv']:.2f}", fontsize=16)
#                     plt.xlabel("Sales Week Start Date", fontsize=14); plt.ylabel("Quantity", fontsize=14)
#                     plt.legend(title="Legend", title_fontsize='12', fontsize='10', loc='upper left'); plt.grid(True, linestyle=':', alpha=0.6)
#                     plt.xticks(rotation=30, ha='right'); plt.tight_layout(); plt.show()
#         else: 
#             print(f"  No data available for {target_product_name_for_plot} after initial filtering for ratio calculation.")
#     else:
#         print(f"  Could not re-generate data for {target_product_name_for_plot} or critical columns are missing.")
#     print(f"--- End of Ad-hoc Plotting for Product ID: {FMC_PRODUCT_ID_FOR_PLOT} ---\n")
# else:
#     print(f"ERROR: Product ID {FMC_PRODUCT_ID_FOR_PLOT} not found in the 'products' configuration. Cannot proceed with ad-hoc plotting.")

# COMMAND ----------

dbutils.notebook.exit(json.dumps({"success": True, "model_uri": f"runs:/{run.info.run_id}/main_model"}))

# COMMAND ----------

# DBTITLE 1,test simulation
# x_values = [0.95, 0.975, 1.025, 1.05]
# # x_values = [1.01].query("account_state=='NC'")
# results = []

# for x in x_values:
#     for price_tier in train_input_df.price_tier.unique():
#         df_sim = (train_input_df[train_mask]
#         .query(f"price_tier=='{price_tier}'").copy()
#         )
#         if df_sim.shape[0]==0:
#             continue
#         df_sim["rsd_price_perc"] = x

#         y_pred = estimator.predict(df_sim[enable_features])
#         df_sim["prediction"] = y_pred
#         df_sim["rsd_price_perc"] = 1.0
#         y_pred = estimator.predict(df_sim[enable_features])
#         df_sim["prediction_no_price_change"] = y_pred

#         weighted_avg = (df_sim["prediction"] * df_sim["rsd_quantity_roll4"]).sum() / (df_sim["prediction_no_price_change"]* df_sim["rsd_quantity_roll4"]).sum()
#         results.append({"rsd_price_perc_simulated": x, "weighted_avg_prediction": weighted_avg, "price_tier": price_tier})

# df_test = pd.DataFrame(results)
# df_test["approx_elasticity"] = (df_test["weighted_avg_prediction"] - 1) / (df_test["rsd_price_perc_simulated"] - 1)

# df_test

# COMMAND ----------

# DBTITLE 1,Hyperparameters Optimization
# def run_optuna_tuning(train_input_df, enable_features, target_col, weight_col, cat_feats,
#                       train_mask, test_mask, test_clean_mask, number_of_cross_products, n_trials=20):
#     import optuna
#     from sklearn.metrics import mean_absolute_error
#     import lightgbm as lgb

#     def objective(trial):
#         params = {
#             'boosting_type': 'gbdt',
#             'metric': 'mae',
#             'verbosity': -1,
#             'n_estimators': trial.suggest_int('n_estimators', 100, 800),
#             'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#             'num_leaves': trial.suggest_int('num_leaves', 20, 150),
#             'min_data_in_leaf': int(0.05 * train_input_df[train_mask].shape[0]),
#             'max_depth': trial.suggest_int('max_depth', 3, 12),
#             'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),
#             'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),
#             'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),
#             'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),
#             'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),
#             'min_split_gain': trial.suggest_float('min_split_gain', 0.0, 1.0),
#             'monotone_constraints': f"-1,{','.join(['1']*number_of_cross_products)},{','.join(['0']*(len(enable_features)-1-number_of_cross_products))}",
#             'monotone_constraints_method': 'advanced',
#             'seed': 21,
#             'num_threads': -1
#         }

#         train_data = lgb.Dataset(
#             train_input_df[train_mask][enable_features],
#             label=train_input_df[train_mask][target_col],
#             weight=train_input_df[train_mask][weight_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         test_data = lgb.Dataset(
#             train_input_df[test_mask][enable_features],
#             label=train_input_df[test_mask][target_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         test_clean_data = lgb.Dataset(
#             train_input_df[test_clean_mask][enable_features],
#             label=train_input_df[test_clean_mask][target_col],
#             categorical_feature=cat_feats,
#             free_raw_data=False
#         )

#         model = lgb.train(
#             params,
#             train_data,
#             valid_sets=[train_data, test_data, test_clean_data] if test_data.data.shape[0] > 0 else [train_data],
#             verbose_eval=False
#         )

#         preds = model.predict(train_input_df[test_mask][enable_features])
#         y_true = train_input_df[test_mask][target_col].values
#         mae = mean_absolute_error(y_true, preds)

#         print(f"Trial MAE: {mae:.4f}")
#         return mae

#     print("🔧 Running Optuna tuning...")
#     study = optuna.create_study(direction="minimize")
#     study.optimize(objective, n_trials=n_trials)

#     print(f"✅ Best MAE: {study.best_value}")
#     print(f"🏆 Best Params: {study.best_params}")
#     return study.best_params

# product_hyperparams = run_optuna_tuning(
#     train_input_df=train_input_df,
#     enable_features=enable_features,
#     target_col=target_col,
#     weight_col="weight",
#     cat_feats=cat_feats,
#     train_mask=train_mask,
#     test_mask=test_mask,
#     test_clean_mask=test_clean_mask,
#     number_of_cross_products=number_of_cross_products,
#     n_trials=70
# )

# product_hyperparams

# COMMAND ----------



</file>

<file path="notebooks/03.1 Simulation RSD.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # 3-4 Simulation and RSD
# MAGIC
# MAGIC This notebook merges the functionality of `03 - Simulation` and `04 - PE RSD`.
# MAGIC
# MAGIC **Objective:**
# MAGIC Eliminate intermediate S3 writes between simulation and Price Elasticity (PE) calculation to improve performance and reduce storage costs.
# MAGIC
# MAGIC **Workflow:**
# MAGIC 1.  **Load Parameters & Model**: Fetches all necessary parameters and loads the trained ML model from MLflow.
# MAGIC 2.  **Generate Simulation Data**: For each product, it creates price variation scenarios using the ML model. This step now keeps the resulting DataFrame in Spark's memory.
# MAGIC 3.  **Calculate Price Elasticity**: The in-memory simulation DataFrame is immediately used to compute Price Elasticity via log-linear regression.
# MAGIC 4.  **Write Final Output**: The final PE results are written to the specified export path, maintaining compatibility with downstream processes.

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

dbutils.widgets.removeAll()

# Parameters from 03 - Simulation
dbutils.widgets.text("import_data_path", "s3://...")
dbutils.widgets.text("model_uri", "runs:/...")
dbutils.widgets.text("compute_cross_pe", "1")
dbutils.widgets.text("simulation_price_variation", "[0.95, 0.975, 1, 1.025, 1.05]")
dbutils.widgets.text("cpu_or_serving", "cpu")
dbutils.widgets.text("price_threshold", "1")
dbutils.widgets.text("dynamic_dates_alignment", "0")

# Parameters from 04 - PE RSD
dbutils.widgets.text("products", """{}""")
dbutils.widgets.text("min_weekly_rsd_quantity", "1")
dbutils.widgets.text("predict_date", "2023-03-01")

# Common Parameters
dbutils.widgets.text("run_id", "pe_run/2023-03-01")
dbutils.widgets.text("export_path", "s3://...")
dbutils.widgets.text("product_names", """["PRODUCT_A", "PRODUCT_B"]""")
dbutils.widgets.text("states", """["GA", "FL"]""")
dbutils.widgets.text("get_by_length_level", "0")
dbutils.widgets.text("get_by_price_tier", "0")
dbutils.widgets.text("get_by_menthol", "0")
dbutils.widgets.text("delta", "1")
dbutils.widgets.text("industry_unique_id", "1")
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

import json
import numpy as np
import pandas as pd
from functools import reduce

import mlflow
from mlflow.tracking.client import MlflowClient
from sklearn import linear_model

from pyspark.sql import DataFrame
from pyspark.sql.functions import lit, col, when
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, DateType, StringType, FloatType, IntegerType

# COMMAND ----------

# Simulation parameters
import_data_path = dbutils.widgets.get("import_data_path")
model_uri = dbutils.widgets.get("model_uri")
compute_cross_pe = int(dbutils.widgets.get("compute_cross_pe"))
simulation_price_variation = json.loads(dbutils.widgets.get("simulation_price_variation"))
cpu_or_serving = dbutils.widgets.get("cpu_or_serving")
price_threshold = dbutils.widgets.get("price_threshold")
dynamic_dates_alignment = int(dbutils.widgets.get("dynamic_dates_alignment"))
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))

# PE RSD parameters
products_config = json.loads(dbutils.widgets.get("products"))
min_weekly_rsd_quantity = int(dbutils.widgets.get("min_weekly_rsd_quantity"))
predict_date = dbutils.widgets.get("predict_date")

# Common parameters
run_id = dbutils.widgets.get("run_id")
export_path = dbutils.widgets.get("export_path")
products_names = json.loads(dbutils.widgets.get("product_names"))
states = json.loads(dbutils.widgets.get("states"))
get_by_length_level = int(dbutils.widgets.get("get_by_length_level"))
get_by_price_tier = int(dbutils.widgets.get("get_by_price_tier"))
get_by_menthol = int(dbutils.widgets.get("get_by_menthol"))
delta = int(dbutils.widgets.get("delta"))
export_suffix = dbutils.widgets.get("export_suffix")

print(f"Run ID: {run_id}")
print(f"Export Path: {export_path}")
print(f"In-memory data passing enabled. Intermediate S3 writes are disabled.")
print(f"Output format: {'Delta' if delta == 1 else 'Parquet'}")

# COMMAND ----------

client = MlflowClient()
model = mlflow.pyfunc.load_model(model_uri)
artifact_uri = client.get_run(model.metadata.run_id).to_dictionary()["info"]["artifact_uri"]
model_parameters = mlflow.artifacts.load_dict(artifact_uri + "/parameters.json").get("parameters")

L_PRODUCTS = {}
for product_name in products_names:
    prod_params = model_parameters.get(f'model_{product_name.lower().replace(" ","").replace("/", "")}')
    product_lineup_sk = int(prod_params.get("product_lineup_sk"))
    l_product_lineup_sk = prod_params.get("l_product_lineup_sk")
    L_PRODUCTS[product_lineup_sk] = l_product_lineup_sk

# COMMAND ----------

# DBTITLE 1,Helper Function: Simulation (from Notebook 03)
def simulate_points(df):
    """Simulate new data points using the prediction model and different price variations."""
    product_lineup_sk = df['product_lineup_sk'].astype(int).values[0]
    pe_type = df['pe_type'].values[0]
    cross_pid = df['simulated_price_product_lineup_sk'].values[0]
    l_product_linup_sk_tmp = L_PRODUCTS.get(product_lineup_sk, []).copy()

    # DEBUG: Print simulation entry info
    if df.shape[0] > 0 and df.shape[0] < 10:  # Only print for small groups to avoid spam
        print(f"  [DEBUG-SIM] Simulating for product_lineup_sk={product_lineup_sk}, pe_type={pe_type}, cross_pid={cross_pid}, rows={df.shape[0]}")

    X = df.copy()
    if cat_feats:
        for cat_col in cat_feats:
            if cat_col in X.columns:
                X[cat_col] = X[cat_col].astype("category")
    
    l_df = []
    
    if cross_pid == product_lineup_sk and pe_type == "pe":
        n_pid = -1
    elif cross_pid in l_product_linup_sk_tmp:
        n_pid = l_product_linup_sk_tmp.index(cross_pid)
    else:
        return pd.DataFrame()

    if X.shape[0] == 0:
        return pd.DataFrame()

    for i in simulation_price_variation:
        i = np.round(i, 4)
        
        # Update the price percentage change for the simulation
        if n_pid == -1:
            X.iloc[:, 0] = i
        else:
            X.iloc[:, n_pid + 1] = i

        # Create simulated data dictionary
        price_col_key = "rsd_price_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_price_roll4"
        quantity_col_key = "rsd_quantity_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_quantity_roll4"
        
        prediction_features = X[enable_features + ['product_lineup_sk']]
        predicted_volume_ratio = model.predict(prediction_features)

        data = {
            "pe_type": pe_type,
            "account_sk": df["account_sk"].values,
            "account_state": df["account_state"].values,
            "product_lineup_sk": df["product_lineup_sk"].values,
            "sales_week_forecast": df["sales_week_forecast"].values,
            "price": df[price_col_key].values * i,
            "volume": predicted_volume_ratio * df["rsd_quantity_roll4"],
            "rsd_price": df["rsd_price_roll4"].values,
            "rsd_quantity": df["rsd_quantity_roll4"].values,
            "rsd_quantity_industry": df["rsd_quantity_industry_roll4"].values,
            "rsd_price_industry": df["rsd_price_industry_roll4"].values,
            "rsd_price_cross": df[price_col_key].values,
            "rsd_quantity_cross": df[quantity_col_key].values,
            "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values
        }
        
        if get_by_length_level: data["cig_length"] = df["cig_length"].values
        if get_by_price_tier: data["price_tier"] = df["price_tier"].values
        if get_by_menthol: data["menthol_non_menthol"] = df["menthol_non_menthol"].values
            
        l_df.append(pd.DataFrame(data=data))

    return pd.concat(l_df) if l_df else pd.DataFrame()

# COMMAND ----------

# DBTITLE 1,Helper Function: PE Calculation (from Notebook 04)
def custom_linear_reg(df):
    """Compute price elasticity using a log-linear regression model."""
    constant = 1  # for log purposes
    
    # log(quantity) = c + pe*log(price) + dummies(weeks)
    X = df[["price", "sales_week_forecast"]]
    X.loc[:, "price"] = np.log(X["price"])
    X = pd.get_dummies(data=X, drop_first=True).values
    y = np.log(df.volume + constant)

    model = linear_model.LinearRegression(n_jobs=1)
    model.fit(X, y)
    
    results = {
        "account_sk": df["account_sk"].values[0],
        "account_state": df["account_state"].values[0],
        "product_lineup_sk": df["product_lineup_sk"].values[0],
        "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values[0],
        "pe_type": df["pe_type"].values[0],
        "pe": [model.coef_[0]],
        "intercept": [model.intercept_],
        "rsquared": [model.score(X, y)],
        "data_points_weeks": len(df.sales_week_forecast.unique()),
        "data_points": df.shape[0],
        "rsd_price": df.rsd_price.mean(),
        "rsd_quantity": df.rsd_quantity.mean(),
        "rsd_quantity_industry": df.rsd_quantity_industry.mean(),
        "rsd_price_industry": df.rsd_price_industry.mean(),
        "rsd_price_cross": df.rsd_price_cross.mean(),
        "rsd_quantity_cross": df.rsd_quantity_cross.mean(),
        "min_week": df.sales_week_forecast.min(),
        "max_week": df.sales_week_forecast.max(),
        "predict_date": df.predict_date.values[0]
    }
    
    if get_by_length_level: results["cig_length"] = df["cig_length"].values[0]
    if get_by_price_tier: results["price_tier"] = df["price_tier"].values[0]
    if get_by_menthol: results["menthol_non_menthol"] = df["menthol_non_menthol"].values[0]
  
    return pd.DataFrame(results)

# COMMAND ----------

# Schema for the output of simulate_points
simulation_schema_fields = [
    StructField("pe_type", StringType()),
    StructField("account_sk", IntegerType()),
    StructField("account_state", StringType()),
    StructField("product_lineup_sk", IntegerType()),
    StructField("sales_week_forecast", DateType()),
    StructField("price", FloatType()),
    StructField("volume", FloatType()),
    StructField("rsd_price", FloatType()),
    StructField("rsd_quantity", FloatType()),
    StructField("rsd_quantity_industry", FloatType()),
    StructField("rsd_price_industry", FloatType()),
    StructField("rsd_price_cross", FloatType()),
    StructField("rsd_quantity_cross", FloatType()),
    StructField("simulated_price_product_lineup_sk", IntegerType())
]
if get_by_length_level: simulation_schema_fields.append(StructField("cig_length", StringType()))
if get_by_price_tier: simulation_schema_fields.append(StructField("price_tier", StringType()))
if get_by_menthol: simulation_schema_fields.append(StructField("menthol_non_menthol", StringType()))
simulation_schema = StructType(simulation_schema_fields)

# Schema for the output of custom_linear_reg
pe_schema_fields = [
    StructField("account_sk", IntegerType()),
    StructField("account_state", StringType()),
    StructField("product_lineup_sk", IntegerType()),
    StructField("simulated_price_product_lineup_sk", IntegerType()),
    StructField("pe_type", StringType()),
    StructField("pe", FloatType()),
    StructField("intercept", FloatType()),
    StructField("rsquared", FloatType()),
    StructField("data_points_weeks", IntegerType()),
    StructField("data_points", IntegerType()),
    StructField("rsd_price", FloatType()),
    StructField("rsd_quantity", FloatType()),
    StructField("rsd_quantity_industry", FloatType()),
    StructField("rsd_price_industry", FloatType()),
    StructField("rsd_price_cross", FloatType()),
    StructField("rsd_quantity_cross", FloatType()),
    StructField("min_week", DateType()),
    StructField("max_week", DateType()),
    StructField("predict_date", StringType())
]
if get_by_length_level: pe_schema_fields.append(StructField("cig_length", StringType()))
if get_by_price_tier: pe_schema_fields.append(StructField("price_tier", StringType()))
if get_by_menthol: pe_schema_fields.append(StructField("menthol_non_menthol", StringType()))
pe_schema = StructType(pe_schema_fields)

# COMMAND ----------

# DBTITLE 1,Main Processing Loop: Simulation and PE Calculation
from pyspark.sql.types import DecimalType, DoubleType
from pyspark.sql import functions as F
from functools import reduce
import time

for product_name in products_names:
    print(f"--- Processing product: {product_name} ---")
    product_key = product_name.lower().replace(" ", "").replace("/", "")
    prod_params = model_parameters.get(f"model_{product_key}")

    if not prod_params:
        print(f"Warning: Could not find model parameters for {product_name}. Skipping.")
        continue

    # 1. SETUP
    cat_feats = prod_params.get("cat_feats")
    num_feats = prod_params.get("num_feats")
    agg_feats = prod_params.get("agg_feats")
    macroeconomics_flag = int(prod_params.get("include_macroeconomics"))
    product_lineup_sk = int(prod_params.get("product_lineup_sk"))
    l_product_lineup_sk = prod_params.get("l_product_lineup_sk")

    enable_features = num_feats + cat_feats

    end_date_pe = pd.to_datetime(prod_params.get("train_end_date")).date()
    start_date_pe = (end_date_pe - pd.Timedelta("1y"))
    if dynamic_dates_alignment == 1:
        start_date_pe, _ = get_dynamic_start_end_dates(no_years=1)
    
    print(f"  Simulation period: {start_date_pe} to {end_date_pe}")
    
    def simulate_points(df):
        """Simulate new data points using the prediction model and different price variations."""
        # This function will now correctly "close over" the variables from the loop
        
        # Get product context from the first row of the pandas DataFrame
        current_product_lineup_sk = df['product_lineup_sk'].astype(int).values[0]
        pe_type = df['pe_type'].values[0]
        cross_pid = df['simulated_price_product_lineup_sk'].values[0]
        l_product_linup_sk_tmp = L_PRODUCTS.get(current_product_lineup_sk, []).copy()

        X = df.copy()
        if cat_feats:
            for cat_col in cat_feats:
                if cat_col in X.columns:
                    X[cat_col] = X[cat_col].astype("category")
        
        l_df = []
        
        if cross_pid == current_product_lineup_sk and pe_type == "pe":
            n_pid = -1
        elif cross_pid in l_product_linup_sk_tmp:
            n_pid = l_product_linup_sk_tmp.index(cross_pid)
        else:
            return pd.DataFrame()

        if X.shape[0] == 0:
            return pd.DataFrame()

        for i in simulation_price_variation:
            i = np.round(i, 4)
            
            # Update the price percentage change for the simulation
            if n_pid == -1:
                X.iloc[:, 0] = i
            else:
                X.iloc[:, n_pid + 1] = i

            # Create simulated data dictionary
            price_col_key = "rsd_price_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_price_roll4"
            quantity_col_key = "rsd_quantity_roll4" if n_pid == -1 else f"{l_product_linup_sk_tmp[n_pid]}_rsd_quantity_roll4"
            
            # The model wrapper needs 'product_lineup_sk' for routing logic.
            columns_for_prediction = enable_features + ['product_lineup_sk']
            predicted_volume_ratio = model.predict(X[columns_for_prediction])

            data = {
                "pe_type": pe_type,
                "account_sk": df["account_sk"].values,
                "account_state": df["account_state"].values,
                "product_lineup_sk": df["product_lineup_sk"].values,
                "sales_week_forecast": df["sales_week_forecast"].values,
                "price": df[price_col_key].values * i,
                "volume": predicted_volume_ratio * df["rsd_quantity_roll4"],
                "rsd_price": df["rsd_price_roll4"].values,
                "rsd_quantity": df["rsd_quantity_roll4"].values,
                "rsd_quantity_industry": df["rsd_quantity_industry_roll4"].values,
                "rsd_price_industry": df["rsd_price_industry_roll4"].values,
                "rsd_price_cross": df[price_col_key].values,
                "rsd_quantity_cross": df[quantity_col_key].values,
                "simulated_price_product_lineup_sk": df["simulated_price_product_lineup_sk"].values
            }
            
            if get_by_length_level: data["cig_length"] = df["cig_length"].values
            if get_by_price_tier: data["price_tier"] = df["price_tier"].values
            if get_by_menthol: data["menthol_non_menthol"] = df["menthol_non_menthol"].values
                
            l_df.append(pd.DataFrame(data=data))

        return pd.concat(l_df) if l_df else pd.DataFrame()

    create_training_data_view(product_lineup_sk, l_product_lineup_sk, import_data_path,
        macroeconomics_flag, price_threshold, get_by_price_tier, get_by_length_level, get_by_menthol, cpu_or_serving,
        industry_unique_id=industry_unique_id)

    base_sql_query = f"""
        SELECT * FROM train_vw
        WHERE sales_week_forecast >= date('{start_date_pe}') AND sales_week_forecast <= date('{end_date_pe}')
          -- AND (rsd_quantity_y / rsd_quantity_roll4 < 2) AND (rsd_quantity_roll4 / rsd_quantity_y < 2)
          -- AND (rsd_price_y / rsd_price_roll4 < 1.5) AND (rsd_price_roll4 / rsd_price_y < 1.5)
          AND account_state IN {tuple(states) if len(states) > 1 else '(%s)' % ', '.join(map(repr, states))}
    """
    
    model_features = num_feats + cat_feats
    additional_cols = ["account_sk", "product_lineup_sk", "sales_week_forecast", 
                       "rsd_price_roll4", "rsd_quantity_roll4", "rsd_quantity_industry_roll4", "rsd_price_industry_roll4"] + \
                      [f"{pid}_rsd_quantity_roll4" for pid in l_product_lineup_sk] + \
                      [f"{pid}_rsd_price_roll4" for pid in l_product_lineup_sk]
    
    all_needed_cols = model_features + [col for col in additional_cols if col not in model_features]
    df_for_simulation = spark.sql(base_sql_query).select(*all_needed_cols)

    for field in df_for_simulation.schema.fields:
        if isinstance(field.dataType, (DecimalType, DoubleType)):
            df_for_simulation = df_for_simulation.withColumn(field.name, col(field.name).cast(FloatType()))

    df_for_simulation.cache()
    print(f"  Base data for simulation cached. Count: {df_for_simulation.count()}")
    
    # 2. SIMULATION
    all_simulation_dfs = []
    sim_groups = ["sales_week_forecast", "account_state"]
    if get_by_length_level: sim_groups.append("cig_length")
    if get_by_price_tier: sim_groups.append("price_tier")
    if get_by_menthol: sim_groups.append("menthol_non_menthol")

    print("  Generating PE simulations...")
    results_pe = (df_for_simulation
        .withColumn("simulated_price_product_lineup_sk", lit(product_lineup_sk))
        .withColumn("pe_type", lit("pe"))
        .filter("rsd_quantity_roll4 > 0")
        .groupBy(sim_groups)
        .applyInPandas(simulate_points, schema=simulation_schema))
    all_simulation_dfs.append(results_pe)

    # ---------------------------------------------------------------------------------------------------------------------------
    start = time.time()
    if compute_cross_pe:
        print("  Generating CROSSPE simulations...")
        # for cross_pid in list(set([1059168, 36744, 4157]) & set(l_product_lineup_sk)):
        for cross_pid in l_product_lineup_sk:
            results_pe = (
                df_for_simulation
                .withColumn("simulated_price_product_lineup_sk", lit(cross_pid))
                .withColumn("pe_type", lit("cross_pe"))
                .filter(f"{cross_pid}_rsd_quantity_roll4 > 0") 
                .groupBy(sim_groups)
                .applyInPandas(simulate_points, schema=simulation_schema)
            )
            all_simulation_dfs.append(results_pe)
        print("  CROSSPE simulations complete")

 
    print("  Combining all simulations without reduce using RDD...")
    first_schema = all_simulation_dfs[0].schema  
    combined_rdd = spark.sparkContext.union([df.rdd for df in all_simulation_dfs])
    df_simulation_combined = spark.createDataFrame(combined_rdd, schema=first_schema)

    # -----------------------------------------------------------------------------------------------
    df_simulation_combined.cache()
    conteo = df_simulation_combined.count()
    print(f"Total count: {conteo}")
    print("Total time to generate and cache CROSSPE:", time.time() - start, "seconds")


    # ---------------------------------------------------------------------------------------------------------------------------
    print("  [REPLICATION STEP] Rounding simulated data to mimic I/O precision changes.")
    rounding_precision = 6
    df_simulation_combined = df_simulation_combined.withColumn(
        "volume", F.round(col("volume"), rounding_precision)
    ).withColumn(
        "price", F.round(col("price"), rounding_precision)
    )

    # 3. PE CALCULATION
    print("  Calculating PE from replicated in-memory simulation data...")
    pe_groups = ["pe_type", "account_sk", "account_state", "product_lineup_sk", "simulated_price_product_lineup_sk"]
    if get_by_length_level: pe_groups.append("cig_length")
    if get_by_price_tier: pe_groups.append("price_tier")
    if get_by_menthol: pe_groups.append("menthol_non_menthol")

    final_pe_results = (df_simulation_combined
        .filter(f"rsd_quantity_cross >= {min_weekly_rsd_quantity} AND price IS NOT NULL")
        .withColumn("predict_date", lit(predict_date))
        .withColumn("volume", when(col("volume") < 0, 0).otherwise(col("volume")))
        .groupBy(pe_groups)
        .applyInPandas(custom_linear_reg, schema=pe_schema)
    )        

    # --- START: MODIFIED SECTION ---
    # 4. MATERIALIZE AND WRITE FINAL OUTPUT
    
    # Force the execution of all lazy transformations for the current product
    # by caching the results and triggering an action (.count()). This prevents
    # the non-deterministic UDF error on subsequent loop iterations.
    # final_pe_results.cache()
    # final_pe_results.count() # This action materializes the cache and triggers the computation.

    output_format = "delta" if delta == 1 else "parquet"
    output_path = f"{export_path}{run_id}/computed_pe{export_suffix}/{product_key}_{'delta' if delta==1 else 'parquet'}"
    
    print(f"  Writing final PE results to: {output_path} (Format: {output_format})")
    # This save operation will now write the readily available cached data.
    final_pe_results.write.mode('overwrite').format(output_format).save(output_path)
    
    # Clean up caches for this iteration to free up memory for the next product
    df_for_simulation.unpersist()
    final_pe_results.unpersist()
    # --- END: MODIFIED SECTION ---

print("--- All products processed. ---")

</file>

<file path="notebooks/06 - Evaluate.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Evaluate Notebook
# MAGIC
# MAGIC Plot the PE computed

# COMMAND ----------

!pip install -U kaleido==0.2.1

# COMMAND ----------

dbutils.library.restartPython() # for kaleido to work (the goal is the display png images as databricks cannot display large outputs)

# COMMAND ----------

# dbutils.widgets.removeAll()
# dbutils.widgets.text("pe_str", "/FileStore/pe/pe_model_dev/2022-06-01/str_pe/*.parquet")
# dbutils.widgets.text("pe_rsd", "/FileStore/pe/pe_model_dev/2022-06-01/computed_pe/*.parquet")
# dbutils.widgets.text("data_path", "s3://test-rey-0f513d7f0af2c2a9-datasource/pe_model_dev_test/databricks/2023/test/data/df_product.parquet")
# dbutils.widgets.text("cross_pe", "0")
# dbutils.widgets.text("delta", "1")
# dbutils.widgets.text("industry_unique_id", "1")

# COMMAND ----------

# MAGIC %run ./utils.py

# COMMAND ----------

import plotly.express as px
import numpy as np
import plotly.graph_objects as go
import pandas as pd
from delta.tables import DeltaTable

from py4j.protocol import Py4JJavaError
def path_exist(path):
    try:
        base_dir = path
        if "*" in path:
             base_dir = "/".join(path.split("/")[:-1]) + "/"
        dbutils.fs.ls(base_dir) 
        return True
    except Exception as e:
        print(f"Path check failed for {path} (or its base dir): {e}")
        return False

pe_str = dbutils.widgets.get("pe_str").strip() 
pe_rsd_base_path_pattern = dbutils.widgets.get("pe_rsd").strip() 
data_path = dbutils.widgets.get("data_path").strip()
cross_pe = int(dbutils.widgets.get("cross_pe")) 
delta = int(dbutils.widgets.get("delta")) 
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))
import re

def normalize_s3_path(p: str) -> str:
    p = p.strip()
    if p.startswith("s3://"):
        scheme = "s3://"
        rest = p[len(scheme):]
        rest = re.sub(r'/+', '/', rest)  # collapse repeated slashes
        return scheme + rest
    return re.sub(r'/+', '/', p)

pe_str = normalize_s3_path(dbutils.widgets.get("pe_str"))
pe_rsd_base_path_pattern = normalize_s3_path(dbutils.widgets.get("pe_rsd"))
data_path = normalize_s3_path(dbutils.widgets.get("data_path"))

print(f"pe_str path pattern: '{pe_str}'") 
print(f"pe_rsd base path pattern: '{pe_rsd_base_path_pattern}'") 
print(f"data_path: '{data_path}'")
print(f"cross_pe flag: {cross_pe}")
print(f"delta flag: {delta}")


# Handle pe_str Reading 
# --- Build 'str' view from every <product>_delta sub-folder ----------------

try:
    (
        load_delta_tables_with_wildcards(spark, [f"{pe_str}/*"], union_all=True)
        .transform(lambda df: df if "pe_rsd" in df.columns else df.withColumnRenamed("pe", "pe_rsd"))
        .createOrReplaceTempView("str")
    )
except Exception as e:
    print(f"WARNING: Could not load STR data from {pe_str}. Using empty schema. Error: {e}")
    schema_str_ddl = (
        "`account_state` STRING, `account_sk` STRING, `product_name` STRING, "
        "`pe_rsd` DOUBLE, `estimated_pe` DOUBLE, `str_weekavg_quantity` DOUBLE, "
        "`knn_computed` INT, `scan_validated_flag` STRING, `contract` STRING, "
        "`rsd_quantity` DOUBLE, `rsd_price` DOUBLE, `str_price` DOUBLE"
    )
    spark.createDataFrame([], schema_str_ddl).createOrReplaceTempView("str")

# Handle 'rsd' view creation 
# Uses pe_rsd_base_path_pattern
if delta == 1:
    # Delta Reading Logic for 'rsd' view 
    try:
        load_delta_tables_with_wildcards(spark, [f"{pe_rsd_base_path_pattern}/*"], union_all=True).createOrReplaceTempView("rsd")
    except Exception as e: 
        print(f"ERROR: Failed during processing RSD delta data from {pe_rsd_base_path_pattern}. Error: {e}")
        raise e
else: # delta == 0
    # Parquet Reading Logic for 'rsd' view 
    pe_rsd_path = pe_rsd_base_path_pattern 
    read_format = "parquet" 
    print(f"\nDelta is not 1, attempting to read PARQUET files from: {pe_rsd_path}")
    try:
        spark.read.format(read_format).load(pe_rsd_path).createOrReplaceTempView("rsd")
        print(f"Successfully read RSD parquet data and created 'rsd' view.")
    except Exception as e: 
        print(f"ERROR: Failed to read RSD parquet data from {pe_rsd_path}. Error: {e}")
        raise e

# Handle 'data' and 'data_industry' views (conditional on delta)
def _table_exists(name: str) -> bool:
    try:
        return spark.catalog.tableExists(name)
    except:
        return False

if delta == 1:
    # --- PRODUCT (view: data) ---
    base_dir_for_product_data = "/".join(data_path.split("/")[:-1])
    product_filename_stem = data_path.split("/")[-1].replace(".parquet", "")

    actual_product_delta_path = f"{base_dir_for_product_data}/delta/{product_filename_stem}"
    alt_product_delta_path    = f"{base_dir_for_product_data}/delta/{product_filename_stem}_delta"

    created_data = False
    try:
        spark.read.format("delta").load(actual_product_delta_path).createOrReplaceTempView("data")
        print(f"Created 'data' from DELTA: {actual_product_delta_path}")
        created_data = True
    except Exception as e1:
        print(f"Warn: product DELTA not found at {actual_product_delta_path}: {e1}")
        try:
            spark.read.format("delta").load(alt_product_delta_path).createOrReplaceTempView("data")
            print(f"Created 'data' from DELTA (alt): {alt_product_delta_path}")
            created_data = True
        except Exception as e2:
            print(f"Warn: product DELTA alt failed at {alt_product_delta_path}: {e2}")

    if not created_data:
        try:
            spark.read.parquet(data_path).createOrReplaceTempView("data")
            print(f"Fell back to PARQUET for 'data': {data_path}")
        except Exception as e3:
            print(f"ERROR: could not create 'data' from parquet {data_path}: {e3}")

    # --- INDUSTRY (view: data_industry) ---
    actual_industry_delta_path = f"{base_dir_for_product_data}/delta/df_industry"
    alt_industry_delta_path    = f"{base_dir_for_product_data}/delta/df_industry_delta"
    industry_parquet_path      = f"{base_dir_for_product_data}/df_industry.parquet"

    created_industry = False
    try:
        (spark.read.format("delta").load(actual_industry_delta_path)
              .filter(f"product_lineup_sk={industry_unique_id}")
              .createOrReplaceTempView("data_industry"))
        print(f"Created 'data_industry' from DELTA: {actual_industry_delta_path}")
        created_industry = True
    except Exception as e1:
        print(f"Warn: industry DELTA not found at {actual_industry_delta_path}: {e1}")
        try:
            (spark.read.format("delta").load(alt_industry_delta_path)
                  .filter(f"product_lineup_sk={industry_unique_id}")
                  .createOrReplaceTempView("data_industry"))
            print(f"Created 'data_industry' from DELTA (alt): {alt_industry_delta_path}")
            created_industry = True
        except Exception as e2:
            print(f"Warn: industry DELTA alt failed at {alt_industry_delta_path}: {e2}")

    if not created_industry:
        try:
            (spark.read.parquet(industry_parquet_path)
                  .filter(f"product_lineup_sk={industry_unique_id}")
                  .createOrReplaceTempView("data_industry"))
            print(f"Fell back to PARQUET for 'data_industry': {industry_parquet_path}")
        except Exception as e3:
            print(f"ERROR: could not create 'data_industry' from parquet {industry_parquet_path}: {e3}")

else:
    # Existing parquet branch: keep it, but don't raise if it fails
    print(f"\nAttempting to read product data from: {data_path}")
    try:
        spark.read.parquet(data_path).createOrReplaceTempView("data")
        print(f"Created 'data' (parquet).")
    except Exception as e:
        print(f"ERROR: product parquet failed at {data_path}: {e}")

    try:
        industry_data_path = "/".join(data_path.split("/")[:-1]) + "/df_industry.parquet"
        print(f"Attempting to read industry data: {industry_data_path}")
        (spark.read.parquet(industry_data_path)
              .filter(f"product_lineup_sk={industry_unique_id}")
              .createOrReplaceTempView("data_industry"))
        print(f"Created 'data_industry' (parquet).")
    except Exception as e:
        print(f"ERROR: industry parquet failed at {industry_data_path}: {e}")


# COMMAND ----------

# MAGIC %md
# MAGIC # PE

# COMMAND ----------

# DBTITLE 1,Training volumes
sql = """
select pe_type, product_lineup_sk, simulated_price_product_lineup_sk, count(*) nb_rows
, sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
, sum(rsd_quantity_cross) rsd_quantity_cross
, count(distinct account_sk) n_acccount
from rsd 
group by pe_type, product_lineup_sk, simulated_price_product_lineup_sk
"""
df = spark.sql(sql).toPandas()
display(df)

# COMMAND ----------

# DBTITLE 1,Training and real volumes
# Training and real volumes
max_date = pe_rsd_base_path_pattern.split("/")[-2]
min_date = str((pd.to_datetime(max_date) - pd.DateOffset(years=1)).date())

if spark.catalog.tableExists("data") and spark.catalog.tableExists("data_industry"):
    display(spark.sql(f"""
        with weekly_avg_rsd as (
            select data.product_lineup_sk, sum(rsd_quantity)/52 rsd_quantity_raw_data
                 , count(distinct data.account_sk) n_account_raw_data
            from data inner join data_industry using (account_sk, sales_week_sk)
            where sales_week_end_date < '{max_date}'
              and sales_week_end_date >= '{min_date}'
              and rsd_quantity > 0
            group by data.product_lineup_sk
        ),
        pe_rsd as (
            select pe_type, product_lineup_sk, simulated_price_product_lineup_sk
                 , count(*) nb_rows
                 , sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
                 , sum(rsd_quantity_cross) rsd_quantity_cross
                 , count(distinct account_sk) n_acccount
            from rsd
            group by pe_type, product_lineup_sk, simulated_price_product_lineup_sk
        )
        select pe_rsd.*
             , weekly_avg_rsd.rsd_quantity_raw_data weekly_avg_raw_data
             , n_account_raw_data
        from pe_rsd left join weekly_avg_rsd using (product_lineup_sk)
    """))
else:
    print("Skipping 'Training and real volumes': 'data' or 'data_industry' view not available.")


# COMMAND ----------

# MAGIC %md
# MAGIC # PE Distribution

# COMMAND ----------

if path_exist(f"{pe_str}"): 
    print("STR path exists, reading from 'str' view for PE distribution...")
    df = spark.sql("""
        SELECT  account_state,
                account_sk,
                product_name,
                pe_rsd,                    -- RSD‑based PE
                estimated_pe  AS pe_str    -- STR‑based / model PE
        FROM    str
    """).toPandas()

    if df.empty:
        print("Warning: 'str' view was empty. Falling back to RSD data only for PE distribution DataFrame.")
        df = spark.sql("""
            SELECT  account_state,
                    account_sk,
                    CAST(product_lineup_sk AS string) AS product_name,
                    pe               AS pe_rsd,
                    CAST(NULL AS double)              AS pe_str
            FROM    rsd
            WHERE   product_lineup_sk = simulated_price_product_lineup_sk
        """).toPandas()  # use 'rsd' view
else:
    print("STR path does not exist, reading from 'rsd' view only for PE distribution...")
    df = spark.sql("""
        SELECT  account_state,
                account_sk,
                CAST(product_lineup_sk AS string) AS product_name,
                pe               AS pe_rsd,
                CAST(NULL AS double)              AS pe_str
        FROM    rsd
        WHERE   product_lineup_sk = simulated_price_product_lineup_sk
    """).toPandas()

print("\nDataFrame 'df' for PE Distribution head:")
print(df.head())


# COMMAND ----------

# DBTITLE 1,Histograms
if not df.empty:
    product_names_in_df = df["product_name"].unique()
    print(f"\nGenerating histograms for products: {product_names_in_df}")
    for product_name in product_names_in_df:
        # Filter data for the current product
        df_product = df.query(f"product_name==@product_name") # Use @ syntax for query variable
        
        # Prepare data for stacking (handle potential missing pe_str if only rsd data was available)
        if 'pe_str' in df_product.columns:
             df_melt = df_product[["account_sk","pe_rsd","pe_str"]].set_index("account_sk").stack().reset_index()
        else: # Should not happen with current logic creating empty view, but safe check
             df_melt = df_product[["account_sk","pe_rsd"]].set_index("account_sk").stack().reset_index()
        
        df_melt = df_melt.rename(columns={"level_1": "pe_type", 0: "pe"})
        
        # Create histogram
        fig = px.histogram(
            df_melt,
            x="pe",
            facet_col="pe_type",
            facet_col_spacing=0.1,
            color="pe_type",
            title=f"PE Distribution for {product_name}"
        )
        fig.update_yaxes(matches=None) # Allow independent y-axes
        fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1])) # Clean up facet titles
        fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))
        fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))
        try:
            display(fig.show(renderer="png")) # Use display() for Databricks environment
        except Exception as plot_err:
            print(f"Warning: Could not display histogram for {product_name}. Error: {plot_err}")
else:
    print("\nDataFrame 'df' for PE Distribution is empty. Skipping histograms.")


# COMMAND ----------

# DBTITLE 1,PE by product_lineup_sk
print("\nDescriptive Statistics for PE by Product:")
if not df.empty:
    # Ensure both columns exist before describing, handle cases where pe_str might be all null
    cols_to_describe = [col for col in ["pe_rsd", "pe_str"] if col in df.columns and df[col].notna().any()]
    if cols_to_describe:
        display(df.groupby(["product_name"])[cols_to_describe].describe().reset_index())
    else:
        print("No valid PE columns found in the DataFrame to describe.")
else:
    print("DataFrame 'df' is empty. Skipping description by product.")

# COMMAND ----------

# DBTITLE 1,PE by product_lineup_sk and state
print("\nDescriptive Statistics for PE by Product and State:")
if not df.empty:
    cols_to_describe = [col for col in ["pe_rsd", "pe_str"] if col in df.columns and df[col].notna().any()]
    if cols_to_describe:
        display(df.groupby(["product_name","account_state"])[cols_to_describe].describe().reset_index())
    else:
         print("No valid PE columns found in the DataFrame to describe.")
else:
    print("DataFrame 'df' is empty. Skipping description by product and state.")

# COMMAND ----------

# MAGIC %md
# MAGIC #PE STR

# COMMAND ----------

def display_pe_str(custom_group: str = ""):
    query = f"""
        SELECT
            product_lineup_sk,
            product_name
            {custom_group}
            , ROUND(SUM(pe_rsd * rsd_quantity) / SUM(rsd_quantity), 4)     AS wpe_rsd
            , ROUND(
                SUM(pe_rsd * str_weekavg_quantity) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity ELSE 0 END),
                4
              ) AS wpe_rsd_str_quant           -- wpe for RSD PE using STR quantity
            , ROUND(
                SUM(
                    estimated_pe * (
                        CASE WHEN scan_validated_flag = 'Y'
                             THEN str_weekavg_quantity ELSE 0 END
                    )
                ) /
                SUM(
                    CASE WHEN scan_validated_flag = 'Y'
                         THEN str_weekavg_quantity ELSE 0 END
                ),
                4
              ) AS wpe_str_scan                -- scanned stores
            , ROUND(
                SUM(
                    estimated_pe * (
                        CASE WHEN scan_validated_flag = 'N'
                             THEN str_weekavg_quantity ELSE 0 END
                    )
                ) /
                SUM(
                    CASE WHEN scan_validated_flag = 'N'
                         THEN str_weekavg_quantity ELSE 0 END
                ),
                4
              ) AS wpe_str_nonscan             -- non‑scanned stores
            , ROUND(
                SUM(estimated_pe * str_weekavg_quantity) /
                SUM(str_weekavg_quantity),
                4
              ) AS wpe_str_all                 -- all STR stores
            , ROUND(
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity ELSE 0 END),
                0
              ) AS str_weekavg_quantity_rsd
            , SUM(str_weekavg_quantity)                                    AS str_weekavg_quantity
            , ROUND(
                SUM(CASE WHEN knn_computed = 0 THEN 1 ELSE 0 END),
                0
              ) AS n_acccount_rsd
            , COUNT(DISTINCT CASE WHEN scan_validated_flag = 'N' THEN account_sk END)
              AS n_acccount_str_scan
            , COUNT(DISTINCT account_sk)                                    AS n_acccount
            , ROUND(SUM(rsd_quantity), 0)                                   AS rsd_quantity
            , ROUND(
                SUM(
                    CASE WHEN knn_computed = 0 THEN rsd_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity END),
                2
              ) AS rsd_price
            , ROUND(
                SUM(
                    CASE WHEN knn_computed = 0 THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN knn_computed = 0 THEN str_weekavg_quantity END),
                2
              ) AS str_price_rsd
            , ROUND(
                SUM(
                    CASE WHEN scan_validated_flag = 'Y' THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN scan_validated_flag = 'Y' THEN str_weekavg_quantity END),
                2
              ) AS str_price_scan
            , ROUND(
                SUM(
                    CASE WHEN scan_validated_flag = 'N' THEN str_price * str_weekavg_quantity END
                ) /
                SUM(CASE WHEN scan_validated_flag = 'N' THEN str_weekavg_quantity END),
                2
              ) AS str_price_nonscan
        FROM
            str   -- the Spark VIEW we created
        GROUP BY
            product_lineup_sk,
            product_name
            {custom_group}
        ORDER BY
            product_name {custom_group}
    """
    try:
        group_label = custom_group if custom_group else "Product Only"
        print(f"\nDisplaying PE‑STR results (grouped by: '{group_label}')")
        display(spark.sql(query))
    except Exception as e:
        print(f"ERROR executing display_pe_str query (group: '{custom_group}'). Error: {e}")


if path_exist(f"{pe_str}"): 
    print("\n--- PE STR Section ---")
    str_view_count = spark.sql("SELECT COUNT(*) FROM str").first()[0]
    if str_view_count > 0:
        display_pe_str()
        display_pe_str(", contract")
        display_pe_str(", account_state")
        display_pe_str(", account_state, contract")
    else:
        print("Skipping PE STR displays because the 'str' view is empty.")
else:
    print("\nSkipping PE STR displays because STR path does not exist.")

# COMMAND ----------

# MAGIC %md
# MAGIC # Cross PE Distribution

# COMMAND ----------

df_crosspe = pd.DataFrame() 
if cross_pe:
    print("\n--- Cross PE Section ---")
    try:
        df_crosspe = spark.sql("""
            with name_map as (select distinct product_lineup_sk, kpl_preferred_name product_name from data)
            select rsd.*, t1.product_name, t2.product_name cross_product_name
            from rsd
            left join name_map t1 on rsd.product_lineup_sk = t1.product_lineup_sk
            left join name_map t2 on rsd.simulated_price_product_lineup_sk = t2.product_lineup_sk
            where rsd.product_lineup_sk != rsd.simulated_price_product_lineup_sk
        """).toPandas()
        print(f"Successfully generated Cross PE DataFrame with {len(df_crosspe)} rows.")
        if df_crosspe.empty:
            print("Warning: Cross PE DataFrame is empty (no cross PE data found in 'rsd' view).")
    except Exception as e:
        print(f"ERROR generating Cross PE DataFrame. Error: {e}")
        cross_pe = 0 
else:
     print("\nSkipping Cross PE section because cross_pe flag is 0.")


# COMMAND ----------

if not cross_pe or df_crosspe.empty:
    if cross_pe and df_crosspe.empty:
         # Exit only if cross_pe was requested but no data was found
         print("\nExiting notebook: Cross PE was requested (cross_pe=1), but no cross PE data was found.")
         dbutils.notebook.exit("Cross PE data not available")
    else:
         # If cross_pe was 0 initially, just print message
         print("\nCross PE section skipped.")

# COMMAND ----------

# DBTITLE 1,Boxplot
if cross_pe and not df_crosspe.empty:
    print("\nGenerating Cross PE boxplots...")
    product_names_crosspe = df_crosspe.product_name.unique()
    print(f"Products with cross PE data: {product_names_crosspe}")
    for product_name in product_names_crosspe:
        try:
            df_product_cross = df_crosspe.query(f"product_name==@product_name")
            gb = df_product_cross.groupby("cross_product_name")[["pe"]]
            stats = gb.agg(
                q1=pd.NamedAgg(column="pe", aggfunc=lambda x: x.quantile(0.25)),
                mean=pd.NamedAgg(column="pe", aggfunc="mean"),
                median=pd.NamedAgg(column="pe", aggfunc="median"),
                q3=pd.NamedAgg(column="pe", aggfunc=lambda x: x.quantile(0.75)),
                lowerfence=pd.NamedAgg(column="pe", aggfunc="min"),
                upperfence=pd.NamedAgg(column="pe", aggfunc="max"),
                count=pd.NamedAgg(column="pe", aggfunc="size") # Get count for y-axis label if needed
            ).reset_index()

            if not stats.empty:
                fig = go.Figure()
                fig.add_trace(go.Box(
                    x=stats["cross_product_name"], # Use cross_product_name for x-axis categories
                    q1=stats["q1"],
                    mean=stats["mean"],
                    median=stats["median"],
                    q3=stats["q3"],
                    lowerfence=stats["lowerfence"],
                    upperfence=stats["upperfence"],
                    name=product_name # Add name for potential legend if plotting multiple traces later
                ))
                fig.update_layout(
                     title_text=f"Cross PE Distribution for {product_name} (Effect OF other product price changes ON {product_name})",
                     xaxis_title="Product Whose Price Changed",
                     yaxis_title="Calculated Cross PE"
                )
                display(fig.show(renderer="png"))
            else:
                 print(f"  Skipping boxplot for {product_name}: No aggregated stats found.")
        except Exception as boxplot_err:
            print(f"Warning: Could not display Cross PE boxplot for {product_name}. Error: {boxplot_err}")

# COMMAND ----------

# DBTITLE 1,Cross PE
if cross_pe and not df_crosspe.empty:
    print("\nCross PE Descriptive Statistics:")
    try:
        # Add product names for better readability
        display(df_crosspe.groupby(["product_name","cross_product_name"])["pe"].describe().reset_index())
    except Exception as desc_err:
         print(f"Warning: Could not display Cross PE descriptive stats. Error: {desc_err}")

# COMMAND ----------



</file>

<file path="notebooks/other/accuracy_rsd_lgbm.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Calculate accuracy of the model in test data
# MAGIC
# MAGIC Predict volume sold at the observation level and compare to actual results.
# MAGIC The output is the accuracy at the global level but can also be computed at the state level.
# MAGIC For observation level results you need to modify the notebook to save that table.
# MAGIC
# MAGIC Accuracy here is actually 1 - MAPE.
# MAGIC

# COMMAND ----------

# dbutils.widgets.removeAll()
dbutils.widgets.text(
    "folder",
    "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/",
)
dbutils.widgets.text("run_id", "2023/08/vapor_cpu_cross_12periods/full")

# dbutils.widgets.text(
#     "export_path", 
#     "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/",
# )
dbutils.widgets.text("model_to_use", "all")
dbutils.widgets.text("max_predict_date", "2023-06-01") 
dbutils.widgets.text("config_path", "../config/vapor_full.json")

dbutils.widgets.text("delta", "0")
dbutils.widgets.text("industry_unique_id", "1")
dbutils.widgets.text("export_suffix", "")

# COMMAND ----------

# %pip install lightgbm==3.3.5
%pip install lightgbm==4.0.0
%pip install mlflow==2.6.0

# COMMAND ----------

# MAGIC %run ../utils.py

# COMMAND ----------

import json
from dateutil.relativedelta import relativedelta
import datetime
import calendar
import os 
import mlflow
from mlflow.entities import ViewType

folder = dbutils.widgets.get("folder")
run_id = dbutils.widgets.get("run_id")
import_data_path = folder + run_id
max_predict_date =  dbutils.widgets.get("max_predict_date")
delta_input = int(dbutils.widgets.get("delta"))
industry_unique_id = int(dbutils.widgets.get("industry_unique_id"))
export_suffix =  dbutils.widgets.get("export_suffix")

raw_config_path = dbutils.widgets.get("config_path")
print(f"Raw config_path from widget: {raw_config_path}")

current_notebook_path_full = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()
current_notebook_dir = os.path.dirname(current_notebook_path_full)
print(f"Current notebook path full: {current_notebook_path_full}")
print(f"Current notebook directory: {current_notebook_dir}")

if not raw_config_path.startswith('/'): 
    resolved_path_relative_to_notebook = os.path.abspath(os.path.join(current_notebook_dir, raw_config_path))
    print(f"Resolved path relative to notebook: {resolved_path_relative_to_notebook}")

    if current_notebook_path_full.startswith('/Users/') and \
       resolved_path_relative_to_notebook.startswith('/Users/'):
        
        workspace_equivalent_path = "/Workspace" + resolved_path_relative_to_notebook
        print(f"Attempting Workspace equivalent path: {workspace_equivalent_path}")
        
        try:
            dbutils.fs.ls(workspace_equivalent_path) 
            print(f"Workspace equivalent path {workspace_equivalent_path} seems to exist. Using it.")
            config_path = workspace_equivalent_path
        except Exception:
            print(f"Workspace equivalent path {workspace_equivalent_path} not found. Falling back to resolved path relative to notebook: {resolved_path_relative_to_notebook}")
            config_path = resolved_path_relative_to_notebook # Fallback
    else:
        config_path = resolved_path_relative_to_notebook
else:
    config_path = raw_config_path
    print(f"Config_path from widget is already absolute: {config_path}")

print(f"Final config_path to be used: {config_path}")

try:
    with open(config_path, "r") as f:
        config = json.load(f)
except FileNotFoundError:
    print(f"ERROR: Config file not found at FINAL path: {config_path}")
    # Check if the other variant exists if one failed
    if config_path.startswith("/Workspace"):
        alternative_path = config_path.replace("/Workspace", "", 1) # /Users/...
        print(f"Attempting alternative path: {alternative_path}")
        try:
            with open(alternative_path, "r") as f_alt: config = json.load(f_alt)
            print(f"Success with alternative path: {alternative_path}")
        except:
            print(f"Alternative path {alternative_path} also failed.")
            raise
    elif config_path.startswith("/Users"):
        alternative_path = "/Workspace" + config_path # /Workspace/Users/...
        print(f"Attempting alternative path: {alternative_path}")
        try:
            with open(alternative_path, "r") as f_alt: config = json.load(f_alt)
            print(f"Success with alternative path: {alternative_path}")
        except:
            print(f"Alternative path {alternative_path} also failed.")
            raise
    else:
        raise

except Exception as e:
    print(f"Error loading config file {config_path}: {e}")
    raise



states = config.get("states")
products_config = config.get("products")
product_names = list(products_config.keys())
product_id_list = [d.get("id") for d in config.get("products").values()]

model_to_use = dbutils.widgets.get("model_to_use")

def get_previous_months(date1):
    """Return a list of the three months preceding ``date1``."""
    date1 = datetime.datetime.strptime(date1, "%Y-%m-%d")
    date_list = []
    for i in range(3):  # iterate backwards month by month
        date1 = date1.replace(day=1)
        date1 = date1 - relativedelta(months=1)
        date_list.append(date1.strftime("%Y-%m-%d"))
    return date_list

predict_dates = [max_predict_date]
predict_dates.extend(get_previous_months(max_predict_date))
predict_dates.sort() # Sort to have oldest first, newest last

print(f"Running for run_id: {run_id}")
print(f"Import data path (for train_vw): {import_data_path}")
print(f"Product names from config: {product_names}")
print(f"Product ids from config: {product_id_list}")
print(f"Predict dates for model loading/testing: {predict_dates}")
print(f"Model to use policy: {model_to_use}")
# print(f"Delta input: {delta_input}")


# COMMAND ----------

import numpy as np
import pandas as pd
from functools import reduce
from pyspark.sql import DataFrame
from mlflow.tracking.client import MlflowClient
print("Attempting to configure MLflow for Databricks environment...")
try:
    host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()
    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

    if host and token:
        os.environ['DATABRICKS_HOST'] = host
        os.environ['DATABRICKS_TOKEN'] = token
        mlflow.set_tracking_uri("databricks")
        print("MLflow tracking URI set to 'databricks'. DATABRICKS_HOST and DATABRICKS_TOKEN populated.")
    else:
        print("Warning: Could not retrieve Databricks host or token using dbutils. MLflow might rely on existing env vars or default config.")
        mlflow.set_tracking_uri("databricks")

except Exception as e:
    print(f"Error trying to get host/token with dbutils or set env vars: {e}")
    try:
        mlflow.set_tracking_uri("databricks")
    except Exception as e_set_uri:
        print(f"Could not even set tracking_uri: {e_set_uri}")


print(f"DATABRICKS_HOST (after attempting set): {os.environ.get('DATABRICKS_HOST')}")
print(f"DATABRICKS_TOKEN (after attempting set): {os.environ.get('DATABRICKS_TOKEN')}")

print("Initializing MlflowClient...")
try:
    client = MlflowClient()
    print("MlflowClient initialized successfully.")
except Exception as e:
    print(f"Error initializing MlflowClient: {e}")
    raise

# COMMAND ----------

def predict_sales(df):
    """Predicts with the model and compares to actual results."""
    global model, get_by_length_level, get_by_price_tier, get_by_menthol, current_cat_feats_for_udf

    product_lineup_sk = df["product_lineup_sk"].astype(int).values[0]
    pe_type = df["pe_type"].values[0]

    X = df.copy()
    if current_cat_feats_for_udf: 
        for cat_col in current_cat_feats_for_udf: 
            if cat_col in X.columns: 
                X[cat_col] = X[cat_col].astype("category")
            else:
                print(f"Warning in predict_sales UDF: Categorical feature '{cat_col}' not found in input DataFrame columns: {X.columns}")
    l_df = []

    columns = [
        "pe_type", "account_sk", "account_state", "product_lineup_sk",
        "sales_week_forecast", "price", "volume", 
        "rsd_price", "rsd_quantity", "rsd_quantity_industry", "rsd_price_industry",
        "rsd_price_cross", "rsd_quantity_cross",
    ]
    if get_by_length_level: columns.append("cig_length")
    if get_by_price_tier: columns.append("price_tier")
    if get_by_menthol: columns.append("menthol_non_menthol")


    if X.shape[0] == 0: 
        empty_data_for_schema = {
            "pe_type": [], "account_sk": [], "account_state": [], "product_lineup_sk": [],
            "sales_week_start_date": [], "sales_week_forecast": [],
            "actual_volume": [], "predicted_volume": [], "rsd_price": [],
            "rsd_quantity_roll4": [], "rsd_quantity_industry": [], "rsd_price_industry": []
        }
        if get_by_length_level: empty_data_for_schema["cig_length"] = []
        if get_by_price_tier: empty_data_for_schema["price_tier"] = []
        if get_by_menthol: empty_data_for_schema["menthol_non_menthol"] = []
        return pd.DataFrame(data=empty_data_for_schema)

    
    predicted_volume_values = model.predict(X) * df.loc[:, "rsd_quantity_roll4"].values
    
    data = {
        "pe_type": pe_type, 
        "account_sk": df.loc[:, "account_sk"].values,
        "account_state": df.loc[:, "account_state"].values,
        "product_lineup_sk": df.loc[:, "product_lineup_sk"].values,
        "sales_week_start_date": df.loc[:, "sales_week_start_date"].values,
        "sales_week_forecast": df.loc[:, "sales_week_forecast"].values,
        "actual_volume": df.loc[:, "rsd_quantity_y"].values,
        "predicted_volume": predicted_volume_values,
        "rsd_price": df.loc[:, "rsd_price_roll4"].values,
        "rsd_quantity_roll4": df.loc[:, "rsd_quantity_roll4"].values,
        "rsd_quantity_industry": df.loc[:, "rsd_quantity_industry_roll4"].values,
        "rsd_price_industry": df.loc[:, "rsd_price_industry_roll4"].values,
    }
    if get_by_length_level: # This global flag is set in compute_predictions
        data["cig_length"] = df.loc[:, "cig_length"].values
    if get_by_price_tier: # This global flag is set in compute_predictions
        data["price_tier"] = df.loc[:, "price_tier"].values
    if get_by_menthol: # This global flag is set in compute_predictions
        data["menthol_non_menthol"] = df.loc[:, "menthol_non_menthol"].values
    
    df_tmp = pd.DataFrame(data=data)
    l_df.append(df_tmp)

    return pd.concat(l_df)

# COMMAND ----------


from pyspark.sql.types import StructType, StructField, DateType, StringType, FloatType, IntegerType, TimestampType
from pyspark.sql.functions import lit
import pyspark.sql.functions as F


def compute_predictions(
    product_name_key,
    product_lineup_sk,
    l_product_lineup_sk,
    start_date,
    end_date,
    current_model_params,
):
    num_feats = current_model_params.get("num_feats", []) # Default to empty list
    cat_feats = current_model_params.get("cat_feats", []) # Default to empty list
    macroeconomics = int(current_model_params.get("include_macroeconomics", 0))
    get_by_length_level_flag = int(current_model_params.get("gb_cig_length", 0))
    get_by_price_tier_flag = int(current_model_params.get("gb_price_tier", 0))
    get_by_menthol_flag = int(current_model_params.get("gb_menthol", 0))
    cpu_or_serving = current_model_params.get("cpu_or_serving")
    price_threshold_val = int(current_model_params.get("price_threshold", 0)) # Provide a default

    field_list = [
        StructField("pe_type", StringType()),
        StructField("account_sk", IntegerType()),
        StructField("account_state", StringType()),
        StructField("product_lineup_sk", IntegerType()),
        StructField("sales_week_start_date", DateType()),
        StructField("sales_week_forecast", DateType()),
        StructField("actual_volume", FloatType()),
        StructField("predicted_volume", FloatType()),
        StructField("rsd_price", FloatType()),
        StructField("rsd_quantity_roll4", FloatType()),
        StructField("rsd_quantity_industry", FloatType()),
        StructField("rsd_price_industry", FloatType()),
    ]
    if get_by_length_level_flag: field_list += [StructField("cig_length", StringType())]
    if get_by_price_tier_flag: field_list += [StructField("price_tier", StringType())]
    if get_by_menthol_flag: field_list += [StructField("menthol_non_menthol", StringType())]
    result_schema = StructType(field_list)

    create_training_data_view(
        product_lineup_sk,
        l_product_lineup_sk,
        import_data_path,
        macroeconomics,
        price_threshold_val,
        gb_price_tier=get_by_price_tier_flag,
        gb_cig_length=get_by_length_level_flag,
        gb_menthol=get_by_menthol_flag,
        cpu_or_serving=cpu_or_serving,
        industry_unique_id=industry_unique_id
    )

    additional_cols_definition = ( # Renamed to avoid confusion with additional_cols list for select
        [
            "account_sk", # Needed by predict_sales
            # "product_lineup_sk", # This will be part of grouping_cols_for_select
            # "sales_week_start_date", # Needed by predict_sales
            # "sales_week_forecast", # This will be part of grouping_cols_for_select
            "rsd_quantity_y", # Needed by predict_sales for actual_volume
            "rsd_price_roll4", # Needed by predict_sales
            "rsd_quantity_roll4", # Needed by predict_sales
            "rsd_quantity_industry_roll4", # Needed by predict_sales
            "rsd_price_industry_roll4", # Needed by predict_sales
        ]
        + [f"{pid}_rsd_quantity_roll4" for pid in l_product_lineup_sk if l_product_lineup_sk] # Add if check
        + [f"{pid}_rsd_price_roll4" for pid in l_product_lineup_sk if l_product_lineup_sk] # Add if check
    )
    if (get_by_length_level_flag or get_by_price_tier_flag or get_by_menthol_flag):
        additional_cols_definition.append(f"{product_lineup_sk}_rsd_quantity_roll4")
        additional_cols_definition.append(f"{product_lineup_sk}_rsd_price_roll4")
    
    additional_cols_definition = sorted(list(set(additional_cols_definition)))


    next_month = str((pd.to_datetime(end_date) + pd.DateOffset(months=1)).date())
    filter_training_lgb = current_model_params.get("filter_training_lgb")
    if filter_training_lgb is None:
        filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
            and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
            and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
            and (relative_price_hist < 1.1) and (relative_price_hist > 0.9)
            and (data_points >= 10)
            and (rsd_quantity_ind_perc>=0.7) and (rsd_quantity_ind_perc<=1.5)"""

    sql = f"""
    SELECT * 
    FROM train_vw
    WHERE sales_week_forecast>=date('{start_date}') AND sales_week_forecast<date('{next_month}')
    AND {filter_training_lgb}
    AND account_state IN {tuple(states) if len(states) > 1 else "('%s')" % ", ".join(map(repr, states))}
    """
    print(f"SQL for fetching training data for product {product_name_key}:\n{sql}")
    
    grouping_cols_for_select = ["product_lineup_sk", "sales_week_forecast", "account_state"]
    cols_for_udf_not_features = ["sales_week_start_date"] 

    if get_by_length_level_flag: grouping_cols_for_select.append("cig_length")
    if get_by_price_tier_flag: grouping_cols_for_select.append("price_tier")
    if get_by_menthol_flag: grouping_cols_for_select.append("menthol_non_menthol")
    
    all_selected_cols = sorted(list(set(num_feats + cat_feats + additional_cols_definition + grouping_cols_for_select + cols_for_udf_not_features)))
    
    print(f"Selecting columns for df: {all_selected_cols}")
    df_from_sql = spark.sql(sql)

    missing_cols_in_train_vw = [col for col in all_selected_cols if col not in df_from_sql.columns]
    if missing_cols_in_train_vw:
        print(f"Warning: The following columns specified for selection do not exist in 'train_vw' after SQL query: {missing_cols_in_train_vw}")
        all_selected_cols = [col for col in all_selected_cols if col in df_from_sql.columns]
        print(f"Revised columns for df: {all_selected_cols}")

    if not all_selected_cols:
        print("Error: No columns to select after checking. Aborting for this product.")
        return pd.DataFrame(), pd.DataFrame()

    df = df_from_sql.select(*all_selected_cols)


    decimals_cols = [
        x for x in df.columns if ("Decimal" in str(df.schema[x].dataType)) or ("Double" in str(df.schema[x].dataType)) # Fixed OR condition
    ]
    for column_to_cast in decimals_cols: # Renamed variable
        df = df.withColumn(column_to_cast, df[column_to_cast].cast(FloatType()))

    groups = grouping_cols_for_select # Use the explicitly defined grouping_cols_for_select

    print(f"Columns in df before groupBy: {df.columns}")
    print(f"Grouping by columns: {groups}")


    print("Processing Predictions")
    global get_by_length_level, get_by_price_tier, get_by_menthol # To modify globals for UDF
    get_by_length_level = get_by_length_level_flag
    get_by_price_tier = get_by_price_tier_flag
    get_by_menthol = get_by_menthol_flag
    
    # Create a temporary Delta table to break the lineage
    import uuid
    tmp_path = f"dbfs:/tmp/pe_accuracy_{product_lineup_sk}_{start_date}_{uuid.uuid4().hex[:8]}"

    results_temp = (
        df.withColumn("pe_type", lit("pe"))
        .filter( (F.col("rsd_quantity_roll4").isNotNull()) & (F.col("rsd_quantity_roll4") > 0) )
        .groupBy(groups)
        .applyInPandas(predict_sales, schema=result_schema)
    )

    # Write and read back to break lineage
    results_temp.write.mode("overwrite").format("delta").save(tmp_path)
    results = spark.read.format("delta").load(tmp_path)

    # Clean up temp table after processing (optional, add at the end of the function)
    # dbutils.fs.rm(tmp_path, recurse=True)

    safe_sum_actual_volume = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
    results_schema_cols = [field.name for field in result_schema] # Get column names from result_schema
    safe_actual_volume_for_mape = F.when(F.col("actual_volume").isNotNull() & (F.col("actual_volume") != 0), F.col("actual_volume")).otherwise(F.lit(None).cast(FloatType()))


    results_summarized_state = (
        results.groupBy("product_lineup_sk", "sales_week_forecast", "account_state")
        .agg(
            F.sum(F.abs(F.col("actual_volume") - F.col("predicted_volume"))).alias("sum_absolute_error"),
            F.sum("actual_volume").alias("sum_actual_volume"),
            F.sum("predicted_volume").alias("sum_predicted_volume"),
            F.countDistinct("account_sk").alias("nb_account"),
            F.mean(F.abs(F.col("actual_volume") - F.col("predicted_volume")) / safe_actual_volume_for_mape).alias("mape")
        )
        .withColumn("w_accuracy", 100 * (1 - F.col("sum_absolute_error") / safe_sum_actual_volume))
        .withColumn("w_accuracy_mape", 100 * (1 - F.col("mape")))
        .withColumn("w_accuracy_week_product_state", 100 * (1 - F.abs(F.col("sum_actual_volume") - F.col("sum_predicted_volume")) / safe_sum_actual_volume))
    )
    
    safe_total_actual_volume_national = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))

    results_summarized_national = (
        results_summarized_state.groupBy("product_lineup_sk")
        .agg(
            F.sum("sum_absolute_error").alias("total_sum_absolute_error"), # Alias to avoid conflict
            F.sum("sum_actual_volume").alias("total_sum_actual_volume"),   # Alias to avoid conflict
            F.sum("nb_account").alias("nb_account"),
            F.mean("w_accuracy_mape").alias("avg_w_accuracy_mape"), # Alias
            F.mean("w_accuracy_week_product_state").alias("avg_w_accuracy_week_product_state") #Alias
        )
        .withColumn( # Recalculate w_accuracy at national level
            "w_accuracy",
            100 * (1 - F.col("total_sum_absolute_error") / F.when(F.col("total_sum_actual_volume")!=0,F.col("total_sum_actual_volume")).otherwise(None) )
        )
    )
    return results_summarized_state.toPandas(), results_summarized_national.toPandas()

# COMMAND ----------

def find_mlflow_run_by_pl_sk(ml_runs, target_pl_sk):
    """
    Correctly searches for a product within a single MLflow run that contains multiple models.
    """
    for run in ml_runs:                           
        try:
            artifact_uri = run.info.artifact_uri
            # Load the entire parameter dictionary for the run
            all_params_in_run = mlflow.artifacts.load_dict(artifact_uri + "/parameters.json").get("parameters")
            if all_params_in_run is None:
                continue # Skip if the parameters file is missing or empty
            
            # **FIXED LOGIC**: Iterate through each model's parameters inside the file
            for model_key, product_params in all_params_in_run.items():
                if int(product_params.get("product_lineup_sk", -1)) == target_pl_sk:
                    # Found the matching product! 
                    # Return the parent run and the full dictionary of all parameters.
                    return run, all_params_in_run
        
        except Exception as e:
            # This can happen if parameters.json is missing, which we handle as a fallback.
            # We can make the warning less alarming.
            print(f"[INFO] Could not read 'parameters.json' for run {run.info.run_id}. This is expected if using the fallback. Details: {e}")
            continue

    # If the target_pl_sk was not found in any of the models within any of the runs
    return None, None

# COMMAND ----------



# COMMAND ----------

# DBTITLE 1,Helper to Reconstruct Parameters
def construct_parameters_from_config(config):
    """
    Reconstructs the 'parameters.json' dictionary from the main config file.
    This is a fallback for when the artifact is missing from an MLflow run.
    """
    print("--- Reconstructing model parameters from configuration file ---")
    reconstructed_params = {}
    products_config = config.get("products", {})
    
    # Get top-level parameters from the config
    include_macroeconomic = config.get("include_macroeconomic", 0)
    get_by_length_level = config.get("cig_length", 0)
    get_by_price_tier = config.get("price_tier", 0)
    get_by_menthol = config.get("menthol", 0)
    get_extra_num_feats_lgb = config.get("get_extra_num_feats_lgb", [])
    cpu_or_serving = config.get("cpu_or_serving")
    price_threshold = config.get("price_threshold")

    for product_name, product_details in products_config.items():
        product_lineup_sk = product_details.get("id")
        l_product_lineup_sk = product_details.get("other", [])
        
        # This is the crucial step: Call the same function used during training
        cat_feats, num_feats, _, _ = get_features_set(
            product_lineup_sk, 
            l_product_lineup_sk, 
            include_macroeconomic, 
            get_by_length_level, 
            get_by_price_tier, 
            get_by_menthol, 
            get_extra_num_feats_lgb
        )

        # Build the parameter dictionary for this specific product
        prod_params = {
            "product_lineup_sk": str(product_lineup_sk),
            "l_product_lineup_sk": l_product_lineup_sk,
            "cat_feats": cat_feats,
            "num_feats": num_feats,
            "include_macroeconomics": str(include_macroeconomic),
            "gb_cig_length": str(get_by_length_level),
            "gb_price_tier": str(get_by_price_tier),
            "gb_menthol": str(get_by_menthol),
            "cpu_or_serving": cpu_or_serving,
            "price_threshold": str(price_threshold)
            # Add any other parameters from the config that compute_predictions might need
        }
        
        # Use the same keying convention as the training script
        model_key = f"model_{product_name.lower().replace(' ', '').replace('/', '')}"
        reconstructed_params[model_key] = prod_params
        
    print(f"--- Successfully reconstructed parameters for {len(reconstructed_params)} products ---")
    return reconstructed_params

# COMMAND ----------

from pyspark.sql.functions import struct, col

predict_dates.sort() # Ensure sorted: oldest to newest
model = None # Initialize global model variable
get_by_length_level = 0
get_by_price_tier = 0
get_by_menthol = 0
current_cat_feats_for_udf = [] # This is the one causing the SyntaxError
results_national_all = []
results_state_all = []

if model_to_use == "all":
    for product in list(config["products"].keys()):
        print("-------------------------------")
        print("product", product)
        product_mlflow_key = f"model_{product.lower().replace(' ', '').replace('/', '')}"
        product_lineup_sk = config["products"][product]["id"]

        for predict_date in predict_dates:
            print("predict_date", predict_date)

            mlflow_run_name = f"{run_id}{export_suffix}/{predict_date}"
            print(f"Using specific model from MLflow run: {mlflow_run_name}")
            
            # Call our new helper function to have the reconstructed parameters ready
            reconstructed_parameters = construct_parameters_from_config(config)
            
            try:
                experiment_name_to_search = "/Shared/experiments/pe"
                experiments = client.search_experiments(filter_string=f"name='{experiment_name_to_search}'")
                if not experiments: 
                    raise ValueError(f"Experiment '{experiment_name_to_search}' not found.")
                target_experiment_id = experiments[0].experiment_id

                ml_runs = client.search_runs(
                    experiment_ids=[target_experiment_id],
                    filter_string=f"attributes.run_name = '{mlflow_run_name}'",
                    order_by=["start_time DESC"]
                )
                if not ml_runs:
                    raise ValueError(f"No MLflow run found with name '{mlflow_run_name}'")
            except Exception as e:
                print(f"Error fetching MLflow runs: {e}")

            selected_ml_run, parameters_for_single_model = find_mlflow_run_by_pl_sk(ml_runs, int(product_lineup_sk))
            if selected_ml_run is None:
                print(f"WARNING: No MLflow run found for product '{product_key_in_config}'. Skipping.")
                continue

            macroeconomics = int(parameters_for_single_model[product_mlflow_key].get("include_macroeconomics", 0))
            price_threshold_val = int(parameters_for_single_model[product_mlflow_key].get("price_threshold", 0))
            create_training_data_view(
                product_lineup_sk,
                config["products"][product].get("other", []),
                import_data_path,
                macroeconomics,
                price_threshold_val,
                gb_price_tier = int(parameters_for_single_model[product_mlflow_key].get("gb_price_tier", 0)),
                gb_cig_length = int(parameters_for_single_model[product_mlflow_key].get("gb_cig_length", 0)),
                gb_menthol = int(parameters_for_single_model[product_mlflow_key].get("gb_menthol", 0)),
                cpu_or_serving = parameters_for_single_model[product_mlflow_key].get("cpu_or_serving"),
                industry_unique_id = int(parameters_for_single_model[product_mlflow_key].get("price_threshold", 0))
            )
            next_month = str((pd.to_datetime(predict_date) + pd.DateOffset(months=1)).date())
            filter_training_lgb = parameters_for_single_model[product_mlflow_key].get("filter_training_lgb")
            if filter_training_lgb is None:
                filter_training_lgb = """(rsd_quantity_y / rsd_quantity_roll4) <= 1.5
                    and (rsd_quantity_roll4 / rsd_quantity_y) <= 1.5
                    and (rsd_price_perc < 1.5) and (rsd_price_perc > 0.7)
                    and (data_points >= 10)"""

            sql = f"""
            SELECT * 
            FROM train_vw
            WHERE sales_week_forecast>=date('{predict_date}') AND sales_week_forecast<date('{next_month}')
            AND {filter_training_lgb}
            AND account_state IN {tuple(states) if len(states) > 1 else "('%s')" % ", ".join(map(repr, states))}
            """
            print(sql)

            enable_features = parameters_for_single_model[product_mlflow_key]["num_feats"] + parameters_for_single_model[product_mlflow_key]["cat_feats"]
            all_selected_cols = list(set(enable_features + ["product_lineup_sk", "sales_week_forecast", "rsd_quantity_roll4", "account_sk", "rsd_quantity_y"]))
            df = spark.sql(sql).select(*all_selected_cols)
            decimals_cols = [
                x for x in df.columns if ("Decimal" in str(df.schema[x].dataType)) or ("Double" in str(df.schema[x].dataType)) # Fixed OR condition
            ]
            for column_to_cast in decimals_cols: # Renamed variable
                df = df.withColumn(column_to_cast, df[column_to_cast].cast(FloatType()))
            # for cat_col in parameters_for_single_model[product_mlflow_key]["cat_feats"]: 
            #     if cat_col in df.columns:
            #         df[cat_col] = df[cat_col].astype("category")

            # model = mlflow.pyfunc.load_model(f"runs:/{selected_ml_run.info.run_id}/main_model")
            loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=f"runs:/{selected_ml_run.info.run_id}/main_model")

            results = (
                df
                .filter( (F.col("rsd_quantity_roll4").isNotNull()) & (F.col("rsd_quantity_roll4") > 0) )
                .withColumn("predictions", loaded_model(struct(*map(col, df.columns))))
                .withColumn("predicted_volume", F.col("predictions") * F.col("rsd_quantity_roll4"))
                # rsd_quantity_y
            )
            results.cache()

            safe_actual_volume_for_mape = F.when(F.col("rsd_quantity_y").isNotNull() & (F.col("rsd_quantity_y") != 0), F.col("rsd_quantity_y")).otherwise(F.lit(None).cast(FloatType()))
            safe_sum_actual_volume = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
            results_summarized_state = (
                results.groupBy("product_lineup_sk", "sales_week_forecast", "account_state")
                .agg(
                    F.sum(F.abs(F.col("rsd_quantity_y") - F.col("predicted_volume"))).alias("sum_absolute_error"),
                    F.sum("rsd_quantity_y").alias("sum_actual_volume"),
                    F.sum("predicted_volume").alias("sum_predicted_volume"),
                    F.countDistinct("account_sk").alias("nb_account"),
                    F.mean(F.abs(F.col("rsd_quantity_y") - F.col("predicted_volume")) / safe_actual_volume_for_mape).alias("mape")
                )
                .withColumn("w_accuracy", 100 * (1 - F.col("sum_absolute_error") / safe_sum_actual_volume))
                .withColumn("w_accuracy_mape", 100 * (1 - F.col("mape")))
                .withColumn("w_accuracy_week_product_state", 100 * (1 - F.abs(F.col("sum_actual_volume") - F.col("sum_predicted_volume")) / safe_sum_actual_volume))
            )
            
            safe_total_actual_volume_national = F.when(F.col("sum_actual_volume").isNotNull() & (F.col("sum_actual_volume") != 0), F.col("sum_actual_volume")).otherwise(F.lit(None).cast(FloatType()))
            results_summarized_national = (
                results_summarized_state.groupBy("product_lineup_sk")
                .agg(
                    F.sum("sum_absolute_error").alias("total_sum_absolute_error"), # Alias to avoid conflict
                    F.sum("sum_actual_volume").alias("total_sum_actual_volume"),   # Alias to avoid conflict
                    F.sum("nb_account").alias("nb_account"),
                    F.mean("w_accuracy_mape").alias("avg_w_accuracy_mape"), # Alias
                    F.mean("w_accuracy_week_product_state").alias("avg_w_accuracy_week_product_state") #Alias
                )
                .withColumn( # Recalculate w_accuracy at national level
                    "w_accuracy",
                    100 * (1 - F.col("total_sum_absolute_error") / F.when(F.col("total_sum_actual_volume")!=0,F.col("total_sum_actual_volume")).otherwise(None) )
                )
            )
            results_summarized_state = results_summarized_state.toPandas()
            results_summarized_national = results_summarized_national.toPandas()
            results_summarized_state["model_date"], results_summarized_national["model_date"] = predict_date, predict_date
            results_summarized_state["predict_start"], results_summarized_national["predict_start"] = predict_date, predict_date
            results_summarized_state["predict_end"], results_summarized_national["predict_end"] = next_month, next_month
            results_summarized_state["prod_name"], results_summarized_national["prod_name"] = product, product
            results_state_all.append(results_summarized_state)
            results_national_all.append(results_summarized_national)
        print('------------------------')
        print()
    if results_national_all:
        results_national_pd = pd.concat(results_national_all)
        display(results_national_pd)
    else:
        results_national_pd = pd.DataFrame()
    
    results_national_pd_processed = (
        results_national_pd
            .rename(columns={"predict_start": "campaign_start_date", "predict_end": "campaign_end_date"}).groupby(["product_lineup_sk", "prod_name"]).agg({
                "campaign_start_date": "min",
                "campaign_end_date": "max",
                "total_sum_absolute_error": "sum",
                "total_sum_actual_volume": "sum",
                "avg_w_accuracy_mape": "mean",
                "avg_w_accuracy_week_product_state": "mean"
            })
    )
    results_national_pd_processed['w_accuracy'] = results_national_pd_processed.apply(
            lambda x: 100 * (1 - x.total_sum_absolute_error / x.total_sum_actual_volume) if x.total_sum_actual_volume and x.total_sum_actual_volume != 0 else None,
            axis=1
        )
    display(results_national_pd_processed.reset_index())
    accuracy_base_path = f"{folder}{run_id}{export_suffix}/other_files/accuracy_rsd/" 
    df_accuracy_spark = spark.createDataFrame(results_national_pd_processed.reset_index())

    # delta folder per product_lineup_sk
    product_keys = (df_accuracy_spark
                    .select("prod_name")
                    .distinct()
                    .collect())

    for row in product_keys:
        prod_name_val = row["prod_name"]
        folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

        per_prod_df = df_accuracy_spark.filter(F.col("prod_name") == F.lit(prod_name_val))
        per_product_path = f"{accuracy_base_path}{folder_name}"
        (per_prod_df
        .write.format("delta")
        .mode("overwrite")
        .option("overwriteSchema", "true")
        .save(per_product_path))

    print(f"Per-product accuracy written under: {accuracy_base_path}")

# COMMAND ----------

if model_to_use == "all":
    dbutils.notebook.exit(json.dumps({"success": True}))

# COMMAND ----------

if model_to_use != "all":

    predict_dates.sort() # Ensure sorted: oldest to newest
    model = None # Initialize global model variable
    get_by_length_level = 0
    get_by_price_tier = 0
    get_by_menthol = 0
    current_cat_feats_for_udf = [] # This is the one causing the SyntaxError

    mlflow_run_name = f"{run_id}{export_suffix}/{model_to_use}"
    print(f"Using specific model from MLflow run: {mlflow_run_name}")
    
    # Call our new helper function to have the reconstructed parameters ready
    reconstructed_parameters = construct_parameters_from_config(config)
    
    try:
        experiment_name_to_search = "/Shared/experiments/pe"
        experiments = client.search_experiments(filter_string=f"name='{experiment_name_to_search}'")
        if not experiments: 
            raise ValueError(f"Experiment '{experiment_name_to_search}' not found.")
        target_experiment_id = experiments[0].experiment_id

        ml_runs = client.search_runs(
            experiment_ids=[target_experiment_id],
            filter_string=f"attributes.run_name = '{mlflow_run_name}'",
            order_by=["start_time DESC"]
        )
        if not ml_runs:
            raise ValueError(f"No MLflow run found with name '{mlflow_run_name}'")

    except Exception as e:
        print(f"Error fetching MLflow runs: {e}")
        results_state_all    = []
        results_national_all = []
        results_national_pd  = pd.DataFrame()

    else:
        results_state_all = []
        results_national_all = []

        for product_key_in_config, product_id in zip(product_names, product_id_list):

            # This function will still get the MLflow run, but 'parameters_for_single_model' might be None
            selected_ml_run, parameters_for_single_model = find_mlflow_run_by_pl_sk(ml_runs, int(product_id))

            if selected_ml_run is None:
                print(f"WARNING: No MLflow run found for product '{product_key_in_config}'. Skipping.")
                continue

            # ** THIS IS THE KEY CHANGE **
            # If parameters were not found in the artifact, use our reconstructed ones.
            if parameters_for_single_model is None:
                print(f"WARNING: 'parameters.json' not found in MLflow run for '{product_key_in_config}'. Using reconstructed parameters from config file.")
                parameters_for_single_model = reconstructed_parameters
            
            globals()['model'] = mlflow.pyfunc.load_model(f"runs:/{selected_ml_run.info.run_id}/main_model")

            # Now, 'parameters_for_single_model' is guaranteed to be populated (if the product is in the config)
            if parameters_for_single_model:
                for date_to_predict_on in predict_dates:
                    print(f"--- Predicting for date: {date_to_predict_on} using model from {model_to_use} ---")
                    print(f"Processing product: {product_key_in_config} for prediction date {date_to_predict_on}")
                    product_mlflow_key = f"model_{product_key_in_config.lower().replace(' ', '').replace('/', '')}"

                    if product_mlflow_key not in parameters_for_single_model:
                        print(f"Warning: Parameters for '{product_mlflow_key}' not found in reconstructed params. Skipping product.")
                        continue
                    
                    prod_specific_mlflow_params = parameters_for_single_model.get(product_mlflow_key)

                    if product_key_in_config not in products_config:
                        print(f"Warning: Product key '{product_key_in_config}' not found in main products_config. Skipping.")
                        continue
                    main_prod_config_details = products_config[product_key_in_config]
                    p_l_sk = int(main_prod_config_details.get("id"))
                    l_p_l_sk = main_prod_config_details.get("other", [])

                    globals()['current_cat_feats_for_udf'] = prod_specific_mlflow_params.get("cat_feats", [])
                    result_state_df, result_national_df = compute_predictions(
                        product_name_key=product_key_in_config,
                        product_lineup_sk=p_l_sk,
                        l_product_lineup_sk=l_p_l_sk,
                        start_date=date_to_predict_on,
                        end_date=date_to_predict_on,
                        current_model_params=prod_specific_mlflow_params
                    )
                    results_state_all.append(result_state_df)
                    results_national_all.append(result_national_df)
                if results_national_all:
                    results_national_pd = pd.concat(results_national_all)
                else:
                    results_national_pd = pd.DataFrame()
            else:
                print(f"Skipping predictions as parameters for model {mlflow_run_name} could not be loaded or reconstructed.")
                results_national_pd = pd.DataFrame()

# COMMAND ----------

if not products_config: # Check if products_config is empty or None
    print("Warning: products_config is empty. Cannot create names_df for merging.")
    names_df = pd.DataFrame(columns=["product_lineup_sk", "prod_name"])
else:
    names = []
    ids = []
    for name, content in products_config.items(): # Use products_config
        names.append(name) # This is the product key like "VUSE ALTO POD MEN 4CART"
        ids.append(content.get('id')) # This is the product_lineup_sk
    names_df = pd.DataFrame({ "product_lineup_sk": ids, "prod_name": names})

# COMMAND ----------

if 'results_national_pd' not in locals() or results_national_pd.empty:
    print("results_national_pd is not defined or empty. Skipping merge and display of national results.")
    results_national_pd_merged_display = pd.DataFrame() # Ensure it exists as an empty DF
else:
    # Ensure product_lineup_sk types match for merging
    results_national_pd['product_lineup_sk'] = results_national_pd['product_lineup_sk'].astype(int)
    if not names_df.empty: # names_df is defined in a previous cell
        names_df['product_lineup_sk'] = names_df['product_lineup_sk'].astype(int)
        results_national_pd_merged_display = results_national_pd.merge(names_df, on='product_lineup_sk', how='left').sort_values(['prod_name', 'product_lineup_sk'])
    else:
        results_national_pd_merged_display = results_national_pd.sort_values(['product_lineup_sk']).copy() # Sort by available key
        results_national_pd_merged_display['prod_name'] = 'Unknown (names_df empty)'

    if 'predict_dates' in locals() and predict_dates:
        datetime_predict_dates = pd.to_datetime(predict_dates)
        
        campaign_start_date = datetime_predict_dates.min()
        
        # The end date is the end of the month of the latest start date in predict_dates
        latest_month_start_in_campaign = datetime_predict_dates.max()
        campaign_end_date = latest_month_start_in_campaign + pd.offsets.MonthEnd(0)
        
        results_national_pd_merged_display['campaign_start_date'] = campaign_start_date.strftime('%Y-%m-%d')
        results_national_pd_merged_display['campaign_end_date'] = campaign_end_date.strftime('%Y-%m-%d')
        
        # Reorder columns to have dates appear earlier if desired
        cols = results_national_pd_merged_display.columns.tolist()
        if 'prod_name' in cols and 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            try:
                prod_name_idx = cols.index('prod_name')
                # Remove them
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                # Insert them after prod_name
                cols.insert(prod_name_idx + 1, 'campaign_end_date')
                cols.insert(prod_name_idx + 1, 'campaign_start_date')
                results_national_pd_merged_display = results_national_pd_merged_display[cols]
            except ValueError:
                # prod_name not found, just keep original order with new columns at the end
                pass
        elif 'product_lineup_sk' in cols and 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            # Fallback: place after product_lineup_sk if prod_name is not there
            try:
                pl_sk_idx = cols.index('product_lineup_sk')
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                cols.insert(pl_sk_idx + 1, 'campaign_end_date')
                cols.insert(pl_sk_idx + 1, 'campaign_start_date')
                results_national_pd_merged_display = results_national_pd_merged_display[cols]
            except ValueError:
                 pass 

    else:
        print("Warning: 'predict_dates' variable not found or empty. Cannot add campaign date columns to results_national_pd_merged_display.")
        results_national_pd_merged_display['campaign_start_date'] = None
        results_national_pd_merged_display['campaign_end_date'] = None

    display(results_national_pd_merged_display)

# COMMAND ----------

if 'results_national_pd_merged_display' in locals() and not results_national_pd_merged_display.empty:
    if 'prod_name' not in results_national_pd_merged_display.columns:
        print("Warning: 'prod_name' column missing in results_national_pd_merged_display. Adding placeholder.")
        results_national_pd_merged_display['prod_name'] = 'Unknown'
    
    aggregation_dict = {
        "total_sum_absolute_error": "sum",  # Use the aliased name from national results
        "total_sum_actual_volume": "sum",   # Use the aliased name from national results
        "avg_w_accuracy_mape": "mean",
        "avg_w_accuracy_week_product_state": "mean"
    }
    print(f"Aggregating with dict: {aggregation_dict}")
    print(f"Columns available in results_national_pd_merged_display: {results_national_pd_merged_display.columns.tolist()}")

    global_result = (
        results_national_pd_merged_display.groupby(["product_lineup_sk", "prod_name"])
        .agg(aggregation_dict) # Use the corrected dictionary
        .reset_index()
        .sort_values('prod_name')
    )
    global_result['w_accuracy'] = global_result.apply(
        lambda x: 100 * (1 - x.total_sum_absolute_error / x.total_sum_actual_volume) if x.total_sum_actual_volume and x.total_sum_actual_volume != 0 else None,
        axis=1
    )

    if 'predict_dates' in locals() and predict_dates:
        # Convert to datetime objects if they are strings
        datetime_predict_dates = pd.to_datetime(predict_dates)
        
        campaign_start_date = datetime_predict_dates.min()
        
        # The end date is the end of the month of the latest start date in predict_dates
        latest_month_start_in_campaign = datetime_predict_dates.max()
        campaign_end_date = latest_month_start_in_campaign + pd.offsets.MonthEnd(0)
        
        global_result['campaign_start_date'] = campaign_start_date.strftime('%Y-%m-%d')
        global_result['campaign_end_date'] = campaign_end_date.strftime('%Y-%m-%d')
        
        # Reorder columns to have dates appear earlier if desired
        cols = global_result.columns.tolist()
        # Example: move campaign dates after prod_name
        if 'campaign_start_date' in cols and 'campaign_end_date' in cols:
            # Find index of prod_name
            try:
                prod_name_idx = cols.index('prod_name')
                # Remove them
                cols.pop(cols.index('campaign_start_date'))
                cols.pop(cols.index('campaign_end_date'))
                # Insert them after prod_name
                cols.insert(prod_name_idx + 1, 'campaign_end_date')
                cols.insert(prod_name_idx + 1, 'campaign_start_date')
                global_result = global_result[cols]
            except ValueError:
                # prod_name not found, just keep original order with new columns at the end
                pass

    else:
        print("Warning: 'predict_dates' variable not found or empty. Cannot add campaign date columns.")
        global_result['campaign_start_date'] = None
        global_result['campaign_end_date'] = None
    # ---- END ADDED SECTION ----

    display(global_result)
else:
    print("No national results available to compute global_result.")
    global_result = pd.DataFrame()

if 'global_result' in locals() and not global_result.empty:
    print("Final Global Result (already displayed if computed):")
else:
    print("Global result is empty.")

# COMMAND ----------

if 'global_result' in locals() and not global_result.empty:
    accuracy_output_path = f"{folder}{run_id}/other_files/accuracy_rsd/"
    print(f"Writing final accuracy results to Delta table: {accuracy_output_path}")
    df_accuracy_spark = spark.createDataFrame(global_result)
    df_accuracy_spark.write.format("delta").mode('overwrite').option("overwriteSchema", "true").save(accuracy_output_path)
    print("Successfully wrote accuracy results.")
else:
    print("Skipping write operation because 'global_result' DataFrame is not available or is empty.")

</file>

<file path="notebooks/other/drift.py">
# Databricks notebook source
# MAGIC %md
# MAGIC # Drift Notebook
# MAGIC
# MAGIC Compute the pe drift and the data drift: number of states that drifted over the last 3 months per product.

# COMMAND ----------

import json
# dbutils.widgets.removeAll()
dbutils.widgets.text("folder", "s3://rai-eap-rgm-qa-us-east-1/insiteai/pe_model_dev_test/databricks/")
dbutils.widgets.text("run_id", "2023/julie/reexecute_allprods/alloff") # Example run_id
dbutils.widgets.text("product_list", '[6]')
dbutils.widgets.text("industry_unique_id", "1")  # Example run_id


folder = dbutils.widgets.get("folder").strip() 
run_id = dbutils.widgets.get("run_id").strip()
industry_unique_id = dbutils.widgets.get("industry_unique_id")
product_list = json.loads(dbutils.widgets.get("product_list"))
dbutils.widgets.text("delta", "1") 
delta_input = int(dbutils.widgets.get("delta"))
dbutils.widgets.text("export_suffix", "") 
export_suffix = dbutils.widgets.get("export_suffix")

print(f"folder: {folder}")
print(f"run_id: {run_id}")
print(f"industry_unique_id: {industry_unique_id}")
print(f"product_list: {product_list}")

# COMMAND ----------

import plotly.express as px
import numpy as np
import pandas as pd
from pandas.tseries.offsets import MonthBegin
pd.options.plotting.backend = "plotly"

BAT_COLORS = [
    '#0e2b63',  
    '#004f9f',  
    '#00b1eb',  
    '#ef7d00',  
    '#ffbb00',  
    '#50af47',  
    '#afca0b',  
    '#5a328a',  
    '#e72582',  
]

# PE data
import plotly.express as px
import numpy as np
import pandas as pd
from pandas.tseries.offsets import MonthBegin
pd.options.plotting.backend = "plotly"

BAT_COLORS = [
    '#0e2b63',  
    '#004f9f',  
    '#00b1eb',  
    '#ef7d00',  
    '#ffbb00',  
    '#50af47',  
    '#afca0b',  
    '#5a328a',  
    '#e72582',  
]

# --- START OF MODIFIED "PE data" SECTION ---
# PE data
# pe_path = f"{folder}/{run_id}/*/computed_pe/" # This general path is less useful for specific iteration
base_run_path = f"{folder.rstrip('/')}/{run_id.rstrip('/')}/"

if delta_input == 1:
    print(f"DELTA MODE: Reading PE data from Delta sources under {base_run_path}<date_folder>/computed_pe{export_suffix}/")
    all_pe_spark_dfs_collected = [] 
    date_folders_found = []
    
    try:
        for item in dbutils.fs.ls(base_run_path):
            if item.isDir() and item.name[:-1].count('-') == 2 and len(item.name[:-1]) == 10:
                date_folders_found.append(item.path)
        
        if not date_folders_found:
            raise ValueError(f"No date-like subdirectories found under {base_run_path}")
        print(f"Found date folders for PE data: {date_folders_found}")

        for date_folder_path in date_folders_found:
            # Include export_suffix in the path
            pe_path_target = f"{date_folder_path.rstrip('/')}/computed_pe{export_suffix}/"
            print(f"  Processing date folder: {date_folder_path}, looking in {pe_path_target}")
            
            delta_tables_to_read_for_current_date = []
            try:
                for file_info in dbutils.fs.ls(pe_path_target):
                    if file_info.isDir() and file_info.name.endswith('_delta/'):
                        delta_tables_to_read_for_current_date.append(file_info.path)
                
                if not delta_tables_to_read_for_current_date:
                    print(f"    No '*_delta/' subdirectories found in {pe_path_target}. Skipping.")
                    continue

                df_list_for_current_date = []
                for p_delta_path in delta_tables_to_read_for_current_date:
                    print(f"      Reading Delta table: {p_delta_path}")
                    try:
                        df_list_for_current_date.append(spark.read.format("delta").load(p_delta_path))
                    except Exception as e_read_specific_delta:
                        print(f"      WARNING: Failed to read Delta table {p_delta_path}. Error: {e_read_specific_delta}")
                
                if df_list_for_current_date:
                    if len(df_list_for_current_date) == 1:
                        all_pe_spark_dfs_collected.append(df_list_for_current_date[0])
                    else:
                        from functools import reduce
                        from pyspark.sql import DataFrame
                        unioned_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), df_list_for_current_date)
                        all_pe_spark_dfs_collected.append(unioned_df)
            except Exception as e_ls_pe_target:
                print(f"    WARNING: Could not process {pe_path_target}. Error: {e_ls_pe_target}")

        if not all_pe_spark_dfs_collected:
            raise ValueError(f"No PE Delta tables successfully read from any date folder under {base_run_path}")

        if len(all_pe_spark_dfs_collected) == 1:
            final_pe_spark_df = all_pe_spark_dfs_collected[0]
        else:
            from functools import reduce
            from pyspark.sql import DataFrame
            print(f"Unioning {len(all_pe_spark_dfs_collected)} collected PE Spark DataFrames.")
            final_pe_spark_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), all_pe_spark_dfs_collected)
        
        final_pe_spark_df.createOrReplaceTempView("pe")
        print("Successfully created 'pe' view from combined Delta sources.")

    except Exception as e:
        raise Exception(f"Critical error during Delta PE data loading process. Error: {e}")

else: # delta_input == 0 (Parquet)
    # Include export_suffix in the Parquet path pattern
    pe_read_path = f"{base_run_path}*/computed_pe{export_suffix}/*.parquet"
    print(f"PARQUET MODE: Reading PE data from Parquet path pattern: {pe_read_path}")
    try:
        spark.read.parquet(pe_read_path).createOrReplaceTempView("pe")
        print("Successfully created 'pe' view from Parquet sources.")
    except Exception as e2:
         raise Exception(f"Failed to read PE data from Parquet path {pe_read_path}. Error: {e2}")
        
df_pe_check = spark.sql("SELECT 1 FROM pe LIMIT 1")
if df_pe_check.count() == 0:
    raise ValueError(f"No data loaded into 'pe' view from path: {base_run_path} (delta={delta_input})")

products = ", ".join(map(str, product_list))
df_pe = spark.sql(f"""select product_lineup_sk, simulated_price_product_lineup_sk, account_state, account_sk,
                  rsd_quantity, pe, predict_date, int(datediff(predict_date, max_week)/7) weeks
                  from pe
                  where product_lineup_sk in ({products})
                  """).toPandas()
if df_pe.empty:
    raise ValueError(f"Pandas DataFrame df_pe is empty after reading from 'pe' view (delta={delta_input}). Check source data.")
unique_predict_dates = sorted(df_pe.predict_date.unique())
if not unique_predict_dates:
    raise ValueError("No prediction dates found in the input data for drift analysis.")
elif len(unique_predict_dates) >= 4:
    start_date_drift = unique_predict_dates[-4]
else:
    start_date_drift = unique_predict_dates[0]
print(f"Using start_date_drift: {start_date_drift}")
data_s3_base_path = f"{base_run_path}data/"
        
df_pe_check = spark.sql("SELECT 1 FROM pe LIMIT 1")
if df_pe_check.count() == 0:
    raise ValueError(f"No data loaded into 'pe' view from path: {pe_path} (delta={delta_input})")

products = ", ".join(map(str, product_list))
df_pe = spark.sql(f"""select product_lineup_sk, simulated_price_product_lineup_sk, account_state, account_sk,
                  rsd_quantity, pe, predict_date, int(datediff(predict_date, max_week)/7) weeks
                  from pe
                  where product_lineup_sk in ({products})
                  """).toPandas()
if df_pe.empty:
    raise ValueError(f"Pandas DataFrame df_pe is empty after reading from 'pe' view (delta={delta_input}). Check source data.")
unique_predict_dates = sorted(df_pe.predict_date.unique())
if not unique_predict_dates:
    raise ValueError("No prediction dates found in the input data for drift analysis.")
elif len(unique_predict_dates) >= 4:
    start_date_drift = unique_predict_dates[-4]
else:
    start_date_drift = unique_predict_dates[0]
print(f"Using start_date_drift: {start_date_drift}")
data_s3_base_path = f"{base_run_path}data/" # Common base for product/industry data

if delta_input == 1:
    print(f"DELTA MODE: Reading product/industry data from Delta sources in {data_s3_base_path}delta/")
    df_product_delta_path_primary = f"{data_s3_base_path}delta/df_product"
    df_product_delta_path_alt = f"{data_s3_base_path}delta/df_product_delta"
    df_industry_delta_path_primary = f"{data_s3_base_path}delta/df_industry"
    df_industry_delta_path_alt = f"{data_s3_base_path}delta/df_industry_delta"

    try:
        print(f"  Reading product data from: {df_product_delta_path_primary}")
        spark.read.format("delta").load(df_product_delta_path_primary).filter(f"product_lineup_sk in ({products})").createOrReplaceTempView("df_product")
    except Exception:
        print(f"  Failed. Reading product data from alternative: {df_product_delta_path_alt}")
        spark.read.format("delta").load(df_product_delta_path_alt).createOrReplaceTempView("df_product")
    print(f"Successfully created 'df_product' view from Delta.")
    
    try:
        print(f"  Reading industry data from: {df_industry_delta_path_primary}")
        spark.read.format("delta").load(df_industry_delta_path_primary).filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("df_industry")
    except Exception:
        print(f"  Failed. Reading industry data from alternative: {df_industry_delta_path_alt}")
        spark.read.format("delta").load(df_industry_delta_path_alt).createOrReplaceTempView("df_industry")
    print(f"Successfully created 'df_industry' view from Delta.")

else: # delta_input == 0 (Parquet)
    df_product_parquet_path = f"{folder}/{run_id}/data/df_product.parquet"
    df_industry_parquet_path = f"{folder}/{run_id}/data/df_industry.parquet"

    print(f"PARQUET MODE: Reading product data from: {df_product_parquet_path}")
    spark.read.parquet(df_product_parquet_path).createOrReplaceTempView("df_product")
    print(f"PARQUET MODE: Reading industry data from: {df_industry_parquet_path}")
    spark.read.parquet(df_industry_parquet_path).createOrReplaceTempView("df_industry")
    print("Successfully created 'df_product' and 'df_industry' views from Parquet.")
unique_pl_sk_for_query = [str(e) for e in df_pe.product_lineup_sk.unique()]
if not unique_pl_sk_for_query:
    print("WARNING: No unique product_lineup_sk found in PE data. df_raw will be empty.")
    df_raw_schema = "account_state STRING, product_lineup_sk BIGINT, sales_week_start_date DATE, rsd_quantity DOUBLE, rsd_price DOUBLE, rsd_quantity_industry DOUBLE, rsd_price_industry DOUBLE"
    df_raw = spark.createDataFrame([], df_raw_schema).toPandas()
else:
    sql = f"""select account_state, df_product.product_lineup_sk, sales_week_start_date
    , sum(rsd_quantity) rsd_quantity
    , sum(rsd_price*rsd_quantity)/sum(rsd_quantity) rsd_price
    , sum(rsd_quantity_industry) rsd_quantity_industry
    , sum(rsd_price_industry*rsd_quantity_industry)/sum(rsd_quantity_industry) rsd_price_industry
    from df_product
    left join df_industry using (account_sk, sales_week_sk)
    where rsd_price>1 and rsd_quantity>0 and rsd_price_industry>1 and rsd_quantity_industry>0
    and df_product.product_lineup_sk in ({", ".join(unique_pl_sk_for_query)})
    group by account_state, df_product.product_lineup_sk, sales_week_start_date
    """
    df_raw = spark.sql(sql).toPandas()

l_c = ["rsd_quantity", "rsd_price", "rsd_quantity_industry", "rsd_price_industry"]
# Ensure columns exist before astype conversion, handle if df_raw is empty
for col_name in l_c:
    if col_name in df_raw.columns:
        df_raw[col_name] = df_raw[col_name].astype(float)
    else:
        if not df_raw.empty: 
             print(f"Warning: Column '{col_name}' not found in df_raw. Skipping astype conversion for it.")

df_name = spark.sql("select distinct product_lineup_sk, kpl_preferred_name from df_product").toPandas()

# COMMAND ----------

# DBTITLE 1,Function (same as pe-enhancement repo)
class Drift():
    
    def __init__(self, start_date_drift="2021-01-01"):
        self.start_date_drift = start_date_drift
                
    def _drift_calculator(self, df, feature):

        # Build rolling intervals to detect significant deviations
        if feature=="pe__":
            df[f"{feature} - sd"] = df["pe_roll"] - df[f"sd_{feature}_interval"] # current interval
            df[f"{feature} + sd"] = df["pe_roll"] + df[f"sd_{feature}_interval"] # current interval
        else:
            df[f"{feature} - sd"] = df[f"{feature}"] - df[f"sd_{feature}_interval"] # current interval
            df[f"{feature} + sd"] = df[f"{feature}"] + df[f"sd_{feature}_interval"] # current interval
        df.loc[:, f"{feature}_min_interval"] = np.nan # will be equal to previous intervals if possible
        df.loc[:, f"{feature}_max_interval"] = np.nan # will be equal to previous intervals if possible
        df.loc[:, f"{feature}_drift"] = False # indicates if there is a drift or not

        l_drift_dates = [
            df.loc[(~df[f"sd_{feature}_interval"].isna()) & (df.predict_date >= self.start_date_drift), "predict_date"].min()
        ]
        
        while(len(l_drift_dates)>0):    
            drift_date = l_drift_dates[0] # take the first date where the value is outside the range

            df.loc[df.predict_date > drift_date, [f"{feature}_min_interval", f"{feature}_max_interval"]] = (
                df.loc[df.predict_date == drift_date, [f"{feature} + sd",f"{feature} - sd"]].values
            )
            df.loc[df.predict_date == drift_date, f"{feature}_drift"] = True

            loc = (
                (df["predict_date"] > drift_date)
                & ( (df[feature] > df[f"{feature}_min_interval"]) | (df[feature] < df[f"{feature}_max_interval"]) )
            )
            l_drift_dates = df.loc[loc].predict_date.values

        return df
    
    @staticmethod
    def _lambda_drift_to_month(x):
        if x == 0:
            return 12 
        elif x > 4 / 12:
            return 1
        elif x > 2 / 12:
            return 3
        else:
            return 6

    def compute_drift(self, df, gb, feature):
        
        def process_feature(df):
            return self._drift_calculator(df, feature)
        # df = pd.concat(applyParallel(df.groupby(gb), process_feature, 1))
        df = df.groupby(gb).apply(process_feature)

        return df, df[df.predict_date>self.start_date_drift].groupby(gb).apply(lambda x: pd.Series({
            f"{feature}_drift": self._lambda_drift_to_month(x[f"{feature}_drift"].mean()),
            f"{feature}": x[f"{feature}"].mean(),
            f"{feature}_avg_interval": np.abs(
                x[f"{feature}_max_interval"].mean()-x[f"{feature}_min_interval"].mean()
            ),
        }))

def pe_drift(pe, product_lineup_sk, account_state, gb, display_data=True, constant_sd=2, start_date_drift="2021-01-01"):
    print(f"Selection: {pe}, {product_lineup_sk}, {account_state}, {gb}")

    if pe == "pe":
        df_tmp = df_pe[(df_pe.product_lineup_sk == df_pe.simulated_price_product_lineup_sk)] # select the pe numbers
    else:
        df_tmp = df_pe[(df_pe.product_lineup_sk != df_pe.simulated_price_product_lineup_sk)] # select the cross-pe numbers

    # filter the pe by widget selection
    if product_lineup_sk != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.product_lineup_sk == product_lineup_sk), :]
    if account_state != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.account_state == account_state), :]
    #if account_county != "ALL": df_tmp = df_tmp[(df_tmp.account_county==account_county)]

    df_tmp = (
        df_tmp
        .query("rsd_quantity>=5")
        .query("weeks<=25")
        .groupby(list(gb)+["predict_date"]).apply(lambda x: pd.Series({
            "pe": (x.pe * x.rsd_quantity).sum() / x.rsd_quantity.sum(),
        }))
        .reset_index()
    )
    df_tmp["pe_roll"] = df_tmp.groupby(list(gb))["pe"].rolling(3, min_periods=1).mean().reset_index(drop=True)
    df_tmp.predict_date = df_tmp.predict_date.astype(str)
    df_tmp["sd_pe"] = df_tmp.groupby(list(gb))[["pe"]].transform(lambda x: x.rolling(12, min_periods=3).std())
    df_tmp["sd_pe_interval"] = constant_sd * df_tmp["sd_pe"]
    
    if pe == "pe": # adjust the inerval for pe, not for cross-pe
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe"].apply(lambda x: np.max([x, 0.1])) # do not retrain if the variation of pe is less than 0.5
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe_interval"].apply(lambda x: np.min([x, 0.1])) # retrain if the variation of pe is more than 0.1
    else:
        df_tmp["sd_pe_interval"] = df_tmp["sd_pe"].apply(lambda x: np.max([x, 0.01]))

    drift = Drift(start_date_drift) # we are interested by the pe drift after start_date_drift
    df_tmp, df_res = drift.compute_drift(df_tmp, list(gb), "pe")
    
    if display_data:
        display(df_res.reset_index())
    
    return df_res, df_tmp

def data_drift(product_lineup_sk, account_state, gb, constant_sd=2, start_date_drift="2020-01-01"):
    
    df_tmp = df_raw.copy()[df_raw.sales_week_start_date >= pd.to_datetime(start_date_drift)]
    
    if product_lineup_sk != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.product_lineup_sk == product_lineup_sk), :]
    if account_state != "ALL":
        df_tmp = df_tmp.loc[(df_tmp.account_state == account_state), :]
    
    df_tmp = (
        df_tmp.groupby(list(gb)+["sales_week_start_date"]).apply(lambda x: pd.Series({
            "rsd_quantity": x.rsd_quantity.sum(),
            "rsd_price": (x.rsd_price*x.rsd_quantity).sum() / x.rsd_quantity.sum(),
            "rsd_quantity_industry": x.rsd_quantity_industry.sum(),
            "rsd_price_industry": (x.rsd_price_industry*x.rsd_quantity_industry).sum() / x.rsd_quantity_industry.sum(),
        }))
        .reset_index()
    )
    df_tmp["rsd_price_perc"] = df_tmp["rsd_price"] / df_tmp["rsd_price_industry"]
    df_tmp["som"] = 100 * (df_tmp["rsd_quantity"] / df_tmp["rsd_quantity_industry"])
    
    # group data per month
    df_tmp["sales_week_start_date"] = pd.DatetimeIndex(df_tmp["sales_week_start_date"])
    df_tmp = df_tmp.set_index(["sales_week_start_date"]).groupby(list(gb)).resample('M')[["rsd_quantity", "rsd_price",
       "rsd_quantity_industry", "rsd_price_industry", "rsd_price_perc", "som"]].mean()
    df_tmp.reset_index(inplace=True)
    df_tmp["predict_date"] = df_tmp["sales_week_start_date"] + MonthBegin(1)
    
    drift = Drift(start_date_drift) # we are interested by the drift after start_date_drift
    
    print("start drift computation")
    l_results = []
    # We want the drift for each of the fetaures in the list
    for feature in ["rsd_quantity","rsd_price","rsd_quantity_industry","rsd_price_industry","rsd_price_perc","som"]:
        df_tmp[f"sd_{feature}"] = df_tmp.groupby(list(gb))[feature].transform(lambda x: x.rolling(12, min_periods=3).std())
        df_tmp[f"sd_{feature}_interval"] = constant_sd*df_tmp[f"sd_{feature}"]
        if feature=="rsd_price_perc":
            df_tmp[f"sd_{feature}_interval"] = 0.1
        elif feature=="rsd_price":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 5% of price variation
        elif feature=="rsd_price_industry":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 5% of price variation
        elif feature=="som":
            df_tmp[f"sd_{feature}_interval"] = 0.1 * df_tmp[f"{feature}"] # allow 10% of som variation
        df_tmp, df_res = drift.compute_drift(df_tmp, list(gb), feature)
        l_results.append(df_res)
    print("end drift computation")
                
    return df_tmp, pd.concat(l_results, ignore_index=False, axis=1)

# COMMAND ----------

# DBTITLE 1,PE Drift
df_res, df_details = pe_drift("pe", product_lineup_sk="ALL", account_state="ALL", gb=["product_lineup_sk", "account_state"], start_date_drift=start_date_drift, display_data=False)
df_drift = df_details[(df_details.predict_date > start_date_drift) & (df_details.pe_drift == True)][["product_lineup_sk", "account_state", "predict_date", "pe_drift"]]

df_drift_counts = (
    df_drift
    .groupby(["product_lineup_sk", "account_state"])
    .size() # Use size() for counting rows in groups, more efficient than count() on a specific column
    .reset_index(name='count_per_state') # Get product_lineup_sk, account_state, count_per_state
    .query("count_per_state >= 2")
    .groupby("product_lineup_sk")
    .size() # Count how many states meet the criteria per product
    .reset_index(name="pe_drift") # Get product_lineup_sk, pe_drift (count of states)
)

df_all_products = df_details[["product_lineup_sk"]].drop_duplicates()

df_merged_counts = pd.merge(
    df_all_products,
    df_drift_counts,
    on="product_lineup_sk",
    how="left" # Keep all products from df_all_products
)

if 'product_lineup_sk' not in df_name.columns:
    df_name_reset = df_name.reset_index()
else:
    df_name_reset = df_name.copy()


df_tmp = pd.merge(
    df_merged_counts,
    df_name_reset[['product_lineup_sk', 'kpl_preferred_name']], # Select only necessary columns from df_name
    on="product_lineup_sk",
    how="inner" # Keep only products that have names
).fillna({'pe_drift': 0}) # Fill only the count column with 0

if df_drift.empty:
    print("No PE drift details to display (df_drift is empty).")
else:
    display(df_drift)

df_tmp["product_lineup_sk"] = df_tmp["product_lineup_sk"].astype(str)
fig = px.bar(df_tmp, x="kpl_preferred_name", y="pe_drift", color_discrete_sequence=BAT_COLORS, title="Number of instances of pe drift")
fig.update_yaxes(range=[0,55])
fig.show()


# COMMAND ----------

df_pe_drift_state = (
    df_drift
    .groupby(["product_lineup_sk","account_state"])
    .size() # Count occurrences per group
    .reset_index(name='pe_drift_count') # Rename the count column
    .query("pe_drift_count >= 2") # Filter based on the count
)
if df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty).")
else:
    display(df_pe_drift_state)

# COMMAND ----------

if df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty).")
else:
    display(
        df_tmp.loc[:, ['kpl_preferred_name', 'pe_drift']]
        .sort_values('kpl_preferred_name')
    )

# COMMAND ----------

import plotly.express as px
import pandas as pd

if 'df_pe_drift_state' not in locals() or df_pe_drift_state.empty:
    print("No states met the PE drift criteria (df_pe_drift_state is empty or not defined). Thus, no PE plot for drifted states will be generated.")
else:
    print(f"Plotting PE for {len(df_pe_drift_state)} drifted product-state combinations.")
    
    # Select only the product_lineup_sk and account_state from df_pe_drift_state
    drifted_states_identifiers = df_pe_drift_state[['product_lineup_sk', 'account_state']].drop_duplicates()
    
    # Filter df_details to get PE time series for these drifted states
    # df_details contains the 'pe' column and 'predict_date' needed for plotting
    df_plot_data = pd.merge(
        df_details, 
        drifted_states_identifiers, 
        on=['product_lineup_sk', 'account_state'], 
        how='inner'
    )
    
    if df_plot_data.empty:
        print("No PE data found in df_details for the identified drifted states.")
    else:
        # Merge with df_name to get kpl_preferred_name for richer labels
        # Ensure df_name has the required columns; df_name was created earlier in the notebook
        if 'product_lineup_sk' not in df_name.columns:
             # If product_lineup_sk is an index
            df_name_to_merge = df_name.reset_index()
        else:
            df_name_to_merge = df_name.copy()

        df_plot_data = pd.merge(
            df_plot_data,
            df_name_to_merge[['product_lineup_sk', 'kpl_preferred_name']],
            on='product_lineup_sk',
            how='left'
        )
        
        # Create a comprehensive legend label
        # Fill missing kpl_preferred_name with product_lineup_sk for robustness
        df_plot_data['kpl_preferred_name_filled'] = df_plot_data['kpl_preferred_name'].fillna(
            'SK: ' + df_plot_data['product_lineup_sk'].astype(str)
        )
        df_plot_data['legend_label'] = df_plot_data['kpl_preferred_name_filled'] + ' - ' + df_plot_data['account_state']
        
        # Ensure predict_date is in datetime format for plotting
        df_plot_data['predict_date'] = pd.to_datetime(df_plot_data['predict_date'])
        
        # Sort data for consistent line plotting
        df_plot_data = df_plot_data.sort_values(by=['legend_label', 'predict_date'])
        
        # Generate the plot
        fig_drifted_pe = px.line(
            df_plot_data,
            x='predict_date',
            y='pe',  # Plotting the 'pe' column as requested
            color='legend_label',
            markers=True,
            title='PE Over Time for States with Significant Drift (pe_drift_count >= 2)'
        )
        
        fig_drifted_pe.update_layout(
            xaxis_title='Prediction Date',
            yaxis_title='Price Elasticity (pe)',
            legend_title='Product - State'
        )
        
        fig_drifted_pe.show()

# COMMAND ----------

#Saving PE drift results per products
from pyspark.sql import functions as F
pe_drift_base_path  = f"{folder}{run_id}/other_files/pe_drift{export_suffix}/" 
print(f"Writing PE drift results to Delta table: {pe_drift_base_path}")
df_pe_drift_spark = spark.createDataFrame(df_tmp)

product_names = (df_pe_drift_spark.select("kpl_preferred_name").distinct().collect())

for row in product_names:
    prod_name_val = row["kpl_preferred_name"]
    folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

    per_prod_df = df_pe_drift_spark.filter(F.col("kpl_preferred_name") == F.lit(prod_name_val))
    per_product_path = f"{pe_drift_base_path}{folder_name}"

    (per_prod_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(per_product_path))

print(f"Per-product PE drift written under: {pe_drift_base_path}")


# COMMAND ----------

# DBTITLE 1,Data Drift
df_details_data, df_res_data = data_drift(product_lineup_sk="ALL", account_state="ALL", gb=["product_lineup_sk", "account_state"], start_date_drift=start_date_drift) # Renamed df_details to avoid conflict
df_drift_data = df_details_data[(df_details_data.predict_date>start_date_drift) & ( # Use df_details_data
    (df_details_data.rsd_quantity_drift==True) | (df_details_data.rsd_price_drift==True) |
    (df_details_data.rsd_quantity_industry_drift==True) | (df_details_data.rsd_price_industry_drift==True) |
    (df_details_data.rsd_price_perc_drift==True) | (df_details_data.som_drift==True)
)][["product_lineup_sk", "account_state", "predict_date"]+[c for c in df_details_data.columns if "_drift" in c]] # Use df_details_data

if df_drift_data.empty:
    print("No Data drift details to display (df_drift_data is empty).")
else:
    display(df_drift_data) # Use df_drift_data


l_data_drift = []
for field in ["rsd_quantity_drift", "rsd_price_drift", "rsd_quantity_industry_drift",
              "rsd_price_industry_drift", "som_drift"]:
    # "rsd_price_perc_drift" not needed
    l_data_drift.append(
        df_drift_data # Use df_drift_data
        .query(f"{field}==True")
        .groupby(["product_lineup_sk","account_state"])
        .size() # Use size()
        .reset_index(name='count_per_state') # Use size()
        .query(f"count_per_state>=2") # Adjusted query
        .groupby("product_lineup_sk")
        .size() # Count products meeting criteria
        .rename(field) # Rename the series to the field name
    )

df_combined_data_drift = pd.concat(l_data_drift, axis=1) # Concatenate Series into a DataFrame, index is product_lineup_sk

df_all_products_data = df_details_data[["product_lineup_sk"]].drop_duplicates()

df_merged_data_counts = pd.merge(
    df_all_products_data,
    df_combined_data_drift,
    left_on="product_lineup_sk",
    right_index=True, # Join left column with right index
    how="left" # Keep all products
)

df_melted = df_merged_data_counts.melt(
    id_vars="product_lineup_sk",
    var_name="variable",
    value_name="value"
)

if 'product_lineup_sk' not in df_name.columns:
    df_name_reset = df_name.reset_index()
else:
    df_name_reset = df_name.copy()


df_tmp = pd.merge(
    df_melted,
    df_name_reset[['product_lineup_sk', 'kpl_preferred_name']], # Select necessary columns
    on="product_lineup_sk",
    how="inner" # Keep only products that have names
).fillna({'value': 0}) # Fill only the count column 'value' with 0


df_tmp["product_lineup_sk"] = df_tmp["product_lineup_sk"].astype(str)
df_tmp = df_tmp.query('~kpl_preferred_name.isin(["NJOY ACE PUK", "NJOY ACE DEVICE"])')
fig = px.bar(df_tmp, x="kpl_preferred_name", y="value", color="variable", color_discrete_sequence=BAT_COLORS[1:],
       barmode = "group", labels={"value": "Number of instances of data drift"},
       title="Number of instances of data drift")
fig.update_yaxes(range=[0,55])
fig.show()

# COMMAND ----------

# --- Weighted PE (wPE) time series for all products (own-price only) ---

import pandas as pd
import plotly.express as px

# Optional: respect the widget-provided product_list if it exists
product_filter = ""
try:
    if 'product_list' in locals() and product_list:
        product_filter = f"AND product_lineup_sk IN ({', '.join(map(str, product_list))})"
except NameError:
    pass

sql = f"""
with agg as (
    select predict_date, pe_type, product_lineup_sk, simulated_price_product_lineup_sk, count(*) nb_rows
    , sum(pe*rsd_quantity_cross)/sum(rsd_quantity_cross) wpe
    , sum(rsd_quantity_cross) rsd_quantity_cross
    , count(distinct account_sk) n_acccount
    from pe 
    where product_lineup_sk = simulated_price_product_lineup_sk
    {product_filter}
    group by predict_date, pe_type, product_lineup_sk, simulated_price_product_lineup_sk
),
names AS (
  SELECT DISTINCT product_lineup_sk, kpl_preferred_name
  FROM df_product
)
SELECT
    a.product_lineup_sk,
    a.predict_date,
    a.wpe,
    COALESCE(n.kpl_preferred_name, CAST(a.product_lineup_sk AS STRING)) AS product_name
FROM agg a
LEFT JOIN names n USING (product_lineup_sk)
ORDER BY product_name, predict_date
"""

df_ts = spark.sql(sql).toPandas()
if df_ts.empty:
    print("No data found for wPE time series.")
else:
    df_ts["predict_date"] = pd.to_datetime(df_ts["predict_date"])
    df_ts = df_ts.sort_values(["product_name", "predict_date"])

    fig = px.line(
        df_ts,
        x="predict_date",
        y="wpe",
        color="product_name",
        markers=True,
        title="Weighted PE over time (own-price)"
    )
    fig.update_layout(
        xaxis_title="Predict date",
        yaxis_title="Weighted PE (wPE)",
        xaxis=dict(rangeslider=dict(visible=True))
    )
    try:
        display(fig.show(renderer="png"))
    except Exception:
        display(fig)

# COMMAND ----------

# Saving Data drift results per product
data_drift_base_path  = f"{folder}{run_id}/other_files/data_drift{export_suffix}/" 
print(f"Writing Data drift results to Delta table: {data_drift_base_path}")
df_data_drift_spark = spark.createDataFrame(df_tmp)


product_names_dd = (df_data_drift_spark.select("kpl_preferred_name").distinct().collect())

for row in product_names_dd:
    prod_name_val = row["kpl_preferred_name"]
    folder_name = prod_name_val.lower().replace(" ", "").replace("/", "")

    per_prod_df = df_data_drift_spark.filter(F.col("kpl_preferred_name") == F.lit(prod_name_val))
    per_product_path = f"{data_drift_base_path}{folder_name}"

    (per_prod_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(per_product_path))

print(f"Per-product Data drift written under: {data_drift_base_path}")


</file>

<file path="notebooks/utils.py.py">
# Databricks notebook source
delta = 1

# COMMAND ----------

# DBTITLE 1,delta helpers
import fnmatch
from typing import List, Union
from pyspark.sql import SparkSession, DataFrame
from functools import reduce

def list_all_dirs(path: str) -> List[str]:
    """
    Recursively list all directories under a given path using dbutils.fs.
    """
    all_dirs = []

    def recurse(p):
        try:
            items = dbutils.fs.ls(p)
        except Exception as e:
            print(f"⚠️ Cannot access {p}: {e}")
            return

        for item in items:
            if item.isDir():
                all_dirs.append(item.path)
                recurse(item.path)

    recurse(path)
    return all_dirs

def find_matching_delta_paths(base_path: str, patterns: List[str]) -> List[str]:
    """
    From all subdirs under base_path, return those that:
    - Match any pattern in patterns (using fnmatch)
    - Contain a `_delta_log/` subfolder (i.e., a Delta table)
    """
    all_dirs = list_all_dirs(base_path)
    matching_paths = []

    for dir_path in all_dirs:
        # Check if dir_path matches any pattern
        if any(fnmatch.fnmatch(dir_path.rstrip('/'), pattern.rstrip('/')) for pattern in patterns):
            # Check if _delta_log exists inside
            try:
                _ = dbutils.fs.ls(f"{dir_path.rstrip('/')}/_delta_log")
                matching_paths.append(dir_path.rstrip('/'))
            except:
                continue

    return matching_paths

def load_delta_tables_with_wildcards(

    spark: SparkSession,
    pattern_paths: List[str],
    union_all: bool = True,
    verbose: bool = True
) -> Union[DataFrame, List[DataFrame]]:
    """
    Load Delta tables from a list of wildcard patterns using dbutils.fs + fnmatch.
    """
    if not pattern_paths:
        raise ValueError("pattern_paths list is empty")

    # Extract base path from first pattern (everything before first '*')
    base_paths = set()
    for pattern in pattern_paths:
        split_index = pattern.find("*")
        if split_index == -1:
            base_path = pattern
        else:
            base_path = pattern[:split_index].rpartition("/")[0] + "/"
        base_paths.add(base_path)

    delta_paths = []
    for base_path in base_paths:
        delta_paths.extend(find_matching_delta_paths(base_path, pattern_paths))

    if not delta_paths:
        raise ValueError(f"No Delta tables found matching patterns: {pattern_paths}")

    dfs = []
    for path in delta_paths:
        try:
            df = spark.read.format("delta").load(path)
            dfs.append(df)
            if verbose:
                print(f"✔ Loaded: {path}")
        except Exception as e:
            if verbose:
                print(f"✘ Skipped: {path} — {e}")

    if not dfs:
        raise RuntimeError("No Delta tables could be loaded.")

    if union_all:
        return reduce(DataFrame.unionByName, dfs)
    else:
        return dfs
    
def load_delta_tables_explicit(
    spark: SparkSession,
    paths: List[str],
    union_all: bool = True,
    verbose: bool = True
) -> Union[DataFrame, List[DataFrame]]:
    """
    Load and optionally union multiple explicit Delta table paths (no wildcards).
    """
    if not paths:
        raise ValueError("paths list is empty")

    valid_paths = []
    for path in paths:
        try:
            _ = dbutils.fs.ls(path.rstrip("/") + "/_delta_log")
            valid_paths.append(path.rstrip("/"))
            if verbose:
                print(f"✔ Valid Delta table: {path}")
        except:
            if verbose:
                print(f"✘ Skipped (no _delta_log): {path}")

    if not valid_paths:
        raise ValueError("No valid Delta tables found in provided paths.")

    dfs = []
    for path in valid_paths:
        try:
            df = spark.read.format("delta").load(path)
            dfs.append(df)
        except Exception as e:
            if verbose:
                print(f"✘ Error loading {path}: {e}")

    if not dfs:
        raise RuntimeError("No Delta tables could be loaded.")

    return reduce(DataFrame.unionByName, dfs) if union_all else dfs

# COMMAND ----------

# DBTITLE 1,ML model - SQL queries
def create_training_data_view(product_lineup_sk, l_product_linup_sk, export_path, macroeconomics, price_threshold, gb_price_tier=0, gb_cig_length=0, gb_menthol=0, cpu_or_serving='cpu', industry_unique_id=0):

    """ Create a training data view `train_vw` to use as the training data for the lgb model and for the simulation model that is using lgb
    inputs: 
     - product_lineup_sk: the product we are interested in, each product has it's own model
     - l_product_linup_sk: the competitors of the product product_lineup_sk
    """
    if industry_unique_id == 0:
      print("missing industry_unique_id")
      return 0
    if delta == 1:
      spark.read.format("delta").load(f"{export_path}/data/delta/df_industry").filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("industry")
      spark.read.format("delta").load(f"{export_path}/data/delta/df_product").createOrReplaceTempView("raw_product")
      if macroeconomics:
        spark.read.format("delta").load(f"{export_path}/data/delta/df_macrolevels_processed").createOrReplaceTempView("macrolevels")
    else:
      spark.read.parquet(f"{export_path}/data/df_industry.parquet").filter(f"product_lineup_sk={industry_unique_id}").createOrReplaceTempView("industry")
      spark.read.parquet(f"{export_path}/data/df_product.parquet").createOrReplaceTempView("raw_product")
      if macroeconomics:
        spark.read.parquet(f"{export_path}/data/df_macrolevels_processed.parquet").createOrReplaceTempView("macrolevels")

    # if cpu_or_serving == 'cpu':
    #     price_threshold = 1
    # if cpu_or_serving == 'serving':
    #     price_threshold = 0.2
    # if cpu_or_serving == 'cartridge':
    #     price_threshold = 1


    # group the product data depending if we want to group it by price_tier, cig_length, menthol
    sql = f"""create or replace temp view product as (
                    select account_sk
                    , product_lineup_sk
                    , kpl_preferred_name
                    , sales_week_sk
                    , {"price_tier" if gb_price_tier else "NULL"} price_tier
                    , {"cig_length" if gb_cig_length else "NULL"} cig_length
                    , {"menthol_non_menthol" if gb_menthol else "NULL"} menthol_non_menthol
                    , sum(rsd_quantity) rsd_quantity
                    , sum(rsd_price*rsd_quantity)/sum(rsd_quantity) rsd_price
                    from raw_product
                    group by 1,2,3,4,5,6,7
                    )"""
    spark.sql(sql)

    # clean the product data prices/volumes
    sql = f"""
    create or replace temp view product_clean_data as (
        with interquartile_price as ( -- there is a lof of wrong prices in the db, we are trying to clean most of them with this query
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol
            , percentile(case when rsd_price >= {price_threshold} then rsd_price else null end, 0.1) q10
            , percentile(case when rsd_price >= {price_threshold} then rsd_price else null end, 0.9) q90
            from product
            where product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
            group by product_lineup_sk, price_tier, cig_length, menthol_non_menthol
        ),
        interquartile_price_range as (
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol
            , q10
            , q90
            , (q90 - q10) * 1.5 cut_off -- interquartile range = q90 - q10
            from interquartile_price
        ),
        interquartile_price_thresholds as (
            select product_lineup_sk, price_tier, cig_length, menthol_non_menthol

            , q10 - cut_off lower
            , q90 + cut_off upper
            from interquartile_price_range
        )
        select account_sk
        , sales_week_sk
        , product.product_lineup_sk
        , kpl_preferred_name
        , product.price_tier
        , product.cig_length
        , product.menthol_non_menthol
        , case when rsd_quantity >= 0 then rsd_quantity else 0 end rsd_quantity
        , case when rsd_price >= {price_threshold} and rsd_price > lower and rsd_price < upper then rsd_price else null end rsd_price
        from product 
        inner join interquartile_price_thresholds i
            on product.product_lineup_sk=i.product_lineup_sk
            {"and product.price_tier=i.price_tier" if gb_price_tier else ""}
            {"and product.cig_length=i.cig_length" if gb_cig_length else ""}
            {"and product.menthol_non_menthol=i.menthol_non_menthol" if gb_menthol else ""}
    )
    """
    spark.sql(sql)
    
    sql_prod = f"""
    --create or replace temp view train_vw as 
    with product_vw as (
    select account_sk
      , sales_week_sk
      , product_clean_data.product_lineup_sk
      , kpl_preferred_name
      , product_clean_data.price_tier
      , product_clean_data.cig_length
      , product_clean_data.menthol_non_menthol
      , rsd_quantity
      , rsd_price
    from product_clean_data
    where product_clean_data.product_lineup_sk={product_lineup_sk}
    ),
    competitors_vw as (
    select * from (
      select product_clean_data.product_lineup_sk
        , sales_week_sk
        , account_sk
        , sum(rsd_quantity) rsd_quantity
        , sum(case when rsd_price is not null then rsd_price*rsd_quantity else 0 end)/sum(case when rsd_price is not null then rsd_quantity else 0 end) rsd_price
      from product_clean_data
      where product_clean_data.product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
      group by product_clean_data.product_lineup_sk, sales_week_sk, account_sk
    ) pivot (
      first(rsd_quantity) rsd_quantity, first(rsd_price) rsd_price for product_lineup_sk in {'(%s)' % ', '.join(map(repr, l_product_linup_sk+[product_lineup_sk]))}
    )
    ),
    min_sk as (
    select account_sk, min(sales_week_start_date) min_sales_week_start_date
    from industry
    group by account_sk
    ),
    joined as (
    select to_timestamp(industry.sales_week_sk) sales_week_sk
      , to_timestamp(sales_week_start_date) Time
      , industry.sales_week_start_date
      , case when datediff(industry.sales_week_start_date, min_sales_week_start_date)/7 <= 52 then datediff(industry.sales_week_start_date, min_sales_week_start_date)/7 else 52 end data_points -- data_points for the account_sk
      , industry.account_sk
      , industry.account_state
      , industry.l1_account_name
      , industry.l1_account_code
      , case when industry.rsd_quantity_industry >= 0 then industry.rsd_quantity_industry else 0 end rsd_quantity_industry
      , case when industry.rsd_price_industry >= {price_threshold} then industry.rsd_price_industry else null end rsd_price_industry
      , product_vw.kpl_preferred_name
      , product_vw.product_lineup_sk
      , product_vw.price_tier
      , product_vw.cig_length
      , product_vw.menthol_non_menthol
      , coalesce(product_vw.rsd_quantity, 0) rsd_quantity
      , product_vw.rsd_price
      {", " if len(l_product_linup_sk)>0 else ""} {", ".join([f"coalesce(competitors_vw.{pid}_rsd_quantity, 0) {pid}_rsd_quantity" for pid in l_product_linup_sk])}
      {", " if len(l_product_linup_sk)>0 else ""} {", ".join([f"competitors_vw.{pid}_rsd_price" for pid in l_product_linup_sk])}

      -- in case we want to compute a price elasticity for a product by price_tier or cig_length,
      -- we need to have this product with the other price_tier and cig_length as competitor
      -- because there can be cross-correlations between the two
      , coalesce(competitors_vw.{product_lineup_sk}_rsd_quantity - product_vw.rsd_quantity, 0) {product_lineup_sk}_rsd_quantity
      , (competitors_vw.{product_lineup_sk}_rsd_quantity*competitors_vw.{product_lineup_sk}_rsd_price - product_vw.rsd_quantity*product_vw.rsd_price)/(competitors_vw.{product_lineup_sk}_rsd_quantity - product_vw.rsd_quantity) {product_lineup_sk}_rsd_price
    
    from industry 
    left join product_vw on industry.account_sk = product_vw.account_sk and industry.sales_week_sk = product_vw.sales_week_sk
    left join competitors_vw on industry.account_sk = competitors_vw.account_sk and industry.sales_week_sk = competitors_vw.sales_week_sk
    left join min_sk on industry.account_sk = min_sk.account_sk
    ),
    ffill as (
    select joined.*
      , last_value(rsd_price, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) rsd_price_ffill
      , last_value(rsd_price_industry, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) rsd_price_industry_ffill
      , {", ".join([f"last_value({pid}_rsd_price, true) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time rows between unbounded preceding and current row) {pid}_rsd_price_ffill" for pid in l_product_linup_sk+[product_lineup_sk]])}
      from joined
    ),
    compute_revenues as (
    select ffill.*
      , coalesce(rsd_quantity * rsd_price_ffill, 0) rsd_revenue
      , coalesce(rsd_quantity_industry * rsd_price_industry_ffill, 0) rsd_revenue_industry
      , {", ".join([f"coalesce({pid}_rsd_quantity * {pid}_rsd_price_ffill, 0) {pid}_rsd_revenue" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from ffill
    ),
    rolled as (
    select sales_week_sk, Time, sales_week_start_date
      , data_points
      , account_sk, account_state
      , l1_account_name, l1_account_code
      , rsd_quantity_industry
      , rsd_price_industry_ffill rsd_price_industry
      , kpl_preferred_name
      , price_tier
      , cig_length
      , menthol_non_menthol
      , product_lineup_sk
      , rsd_quantity
      , rsd_price_ffill rsd_price
      , {", ".join([f"{pid}_rsd_quantity" for pid in l_product_linup_sk+[product_lineup_sk]])}
      , {", ".join([f"{pid}_rsd_price_ffill {pid}_rsd_price" for pid in l_product_linup_sk+[product_lineup_sk]])}
      
      -- mean revenue with min periods
      , mean(rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_revenue_roll4
      , mean(rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_revenue_roll13
      , mean(rsd_revenue_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_revenue_industry_roll4
      , {", ".join([f"mean({pid}_rsd_revenue) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) {pid}_rsd_revenue_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}
      
      -- mean quantity
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 1 weeks preceding and current row) rsd_quantity_roll2
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_quantity_roll4
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 7 weeks preceding and current row) rsd_quantity_roll8
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_quantity_roll13
      , mean(rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 25 weeks preceding and current row) rsd_quantity_roll26
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 1 weeks preceding and current row) rsd_quantity_industry_roll2
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) rsd_quantity_industry_roll4
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 7 weeks preceding and current row) rsd_quantity_industry_roll8
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_quantity_industry_roll13
      , mean(rsd_quantity_industry) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 25 weeks preceding and current row) rsd_quantity_industry_roll26
      , {", ".join([f"mean({pid}_rsd_quantity) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 3 weeks preceding and current row) {pid}_rsd_quantity_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}

      from compute_revenues
    ),
    ffill_price as (
    select rolled.*
      , case when rsd_revenue_roll4 / rsd_quantity_roll4 > 0 then rsd_revenue_roll4 / rsd_quantity_roll4 else null end rsd_price_roll4
      , case when rsd_revenue_industry_roll4 / rsd_quantity_industry_roll4 > 0 then rsd_revenue_industry_roll4 / rsd_quantity_industry_roll4 else null end  rsd_price_industry_roll4
      , {", ".join([f"case when {pid}_rsd_revenue_roll4 / {pid}_rsd_quantity_roll4 > 0 then {pid}_rsd_revenue_roll4 / {pid}_rsd_quantity_roll4 else null end {pid}_rsd_price_roll4" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from rolled
    ),
    lags as (
    select ffill_price.*
      , mean(rsd_revenue_roll4/rsd_quantity_roll4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time range between interval 12 weeks preceding and current row) rsd_price_roll4_roll13
      , lag(rsd_quantity_roll4, 52) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_roll4_prev_y
      , lag(sales_week_start_date, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) sales_week_forecast
      , lag(rsd_quantity_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_y
      , lag(rsd_price_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_price_y
      , lag(rsd_quantity_industry_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_quantity_industry_y
      , lag(rsd_price_industry_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) rsd_price_industry_y
      , {", ".join([f"lag({pid}_rsd_price_roll4, -4) over (partition by account_sk, cig_length, price_tier, menthol_non_menthol order by Time) {pid}_rsd_price_y" for pid in l_product_linup_sk+[product_lineup_sk]])}
    from ffill_price
    ),
    all_data as (
    select lags.*
        , rsd_price_y / rsd_price_roll4 rsd_price_perc
        , rsd_price_y - rsd_price_roll4 rsd_price_diff
        , rsd_quantity_y / rsd_quantity_roll4 rsd_quantity_perc
        , rsd_quantity_industry_y / rsd_quantity_industry_roll4 rsd_quantity_ind_perc
        , rsd_price_industry_y / rsd_price_industry_roll4 rsd_price_ind_perc
        , rsd_price_roll4 / rsd_price_industry_roll4 rsd_price_perc_ind
        , rsd_quantity_roll2 / rsd_quantity_industry_roll2 rsd_quantity_roll2_perc
        , rsd_quantity_roll4 / rsd_quantity_industry_roll4 rsd_quantity_roll4_perc
        , {", ".join([f"{pid}_rsd_quantity_roll4 / rsd_quantity_industry_roll4 {pid}_rsd_quantity_roll4_perc" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , rsd_quantity_roll8 / rsd_quantity_industry_roll8 rsd_quantity_roll8_perc
        , rsd_quantity_roll13 / rsd_quantity_industry_roll13 rsd_quantity_roll13_perc
        , rsd_quantity_roll26 / rsd_quantity_industry_roll26 rsd_quantity_roll26_perc
        , rsd_quantity_industry_roll2 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll2_perc
        , rsd_quantity_industry_roll4 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll4_perc
        , rsd_quantity_industry_roll8 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll8_perc
        , rsd_quantity_industry_roll13 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll13_perc
        , rsd_quantity_industry_roll26 / rsd_quantity_industry_roll4 rsd_quantity_industry_roll26_perc
        , {", ".join([f"coalesce({pid}_rsd_price_y / {pid}_rsd_price_roll4, 1) {pid}_rsd_price_perc" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , {", ".join([f"coalesce({pid}_rsd_price_y - {pid}_rsd_price_roll4, 0) {pid}_rsd_price_diff" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , {", ".join([f"{pid}_rsd_price_roll4 - rsd_price_roll4 {pid}_rsd_price_ref" for pid in l_product_linup_sk+[product_lineup_sk]])}
        , (rsd_revenue_roll4/rsd_quantity_roll4) / (rsd_price_roll4_roll13) relative_price_hist
        , sin((month(sales_week_start_date)-1)*(2.*pi()/12)) month_sin
        , cos((month(sales_week_start_date)-1)*(2.*pi()/12)) month_cos
        , log(1 + rsd_quantity_roll4/5) weight
    from lags
    )
    select all_data.*
    from all_data
    where rsd_price_perc is not null 
     and rsd_quantity_roll4 is not null 
     and rsd_quantity_roll4 > 1 
     and rsd_price_roll4 >= 0
     and product_lineup_sk is not null
    order by account_sk, cig_length, price_tier, menthol_non_menthol, sales_week_start_date

    """
    spark.sql(sql_prod).createOrReplaceTempView('final_product_data')
    
    # if macroeconomic features are required
    if macroeconomics:
        indicator_names = ['Unemployment rate', 'Inflation, consumer price index - % year-on-year', 'Consumer spending, nominal, US$ - Total consumer spending',
                          'Consumer spending, nominal, US$ - Tobacco', 'Gasoline price, retail, regular grade', 'Consumer price index']
        
#         sql_macro = f"""with process_date as ( 
#                    select *
#                    , case when macro_economic_quarter like '%1' then to_date(replace(macro_economic_quarter, 'Q1', '-01-01'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%2' then to_date(replace(macro_economic_quarter, 'Q2', '-04-01'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%3' then to_date(replace(macro_economic_quarter, 'Q3', '-07-01'), 'yyyy-MM-dd')
#                           else to_date(replace(macro_economic_quarter, 'Q4', '-10-01'), 'yyyy-MM-dd') end quarter_start_date
#                   , case when macro_economic_quarter like '%1' then to_date(replace(macro_economic_quarter, 'Q1', '-03-31'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%2' then to_date(replace(macro_economic_quarter, 'Q2', '-06-30'), 'yyyy-MM-dd')
#                           when macro_economic_quarter like '%3' then to_date(replace(macro_economic_quarter, 'Q3', '-09-30'), 'yyyy-MM-dd')
#                           else to_date(replace(macro_economic_quarter, 'Q4', '-12-31'), 'yyyy-MM-dd') end quarter_end_date
#             from macrolevels
#         ),
#         macro_pivoted as (
#              select quarter_start_date
#                    , quarter_end_date
#                    , cast(`Unemployment rate` as float) unemployment_rate
#                    , cast(`Inflation, consumer price index - % year-on-year` as float) inflation_yoy
#                    , cast(`Consumer spending, nominal, US$ - Total consumer spending` as float) consumer_spending_total
#                    , cast(`Consumer spending, nominal, US$ - Tobacco` as float) consumer_spending_tobacco
#                    , cast(`Gasoline price, retail, regular grade` as float) gasoline_price
#                    , cast(`Consumer price index` as float) consumer_price_index
#              from (
#                  select *
#                 from process_date
#              ) pivot (
#                  first(agg_value) agg_value for indicator_name in {tuple(indicator_names)}
#            )
#         )
#         select * 
#         from final_product_data
#           left join macro_pivoted
#         on final_product_data.sales_week_start_date between macro_pivoted.quarter_start_date and macro_pivoted.quarter_end_date
#         """

        sql_macro = f"""with process_date as ( 
                    select *
                     from macrolevels
         ),
        macro_pivoted as (
              select macro_economic_week
                    , cast(`Unemployment rate` as float) unemployment_rate
                    , cast(`Inflation, consumer price index - % year-on-year` as float) inflation_yoy
                    , cast(`Consumer spending, nominal, US$ - Total consumer spending` as float) consumer_spending_total
                    , cast(`Consumer spending, nominal, US$ - Tobacco` as float) consumer_spending_tobacco
                    , cast(`Gasoline price, retail, regular grade` as float) gasoline_price
                    , cast(`Consumer price index` as float) consumer_price_index

              from (
                  select *
                 from process_date
              ) pivot (
                  first(agg_value) agg_value for indicator_name in {tuple(indicator_names)}
            )
        ),
        
        lags_macros as(
            select macro_economic_week,
                lag(unemployment_rate, 4) over ( order by macro_economic_week) unemployment_rate_prev_m,
                lag(inflation_yoy, 4) over ( order by macro_economic_week) inflation_yoy_prev_m,
                lag(consumer_spending_total, 4) over ( order by macro_economic_week) consumer_spending_total_prev_m,
                lag(consumer_spending_tobacco, 4) over ( order by macro_economic_week) consumer_spending_tobacco_prev_m,
                lag(gasoline_price, 4) over ( order by macro_economic_week) gasoline_price_prev_m,
                lag(consumer_price_index, 4) over (order by macro_economic_week) consumer_price_index_prev_m
            from macro_pivoted    
        ),
        all_macros as (
            select macro.*, 
                lag_macros.unemployment_rate_prev_m,
                lag_macros.inflation_yoy_prev_m,
                lag_macros.consumer_spending_total_prev_m,
                lag_macros.consumer_spending_tobacco_prev_m ,
                lag_macros.gasoline_price_prev_m ,
                lag_macros.consumer_price_index_prev_m ,
                case when macro.unemployment_rate / lag_macros.unemployment_rate_prev_m > 0 then macro.unemployment_rate / lag_macros.unemployment_rate_prev_m - 1 else null end unemployment_rate_change,
                case when macro.inflation_yoy / lag_macros.inflation_yoy_prev_m > 0 then macro.inflation_yoy / lag_macros.inflation_yoy_prev_m - 1 else null end inflation_yoy_change,
                case when macro.consumer_spending_total / lag_macros.consumer_spending_total_prev_m > 0 then macro.consumer_spending_total / lag_macros.consumer_spending_total_prev_m - 1 else null end consumer_spending_total_change,
                case when macro.consumer_spending_tobacco / lag_macros.consumer_spending_tobacco_prev_m > 0 then macro.consumer_spending_tobacco / lag_macros.consumer_spending_tobacco_prev_m - 1 else null end consumer_spending_tobacco_change,
                case when macro.gasoline_price / lag_macros.gasoline_price_prev_m > 0 then macro.gasoline_price / lag_macros.gasoline_price_prev_m - 1 else null end gasoline_price_change,
                case when macro.consumer_price_index / lag_macros.consumer_price_index_prev_m > 0 then macro.consumer_price_index / lag_macros.consumer_price_index_prev_m - 1 else null end consumer_price_index_change
            from   macro_pivoted macro
            join  lags_macros lag_macros
            on macro.macro_economic_week = lag_macros.macro_economic_week
        )
        
        select final_product_data.*, 
                all_macros.*

        from final_product_data
            left join all_macros 
                on final_product_data.sales_week_start_date = all_macros.macro_economic_week
            """
        spark.sql(sql_macro).createOrReplaceTempView('final_product_data')

    sql = f"""create or replace temp view train_vw as
                select * from final_product_data
                -- where condition as NewportportMenXSS has no more data on CA in 2023 due to a ban on menthol brands
                where (product_lineup_sk != 63261) or 
                      (product_lineup_sk == 63261 and (account_state != 'CA' or (account_state=='CA' and sales_week_forecast < '2022-11-01')))
                order by account_sk,  
                cig_length,
                price_tier,
                menthol_non_menthol,
                sales_week_start_date
           """
    spark.sql(sql)

# COMMAND ----------

def get_extra_info(export_path):
  if delta == 1:
    spark.read.format("delta").load(f"{export_path}/data/delta/df_product").createOrReplaceTempView("product")
  else:
      spark.read.parquet(f"{export_path}/data/df_product.parquet").createOrReplaceTempView("product")
  
  sql = """
    create or replace temp view extra_info_vw as
      select distinct account_sk, product_lineup_sk, kpl_preferred_name
      , price_tier, cig_length, menthol_non_menthol
      from product
  """
  spark.sql(sql)

# COMMAND ----------

# DBTITLE 1,Load functions for kpi view
# MAGIC %run ./utils_kpi.py

# COMMAND ----------

# DBTITLE 1,List of kpi variables per industry
from typing import List


def kpi_variables_industry(industry: str) -> List:
    if industry == "fmc":
        kpi_variables = [
            "latitude_norm", "longitude_norm",
            "VOL_TC_norm",
            "IDX_TC_norm",
            "share_of_BR_MARLBORO",
            "share_of_BR_NATURAL_AMERICAN_SPIRIT",
            "share_of_BR_NEWPORT",
            "share_of_BR_PALL_LUCKYSTR",
            "share_of_BR_CAMEL_CRUSH",
            "share_of_BR_CAMEL_EX_CRUSH",
            "share_of_MFR_ITG_CIG_Pro_forma",
            "share_of_MFR_NON_BIG_3_Pro_forma",
        ]
    elif industry == "vapor":
        kpi_variables = [
            "latitude_norm", "longitude_norm",
            # "VOL_TV_norm",
            # "IDX_TV_norm",
            "share_of_FLV_MEN",
            "share_of_FLV_NM",
            "share_of_FLV_XM_NM",
            "share_of_br_blu",
            "share_of_br_juul",
            "share_of_br_logic",
            "share_of_br_njoy",
            "share_of_br_vuse",
            "share_of_br_others",
            # "share_of_strg_2",
            # "share_of_strg_2_4",
            # "share_of_strg_4",
            # "share_of_cart_2",
            # "share_of_cart_2_3",
            # "share_of_cart_4",
            # "share_of_sub_liq_bott",
            # "share_of_sub_kits",
            # "share_of_sub_rpl_cart",
            # "share_of_sub_disposables",
            # "share_of_mfr_non_big5",
            # "share_of_mfr_blu",
            # "share_of_mfr_juul",
            # "share_of_mfr_logic",
            # "share_of_mfr_rjr",
            # "share_of_mfr_sottera",
            # "IDX_FLV_MEN_norm",
            # "IDX_FLV_NM_norm",
            # "IDX_FLV_XM_NM_norm",
            # "share_of_mfr_others",
        ]
    elif industry == "mo":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TN_norm",
            # "IDX_TN_norm",
            # "IDX_FLV_MEN_norm",
            # "IDX_FLV_NM_norm",
            "share_of_FLV_MEN",
            "share_of_FLV_NM",
            "share_of_br_velo",
            "share_of_br_zyn_nt",
            "share_of_br_on",
            "share_of_br_rogue",
            # "share_of_mfr_swd",
            # "share_of_mfr_modoral",
            # "share_of_mfr_rogue",
            # "share_of_mfr_helix",
            "share_of_str_2mg_6mg",
            "share_of_str_2mg",
            "share_of_str_6mg",
            "share_of_sub_cat_lozenges",
            "share_of_sub_cat_pouches",
            "share_of_sub_cat_pouches_lozenges",
            "share_of_br_others",
            # "share_of_mfr_others",
        ]
    elif industry == "to":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TM_norm",
            # "IDX_TM_norm",
            # "IDX_FLV_MINT_norm",
            # "IDX_FLV_NATURAL_norm",
            # "IDX_FLV_STRAIGHT_norm",
            # "IDX_FLV_WINTERGREEN_norm",
            # "IDX_PR_BV_norm",
            # "IDX_PR_LV_norm",
            # "IDX_PR_PP_norm",
            # "IDX_PR_PFP_norm",
            # "IDX_FLV_OTHER_norm",
            "share_of_br_copenhagen",
            "share_of_br_grizzly",
            "share_of_br_kodiak",
            "share_of_br_longhorn",
            "share_of_br_red_seal",
            "share_of_br_skoal",
            "share_of_br_stokers",
            "share_of_br_others",
            "share_of_mfr_asc",
            "share_of_mfr_other",
            "share_of_mfr_swd_match",
            "share_of_mfr_usst",
            # "share_of_pt_branded_value",
            "share_of_pt_low_end",
            "share_of_pt_popular_price",
            # "share_of_pt_premium_full_price",
            "share_of_flv_mint_wintergreen",
            "share_of_flv_natural_straight",
            "share_of_flv_other",
            # "share_of_flv_mint",
            # "share_of_flv_natural",
            # "share_of_flv_straight",
            # "share_of_flv_wintergreen",
        ]
    elif industry == "snus":
        kpi_variables = [
            "account_latitude_norm", "account_longitude_norm",
            # "VOL_TS_norm",
            # "IDX_TS_norm",
            "share_of_br_camel",
            "share_of_br_general",
            "share_of_br_grizzly",
            "share_of_br_skoal",
            # "share_of_mfr_asc",
            # "share_of_mfr_rjrt",
            # "share_of_mfr_swd_match",
            # "share_of_mfr_ustt",
            "share_of_flv_mint",
            "share_of_flv_nt",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NT"
        ]
    return kpi_variables

# COMMAND ----------

# DBTITLE 1,STR - SQL Queries
from typing import Optional
from pyspark.sql import functions as F, DataFrame

# ---------------------------------------------------------------------------
def _path_exists(path: str) -> bool:
    try:
        dbutils.fs.ls(path)
        return True
    except Exception:
        return False
# ---------------------------------------------------------------------------

def create_str_training_view(
    pe_file: str,
    export_path: str,
    date: str,
    industry: str,
    gb_price_tier: bool = False,
    gb_cig_length: bool = False,
    gb_menthol: bool = False,
    use_delta: bool = True,
    mech_delta_path: Optional[str] = None,
) -> None:

    # ----------------------------------------------------------------------
    # 0 · Load raw STR
    # ----------------------------------------------------------------------
    str_path_delta   = f"{export_path}/data/delta/df_str"
    str_path_parquet = f"{export_path}/data/df_str.parquet"

    if use_delta:
        spark.read.format("delta").load(str_path_delta) \
             .createOrReplaceTempView("raw_df_str")
    else:
        spark.read.parquet(str_path_parquet) \
             .createOrReplaceTempView("raw_df_str")

    # ----------------------------------------------------------------------
    # 1 · PE‑RSD – union all *_delta folders exported by the PE notebook
    # ----------------------------------------------------------------------
    (
        load_delta_tables_with_wildcards(spark, [f"{pe_file}/*"], union_all=True)
        .createOrReplaceTempView("raw_pe_rsd")
    )

    # ----------------------------------------------------------------------
    # 2 · Mechanical‑Price Delta
    # ----------------------------------------------------------------------
    if mech_delta_path is None:
        mech_delta_path = f"{export_path}/data/delta/mechanical_price"

    mech_df = (
        spark.read.format("delta").load(mech_delta_path)
        .withColumnRenamed("state",  "account_state")
        .withColumnRenamed("price",  "str_price")
        .withColumnRenamed("product","product_name")
    )
    mech_df.createOrReplaceTempView("mechanical_price_contract")

    # ----------------------------------------------------------------------
    # 3 · Build mechanical_price_outlet_contract on the fly
    # ----------------------------------------------------------------------
    spark.sql(f"""
        WITH account_cbo AS (
            SELECT
                account_sk,
                FIRST(CAST(regexp_extract(
                           UPPER(program),
                           'CBO[^0-9]*([0-9]+)', 1) AS INT)) AS cbo  
            FROM   rai_qa_uc_gm.d_core_pace.account_contract_map
            WHERE  program LIKE '%CBO%'
              AND  start_date <= DATE '{date}'
              AND  end_date   >= DATE '{date}'
            GROUP BY account_sk                              
        ),
        account_component AS (               
            SELECT
                account_sk,
                FIRST(
                    CASE
                        WHEN component IN ('MENTHOL BASE', 'PORTFOLIO BASE')
                             THEN 'Base'        
                        WHEN component =  'PORTFOLIO VLP'
                             THEN 'LA'           
                        ELSE component           
                    END
                ) AS component
            FROM   rai_qa_uc_gm.d_core_pace.account_contract_map
            WHERE  component IN ('MENTHOL BASE','MENTHOL EDLP',
                                 'PORTFOLIO BASE','PORTFOLIO EDLP','PORTFOLIO VLP')
              AND  start_date <= DATE '{date}'
              AND  end_date   >= DATE '{date}'
            GROUP BY account_sk
        ),
        acc_meta AS (
            SELECT DISTINCT
                account_sk,
                account_name_and_code,
                account_state
            FROM   raw_df_str                       
        )
        SELECT  ac.account_sk,
                am.account_name_and_code,
                am.account_state,
                ac.component,
                CONCAT('CBO ', ab.cbo) AS contract
        FROM    account_component ac
        LEFT JOIN account_cbo ab USING (account_sk)
        LEFT JOIN acc_meta   am USING (account_sk)
    """).createOrReplaceTempView("mechanical_price_outlet_contract")

    # ----------------------------------------------------------------------
    # 4 · KPI view
    # ----------------------------------------------------------------------
    create_kpi_view(industry, export_path)
    kpi_vars = [v.replace("_norm", "") for v in kpi_variables_industry(industry) if v not in ["account_longitude_norm","account_latitude_norm"]]

    # ----------------------------------------------------------------------
    # 5 · Normalized PE‑RSD
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW pe_rsd AS (
            SELECT
                account_sk,
                product_lineup_sk,
                {"price_tier"          if gb_price_tier  else "''"} AS price_tier,
                {"cig_length"          if gb_cig_length  else "''"} AS cig_length,
                {"menthol_non_menthol" if gb_menthol     else "''"} AS menthol_non_menthol,

                -- ♦ quantity-weighted PE and price
                SUM(pe  * rsd_quantity) / SUM(rsd_quantity) AS pe,
                SUM(rsd_price * rsd_quantity) / SUM(rsd_quantity) AS rsd_price,

                SUM(rsd_quantity) AS rsd_quantity
            FROM   raw_pe_rsd
            WHERE  pe_type = 'pe'
            GROUP  BY 1,2,3,4,5
        )
    """)

    # ----------------------------------------------------------------------
    # 6 · Aggregated STR
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW df_str AS
        SELECT account_sk, scan_validated_flag, product_lineup_sk,
               account_state, account_msa_fips, account_county_fips,
               account_name_and_code, product_name,
               {"price_tier" if gb_price_tier else "''"}       AS price_tier,
               {"cig_length" if gb_cig_length else "''"}       AS cig_length,
               {"menthol_non_menthol" if gb_menthol else "''"} AS menthol_non_menthol,
               cig_meeting_competition_contracted_flag_2017,
               location_sp_edlp_menthol_flag,
               location_sp_edlp_balanced_flag,
               location_sp_base_flag,
               location_sp_edlp_value_flag,
               location_mp_edlp_value_flag,
               location_sp_edlp_premium_flag,
               location_mp_edlp_premium_flag,
               location_mp_edlp_balanced_flag,
               location_mp_base_flag,
               location_mpe_edlp_balanced_flag,
               location_mpe_edlp_premium_flag,
               location_mpe_edlp_value_flag,
               account_status,
               float(account_latitude) account_latitude,
               float(account_longitude) account_longitude,
               SUM(str_quantity) /
                 (DATEDIFF(MAX(max_week_sales), MIN(min_week_sales)) / 7 + 1)
                 AS str_weekavg_quantity,
               FIRST(str_quantity_industry /
                     (DATEDIFF(max_week_sales_industry,
                               min_week_sales_industry) / 7 + 1))
                 AS str_weekavg_quantity_industry,
               MAX(max_week_sales) AS max_week_sales
        FROM   raw_df_str
        GROUP  BY all
    """)

    # ----------------------------------------------------------------------
    # 7 · Final view str_train_vw
    # ----------------------------------------------------------------------
    spark.sql(f"""
        CREATE OR REPLACE TEMP VIEW str_train_vw AS
        WITH product_lineup AS (
            SELECT DISTINCT product_lineup_sk FROM pe_rsd
        ),
        base AS (
            SELECT  d.*, p.pe, p.rsd_price, p.rsd_quantity
            FROM    df_str d
            LEFT JOIN product_lineup pL
                   ON pL.product_lineup_sk = d.product_lineup_sk
            LEFT JOIN pe_rsd p
                   ON p.account_sk = d.account_sk
                  AND p.product_lineup_sk = d.product_lineup_sk
                  {"AND p.price_tier = d.price_tier"             if gb_price_tier  else ""}
                  {"AND p.cig_length = d.cig_length"             if gb_cig_length  else ""}
                  {"AND p.menthol_non_menthol = d.menthol_non_menthol"
                                                               if gb_menthol     else ""}
            WHERE   d.str_weekavg_quantity_industry IS NOT NULL
              AND   d.str_weekavg_quantity           IS NOT NULL
              AND   d.str_weekavg_quantity           > 0
              AND   d.account_status                 = 'Active'
        ),
        with_kpi AS (
            SELECT  b.*,
                    kpi_vw.bsns_sk,
                    {', '.join(kpi_vars)},
                    COALESCE(b.str_weekavg_quantity /
                             b.str_weekavg_quantity_industry, 0) AS str_weekavg_som
            FROM    base b
            LEFT JOIN kpi_vw ON kpi_vw.bsns_sk = b.account_sk
        ),
        with_contract AS (
            SELECT  w.*,
                    m_out.contract,
                    m_out.component,
                    CASE WHEN m_out.contract IS NOT NULL
                        AND  m_out.component IS NOT NULL
                        THEN 1 ELSE 0 END          AS bool_price_outlet_contract
            FROM    with_kpi w
            LEFT JOIN mechanical_price_outlet_contract m_out
                ON  m_out.account_name_and_code = w.account_name_and_code
                AND m_out.account_state         = w.account_state
        ),
        m_price_uni AS (
            SELECT  contract,
                    component,
                    product_name,
                    FIRST(str_price, TRUE) AS str_price
            FROM    mechanical_price_contract
            GROUP BY 1,2,3
        ),
        ranked AS (
                SELECT  wc.*,
                        ROW_NUMBER() OVER (
                            PARTITION BY wc.account_sk, wc.product_lineup_sk
                            ORDER BY wc.str_weekavg_quantity DESC,
                                    wc.product_lineup_sk
                        ) AS rn
                FROM    with_contract wc
            )

        SELECT  r.*,
                m.str_price,
                CASE WHEN m.str_price IS NOT NULL THEN 1 ELSE 0 END AS bool_price_contract
        FROM    ranked r
        LEFT JOIN m_price_uni m
            ON m.contract     = r.contract
            AND m.component    = r.component
            AND m.product_name = r.product_name
        WHERE   r.rn = 1;
    """)

# COMMAND ----------

# DBTITLE 1,ML model - dataset utils
def get_features_set(product_lineup_sk, l_product_lineup_sk, macroeconomics, get_by_length_level, get_by_price_tier, get_by_menthol, get_extra_num_feats_lgb):
  cat_feats = ["account_state"]
  num_feats = (
    ["rsd_price_perc"] + # rsd_price_perc expected to be the first in the list
    [f"{pid}_rsd_price_perc" for pid in l_product_lineup_sk] + # {pid}_rsd_price_perc list expected to be the second in the list
    # this cross product is adding too much in the pe elasticity
    #([f"{product_lineup_sk}_rsd_price_perc"] if get_by_length_level or get_by_price_tier else []) +
    [f"rsd_quantity_roll{r}_perc" for r in [2,4,8,13,26]] +
    ["relative_price_hist", "month_sin", "month_cos", "data_points"] +
    # ["rsd_price_ind_perc"] +
    [f"{pid}_rsd_quantity_roll4_perc" for pid in l_product_lineup_sk] +
    [f"{pid}_rsd_price_ref" for pid in l_product_lineup_sk]
    # this cross product is adding too much in the pe elasticity
    #([f"{product_lineup_sk}_rsd_quantity_roll4_perc"] if get_by_length_level or get_by_price_tier else [])
  )
  if macroeconomics:
    num_feats += ["unemployment_rate", "inflation_yoy", "consumer_spending_tobacco",  "gasoline_price", "consumer_price_index",
    "unemployment_rate_change" ,"inflation_yoy_change",  "consumer_spending_tobacco_change", "gasoline_price_change", "consumer_price_index_change"]
  num_feats += get_extra_num_feats_lgb
  if get_by_length_level:
    cat_feats += ["cig_length"]
  if get_by_price_tier:
    cat_feats += ["price_tier"]
  if get_by_menthol:
    cat_feats += ["menthol_non_menthol"]

  
  additional_cols = ["sales_week_start_date", "sales_week_forecast", "no_outlier", "weight", "rsd_quantity_roll4", "rsd_quantity_y", "rsd_price_y", "rsd_price_roll4", "account_sk", "rsd_quantity", "rsd_price"]    
  target_col = "rsd_quantity_perc"
  
  return cat_feats, num_feats, additional_cols, target_col

def split_train_test(train_input_df, train_start_date, train_end_date):
  print(f"Train start date: {train_start_date}")
  print(f"Train end date: {train_end_date}")

  # Split train/test - in the old code we used sales_week_forecast but we should use sales_week_start_date
  train_mask = (train_input_df.sales_week_forecast >= train_start_date) & \
                 (train_input_df.sales_week_forecast < train_end_date) & train_input_df.no_outlier
  test_mask = train_input_df.sales_week_forecast >= train_end_date
  test_clean_mask = (train_input_df.sales_week_forecast >= train_end_date) & train_input_df.no_outlier
  
  return train_mask, test_mask, test_clean_mask

# COMMAND ----------

# DBTITLE 1,STR KNN Helpers
import numpy as np
import pandas as pd
from pyspark.sql.functions import pandas_udf, PandasUDFType
from scipy.spatial import distance
from pyspark.sql import DataFrame
from pyspark.sql.types import FloatType

contracts = [
    "cig_meeting_competition_contracted_flag_2017",
    "location_sp_edlp_menthol_flag",
    "location_sp_edlp_balanced_flag",
    "location_sp_base_flag",
    "location_sp_edlp_value_flag",
    "location_mp_edlp_value_flag",
    "location_sp_edlp_premium_flag",
    "location_mp_edlp_premium_flag",
    "location_mp_edlp_balanced_flag",
    "location_mp_base_flag",
    "location_mpe_edlp_balanced_flag",
    "location_mpe_edlp_premium_flag",
    "location_mpe_edlp_value_flag",
    "no_contract",
]


@pandas_udf(returnType="float", functionType=PandasUDFType.SCALAR)
def custom_norm(x):
    xlog = np.log(x + 0.001)
    return (xlog - np.mean(xlog)) / np.std(xlog)


def custom_norm2(df: DataFrame) -> DataFrame:  # old method of normalization
    for feature in [
        "str_weekavg_quantity",
        "str_weekavg_quantity_industry",
        "str_weekavg_som",
    ]:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))
        result_schema = df.schema

        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm(df):
            xlog = np.log(df[feature] + 0.001)
            xlog_rsd = np.log(df[~df.pe.isna()][feature] + 0.001)
            df[f"{feature}_norm"] = (xlog - np.mean(xlog_rsd)) / np.std(xlog_rsd)
            df[f"{feature}_norm"] = df[f"{feature}_norm"].clip(-3.5, 3.5)
            return df

        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm)
    return df


def minmax_scaling(
    df: DataFrame, industry: str
) -> DataFrame:  # new method of normalization
    if industry == "fmc":
        features_name = ["VOL_TC", "IDX_TC"]
    elif industry == "vapor":
        features_name = [
            # "VOL_TV",
            # "IDX_TV",
            # "IDX_FLV_MEN",
            # "IDX_FLV_NM",
            # "IDX_FLV_XM_NM",
        ]
    elif industry == "mo":
        features_name = [
            # "VOL_TN",
            # "IDX_TN",
            # "IDX_FLV_MEN",
            # "IDX_FLV_NM",
        ]
    elif industry == "to":
        features_name = [
            # "VOL_TM",
            # "IDX_TM",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NATURAL",
            # "IDX_FLV_STRAIGHT",
            # "IDX_FLV_WINTERGREEN",
            # "IDX_PR_BV",
            # "IDX_PR_LV",
            # "IDX_PR_PP",
            # "IDX_PR_PFP",
            # "IDX_FLV_OTHER"
        ]
    elif industry == "snus":
        features_name = [
            # "VOL_TS",
            # "IDX_TS",
            # "IDX_FLV_MINT",
            # "IDX_FLV_NT"
        ]

    for feature in features_name:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))

        result_schema = df.schema

        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm(df):
            xlog = np.log(df[feature] + 0.001)
            minx = np.min(xlog)
            maxx = np.max(xlog)

            df[f"{feature}_norm"] = (xlog - minx) / (maxx - minx)
            return df

        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm)
    
    for feature in ["account_longitude", "account_latitude"]:
        df = df.withColumn(f"{feature}_norm", lit(None).cast(FloatType()))
        result_schema = df.schema
        @pandas_udf(result_schema, PandasUDFType.GROUPED_MAP)
        def custom_norm_min_max(df):
            minx = np.min(df[feature])
            maxx = np.max(df[feature])
            df[f"{feature}_norm"] = (df[feature] - minx) / (maxx - minx)
            return df
        df = df.groupBy(["product_lineup_sk", "account_state"]).apply(custom_norm_min_max)
    return df

def distance_by_industry(df: pd.DataFrame, row: pd.Series, industry: str) -> pd.Series:
    # create Series of 0
    distance = pd.Series(0, index=df.index)
    # get list of kpi variables
    kpi_variables = kpi_variables_industry(industry)
    # loop over selected variables for given industry
    for kpi_variable in kpi_variables:
        distance = distance + (abs(row[kpi_variable] - df[kpi_variable])) ** 2

    return distance


def custom_knn(
    df: pd.DataFrame, row: pd.Series, cluster_size: int, industry: str
) -> pd.Series:
    if df.shape[0] == 0:
        return pd.Series({"pe": np.nan, "nearest_account_sk": [], "distance": []})

    df["distance"] = distance_by_industry(df, row, industry)
    df["rsd_price"] = df["rsd_price"].round(decimals=2)
    row["str_price"] = round(row["str_price"],2)
    # We are using the price point elasticity method to compute a new pe for the rsd store as if it was using the str_price
    # We are using the following formulas:
    # pe_rsd = ((Q_str-Q_rsd)/Q_rsd) / ((P_str-P_rsd)/P_rsd),
    # we supose pe_rsd = current pe for the rsd store (this is a price point elasticity)
    # pe_str = ((Q_rsd-Q_str)/Q_str) / ((P_rsd-P_str)/P_str)
    # source: https://pressbooks.bccampus.ca/uvicecon103/chapter/4-2-elasticity/
    # we first compute Q_str = Q_rsd + pe_rsd*((P_str-P_rsd)/P_rsd)*Q_rsd
    # then the new pe_str using the above formula
    Q_rsd, P_rsd, P_str, pe_rsd = (
        df["rsd_quantity"],
        df["rsd_price"],
        row["str_price"],
        df["pe"],
    )

    # we don't want price variations greater than 10%
    P_str = pd.Series([P_str] * len(P_rsd), index=df.index.values)
    P_str[P_str / P_rsd > 1.1] = P_rsd[P_str / P_rsd > 1.1] * 1.1
    P_str[P_str / P_rsd < 1 / 1.1] = P_rsd[P_str / P_rsd < 1 / 1.1] * 1 / 1.1

    # Using linear curve and point elasticity
    # Q_str = Q_rsd + pe_rsd*((P_str-P_rsd)/P_rsd)*Q_rsd
    # df["pe_str"] = ((Q_rsd-Q_str)/Q_str) / ((P_rsd-P_str)/P_str)
    # df.loc[df.pe_str.isna(), "pe_str"] = df.loc[df.pe_str.isna(), "pe"]

    # Using log curve and point elasticity
    Q_str = Q_rsd * (P_str / P_rsd) ** pe_rsd
    df["pe_str"] = ((Q_rsd - Q_str) / Q_str) / ((P_rsd - P_str) / P_str)
    df.loc[df.pe_str.isna(), "pe_str"] = df.loc[df.pe_str.isna(), "pe"]

    # cap floor pe difference, no more than 0.1 pe difference
    # df.loc[df.pe_str-df.pe>0.1, "pe_str"] = df.loc[df.pe_str-df.pe>0.1, "pe"]+0.1
    # df.loc[df.pe-df.pe_str>0.1, "pe_str"] = df.loc[df.pe-df.pe_str>0.1, "pe"]-0.1

    df["distance"] = np.sqrt(df["distance"].astype(float))

    # we want to map str stores to rsd stores having a lower price
    if df[df["rsd_price"] <= row["str_price"]].shape[0] > 0:
        df = df[df["rsd_price"] <= row["str_price"]]

    df = df.sort_values(
        by=[
            "distance",
            "rsd_quantity",
            "str_weekavg_quantity",
            "str_weekavg_quantity_industry",
        ],
        ascending=[True, False, False,False],
    )

    return pd.Series(
        {
            "estimated_pe": (
                (
                    df.head(cluster_size).pe_str * df.head(cluster_size).rsd_quantity
                ).sum()
                / df.head(cluster_size).rsd_quantity.sum()
            ),
            "estimated_pe_rsd": (
                (df.head(cluster_size).pe * df.head(cluster_size).rsd_quantity).sum()
                / df.head(cluster_size).rsd_quantity.sum()
            ),
            "nearest_account_sk": list(df.head(cluster_size).account_sk),
            "distance": list(df.head(cluster_size).distance),
            "nearest_account_pe": list(df.head(cluster_size).pe),
            "nearest_account_price": list(df.head(cluster_size).rsd_price),
            "nearest_account_pe_str": list(df.head(cluster_size).pe_str),
        }
    )


def create_knn_spark(
    industry: str = "fmc", use_contract: bool = True, cluster_size: int = 5
):
    # function used to calculate pe_str using knn
    def knn_spark(df: pd.DataFrame) -> pd.DataFrame:
        f = df.knn_computed == 0
        df[["estimated_pe", "estimated_pe_rsd"]] = np.nan, np.nan
        df.loc[f, "estimated_pe"], df.loc[f, "estimated_pe_rsd"] = (
            df.loc[f, "pe"],
            df.loc[f, "pe"],
        )
        l_col = [
            "nearest_account_sk",
            "distance",
            "nearest_account_pe",
            "nearest_account_price",
            "nearest_account_pe_str",
        ]
        for c in l_col:
            df[c] = [None] * df.shape[0]
        if df[f].shape[0] > 0:
            if not use_contract:
                df.loc[~f, ["estimated_pe", "estimated_pe_rsd"] + l_col] = df.loc[
                    ~f
                ].apply(
                    lambda row: custom_knn(
                        df[(df.knn_computed == 0)].copy(), row, cluster_size, industry
                    ),
                    axis=1,
                )
            else:
                for contract in df.contract.unique():
                    f_contract = df.contract == contract
                    f_ref = [True] * df.shape[0]
                    if industry == "fmc":
                        if "EDLP" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                        if "Base" in contract:
                            f_ref = df["contract"].str.contains("Base")
                        if "VLP" in contract:
                            f_ref = df["contract"].str.contains("VLP")
                        if "No Contract" in contract:
                            f_ref = df["contract"].str.contains("Base")
                        if "Limited Assortment" in contract:
                            f_ref = df["contract"].str.contains("Limited Assortment")
                        if "CAMEL PGA" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                        if "NEWPORT PGA" in contract:
                            f_ref = df["contract"].str.contains("EDLP")
                    if industry in ["vapor", "vapr", "mo", "to", "snus"]:
                        # for these industries direct mapping is used
                        f_ref = df.contract == contract

                    if (
                        df[f & f_ref].shape[0] > 0
                    ):  # if the restrictions on the contract are not too restrictive
                        df.loc[
                            (~f) & f_contract,
                            ["estimated_pe", "estimated_pe_rsd"] + l_col,
                        ] = df.loc[(~f) & f_contract].apply(
                            lambda row: custom_knn(
                                df[f & f_ref].copy(), row, cluster_size, industry
                            ),
                            axis=1,
                        )
                    else:  # we remove the filter on the contract
                        df.loc[
                            (~f) & f_contract,
                            ["estimated_pe", "estimated_pe_rsd"] + l_col,
                        ] = df.loc[(~f) & f_contract].apply(
                            lambda row: custom_knn(
                                df[f].copy(), row, cluster_size, industry
                            ),
                            axis=1,
                        )
        return df

    return knn_spark

# COMMAND ----------

# DBTITLE 1,Setting DB parameters
try: 
    scope = "spi-eap-ds-rgm"
    host = dbutils.secrets.get(scope=scope, key="host")
    password = dbutils.secrets.get(scope=scope, key="password")
    user = dbutils.secrets.get(scope=scope, key="user")
    execution_role = dbutils.secrets.get(scope=scope, key="execution_role")
    database = dbutils.secrets.get(scope=scope, key="database")
    tempdir = dbutils.secrets.get(scope=scope, key="tempdir")

    rs_url = "jdbc:redshift://" + host + ":5439/" + database + "?user=" + user + "&password=" + password + "&ssl=true&sslFactory=com.amazon.redshift.ssl.NonValidatingFactory"
except Exception as e:
    print("No access to Redshift secrets. Skipping Redshift config.")
    rs_url = None
    tempdir = None

# COMMAND ----------

# DBTITLE 1,Dynamic dates selection
from typing import Tuple


def get_dynamic_start_end_dates(no_years: int = 2) -> Tuple[str, str]:
    """
    function returns start and end dates using dynamic dates
    for selected number of years
    """
    if no_years == 1:
        period = '(4024)'
    elif no_years == 2:
        period = (4024, 4041)
    else:
        raise ValueError("no_years parameter should be equal to 1 or 2")

    query = f"""select  min(a11.sales_week_start_date) sales_week_start_date,
    max(a11.sales_week_end_date) sales_week_end_date
    from D_RPT_FALCON_V.calendar_sales_lookup a11
    left outer join D_RPT_FALCON_V.calendar_sales_dynamic_weeks_lookup a12
    on (a11.sales_week_sk = a12.sales_week_sk)
    where a12.dynamic_date_str_id in {period}
    """

    df = (
        spark.read.format("com.databricks.spark.redshift")
        .option("url", rs_url)
        .option("tempdir", tempdir)
        .option("aws_iam_role", execution_role)
        .option("query", query)
        .load()
        .toPandas()
    )
    # display(dates_query2)
    start_date = str(df.sales_week_start_date[0])
    end_date = str(df.sales_week_end_date[0])
    return start_date, end_date

</file>

</files>
</repository_contents>

CONTEXT:
I just made another run with another parameters and here were the results
"hyperparameters": {
    "min_delta": 0.00001,
    "early_stopping_rounds": 100
}

Processing GRIZZLY MO X 7MG min_data_in_leaf False ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'rsd_quantity_ind_perc', 'rsd_quantity_industry_roll2_perc', 'rsd_quantity_industry_roll4_perc', 'rsd_quantity_industry_roll8_perc', 'rsd_quantity_industry_roll13_perc', 'rsd_quantity_industry_roll26_perc', 'rsd_quantity_industry_roll2', 'account_state'] -1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Train start date: 2023-06-01 00:00:00 Train end date: 2025-01-01 00:00:00 (9, 40) (39699, 40) sales_week_forecast 2024-12-23 sales_week_start_date 2024-11-25 00:00:00 dtype: object Percentage of rows discarded: 9% Percentage of sales discarded: 5% size of testing dataset (38058, 40) (39689, 40) {'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt', 'monotone_constraints': '-1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0', 'monotone_constraints_method': 'advanced', 'seed': 42, 'n_estimators': 600, 'learning_rate': 0.1, 'bagging_fraction': 1, 'feature_fraction': 0.8, 'num_threads': 10, 'early_stopping_rounds': 100} ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'rsd_quantity_ind_perc', 'rsd_quantity_industry_roll2_perc', 'rsd_quantity_industry_roll4_perc', 'rsd_quantity_industry_roll8_perc', 'rsd_quantity_industry_roll13_perc', 'rsd_quantity_industry_roll26_perc', 'rsd_quantity_industry_roll2', 'account_state'] Training until validation scores don't improve for 25 rounds /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. /local_disk0/.ephemeral_nfs/envs/pythonEnv-7e204ac3-8807-427d-9f6d-17764e761e42/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument 2025/08/19 19:49:31 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.6.0/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model. [25] training's l1: 0.968422 valid_1's l1: 0.914268 valid_2's l1: 0.870527 Early stopping, best iteration is: [1] training's l1: 0.968422 valid_1's l1: 0.914268 valid_2's l1: 0.870527 /databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
Processing VELO PLUS min_data_in_leaf False ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'rsd_quantity_ind_perc', 'rsd_quantity_industry_roll2_perc', 'rsd_quantity_industry_roll4_perc', 'rsd_quantity_industry_roll8_perc', 'rsd_quantity_industry_roll13_perc', 'rsd_quantity_industry_roll26_perc', 'rsd_quantity_industry_roll2', 'account_state'] -1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
Train start date: 2023-06-01 00:00:00 Train end date: 2025-01-01 00:00:00 /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. (241, 40) (1157395, 40) sales_week_forecast 2024-12-16 sales_week_start_date 2024-11-18 00:00:00 dtype: object /databricks/python/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead. Percentage of rows discarded: 53% Percentage of sales discarded: 51% size of testing dataset (1102344, 40) (1156878, 40) {'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt', 'monotone_constraints': '-1,,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0', 'monotone_constraints_method': 'advanced', 'seed': 42, 'n_estimators': 600, 'learning_rate': 0.1, 'bagging_fraction': 1, 'feature_fraction': 0.8, 'num_threads': 10, 'early_stopping_rounds': 100} ['rsd_price_perc', 'rsd_quantity_roll2_perc', 'rsd_quantity_roll4_perc', 'rsd_quantity_roll8_perc', 'rsd_quantity_roll13_perc', 'rsd_quantity_roll26_perc', 'relative_price_hist', 'month_sin', 'month_cos', 'data_points', 'unemployment_rate', 'inflation_yoy', 'consumer_spending_tobacco', 'gasoline_price', 'consumer_price_index', 'unemployment_rate_change', 'inflation_yoy_change', 'consumer_spending_tobacco_change', 'gasoline_price_change', 'consumer_price_index_change', 'rsd_quantity_ind_perc', 'rsd_quantity_industry_roll2_perc', 'rsd_quantity_industry_roll4_perc', 'rsd_quantity_industry_roll8_perc', 'rsd_quantity_industry_roll13_perc', 'rsd_quantity_industry_roll26_perc', 'rsd_quantity_industry_roll2', 'account_state'] /local_disk0/.ephemeral_nfs/envs/pythonEnv-7e204ac3-8807-427d-9f6d-17764e761e42/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument Training until validation scores don't improve for 25 rounds [25] training's l1: 0.788689 valid_1's l1: 1.05273 valid_2's l1: 0.954377 2025/08/19 19:50:13 WARNING mlflow.models.model: Model logged without a signature. Signatures will be required for upcoming model registry features as they validate model inputs and denote the expected schema of model outputs. Please visit https://www.mlflow.org/docs/2.6.0/models.html#set-signature-on-logged-model for instructions on setting a model signature on your logged model. [50] training's l1: 0.626833 valid_1's l1: 1.099 valid_2's l1: 1.00276 Early stopping, best iteration is: [26] training's l1: 0.783463 valid_1's l1: 1.0507 valid_2's l1: 0.952249
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount


06 Evaluate Outputs
pe	70070	70070	92355	-1.2439046827098152	7218092.9617500305	92355
pe	1052079	1052079	511	-0.37781844548027027	1267.75	511
pe	1053221	1053221	6	-7.187315120626452e-8	13.125	6
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount
pe	70070	70070	92516	-1.2493947683852624	7082878.980810404	92516
pe	1052079	1052079	32420	-0.8567867979892296	141952.0819120407	32420
pe	1053221	1053221	319	-1.9297708209861333	672.054166674614	319
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount
pe	70070	70070	92991	-1.2483269752786916	6981655.289623618	92991
pe	1052079	1052079	46759	-0.719354075047942	346484.4602352381	46759
pe	1053221	1053221	1029	-0.7170407261761155	2434.8389871120453	1029
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount
pe	70070	70070	96215	-1.2665698811351338	6974099.209904075	96215
pe	1052079	1052079	53556	-0.6558920170928308	563869.5533422232	53556
pe	1053221	1053221	2134	-0.5803620847251654	5460.387748360634	2134
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount
pe	70070	70070	95813	-1.2631591394111628	6852989.182592273	95813
pe	1052079	1052079	56836	-0.5926077654561015	707592.3101102114	56836
pe	1053221	1053221	2800	-0.5695141123102992	7588.39973115921	2800
pe_type	product_lineup_sk	simulated_price_product_lineup_sk	nb_rows	wpe	rsd_quantity_cross	n_acccount
pe	70070	70070	95340	-1.279592346809058	6757612.639448881	95340
pe	1052079	1052079	59202	-0.31157951107877896	834364.4189138412	59202
pe	1053221	1053221	3538	-0.08563539541232491	9823.226052045822	3538

Accuracy:
product_lineup_sk	prod_name	campaign_start_date	campaign_end_date	total_sum_absolute_error	total_sum_actual_volume	avg_w_accuracy_mape	avg_w_accuracy_week_product_state	w_accuracy
1052079	VELO PLUS	2025-03-01	2025-07-01	3011273.8659356325	17425730.75004697	73.89043028524165	88.22215321150045	82.71938256633791
1053221	GRIZZLY MO X 7MG	2025-03-01	2025-07-01	17720.82901634259	83269.66666948795	66.7659192130092	79.63037313458058	78.71874630328509

# Meta's Dream

I have been seeing many guesses as to why Meta is recently investing so much in talent. Even though pay packages have allegedly been misrepresented or exaggerated, it is clear that Meta's final goal is to be at the top of the frontier labs. But what are Mark's intentions? Meta has never caught up to the SOTA models, so why not a year ago? I do not think Meta is aiming to be the leading API provider like Anthropic or seeking the popularity of OpenAI. 

Meta has the trend hit, but its model roadmap is still behind. Mark's Super Intelligence gamble is an attempt to close the gap before all the other teams crack the hardware dilemma around AI.

I do not live in the US (yet), but one of my friends has been trying to get his hands on the newest Meta Ray-Ban glasses. He told me the girl who worked at Sunglass Hut said that they are always out of stock. Apparently, demand for these glasses is significantly high.

[EssilorLuxottica recently reported](https://www.essilorluxottica.com/cap/content/259500/) Apparently, revenue from sales of Ray-Ban Meta smart glasses more than tripled year-over-year. Their new Oakley Meta glasses, which feature up to 8 hours versus 4 hours on the first generation, are now also available. Meta’s CFO Susan Li emphasized this trend in back-to-back quarterly calls:

* [Q1 2025 Earnings Transcript](https://s21.q4cdn.com/399680738/files/doc_financials/2025/q1/Transcripts/META-Q1-2025-Earnings-Call-Transcript-1.pdf):

> "Within our Reality Labs segment, Q1 revenue was \$412 million, down 6% year-over-year due to lower Meta Quest sales, which were partially offset by increased sales of Ray-Ban Meta AI glasses."

* [Q2 2025 Earnings Remarks](https://s21.q4cdn.com/399680738/files/doc_downloads/2025/META-Q2-2025-Prepared-Remarks.pdf):

> "Within our Reality Labs segment, Q2 revenue was \$370 million, up 5% year-over-year due to increased sales of AI glasses, partially offset by lower Quest sales."

While anecdotal, my observations indicate significant public interest in these glasses. However, users frequently express frustration with the AI interactions. A common complaint illustrates this current gap in user experience:

> "You want to ask AI questions? It's frustratingly slow and ineffective unless you guide it step-by-step."

Right now, these glasses feel like a toy. Mark's gamble is set up for AI glasses to stop feeling this way; they are meant to replicate Apple's magic. Apple won the smartphone war because IOS worked. If the next generations of Llama feel slow or dumb, all the FLOPS in the US won't save them.

In my mind:

* **Meta has the product but not the model.**
* **All other companies have the model but not the product.**

We do not have all the information heads of million-dollar corporations do, but the players at this table have all placed their bets. Not all bets succeed (looking at you, Metaverse). However, Meta could genuinely succeed if they aggressively close the performance-per-TFLOP gap to bring back the magic we felt with Sonnet 3.5 but through glasses. This situation feels familiar to another bet that initially didn't seem logical: the ballad of Sam Altman and Jony Ive. This new duo want to create something Meta already has.

When Meta lands both the software and hardware, all the other big labs will become just apps on Meta's platform. Their SOTA models could become features that users access through "Hey Meta" with Mark controlling the distribution and the users.

## Meta's Reality

I couldn't confirm exactly which Llama model versions are linked to the Ray-Ban or Oakley glasses, but let's imagine they're using the best Vision Llama 4 model: meta-llama/Llama-4-Maverick-17B-128E-Instruct. Ranked 21st on the Vision Arena at [Hugging Face](https://huggingface.co/spaces/lmarena-ai/lmarena-leaderboard), this model may leave users who already love the product desperately wishing for more advanced features.

One of the most impressive abilities I personally found of OpenAI's o3 model was its capability to reliably pinpoint your exact location from just a photo. While writing this entry, I found myself daydreaming about how amazing it would be to pair these glasses with o3. Imagine asking for directions without having to pull out your phone or specify your location explicitly. For instance, while driving, instead of dangerously reaching for your phone on the highway, you could simply ask, "Hey Meta, which exit takes me to work?" The o3 model would intelligently analyze your surroundings from the glasses' view, identify your exact location, and provide precise instructions.

This vision aligns with the sentiment expressed by SemiAnalysis, suggesting that Meta remains a strong competitor in the AI race. The company's significant ramp-up in training FLOPS could potentially rival OpenAI. For more on this perspective, check out the recent [SemiAnalysis article](https://semianalysis.com/2025/07/11/meta-superintelligence-leadership-compute-talent-and-data/).

## Meta's Vision (Literally)

I share the view that a significant challenge facing Llama 4 is its limited multimodal data capabilities. If Meta does not have access to YouTube videos as training data, the next logical step would be leveraging the massive amounts of photos and videos collected through their smart glasses to enhance their training datasets. Access to proprietary context that no one else can touch can be the final edge on solving vision in the long run.

> “What it means to use technology can change in a profound way. I hope we can bring some of the delight, wonder, and creative spirit that I first felt using an Apple Computer 30 years ago.”
>
> — Sam Altman
https://openai.com/sam-and-jony/
